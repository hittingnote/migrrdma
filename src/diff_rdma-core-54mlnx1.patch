diff --git a/kernel-headers/rdma/ib_user_ioctl_cmds.h b/kernel-headers/rdma/ib_user_ioctl_cmds.h
index dafc7eb..a0deb11 100644
--- a/kernel-headers/rdma/ib_user_ioctl_cmds.h
+++ b/kernel-headers/rdma/ib_user_ioctl_cmds.h
@@ -58,6 +58,7 @@ enum uverbs_default_objects {
 	UVERBS_OBJECT_DM,
 	UVERBS_OBJECT_COUNTERS,
 	UVERBS_OBJECT_ASYNC_EVENT,
+	UVERBS_OBJECT_FOOTPRINT,
 };
 
 enum {
@@ -121,6 +122,20 @@ enum uverbs_attrs_create_flow_action_esp {
 	UVERBS_ATTR_FLOW_ACTION_ESP_ENCAP,
 };
 
+enum uverbs_attrs_install_footprint {
+	UVERBS_ATTR_FOOTPRINT_IN_FD,
+};
+
+enum uverbs_attrs_install_ctx_resp {
+	UVERBS_ATTR_RESP_PTR,
+	UVERBS_ATTR_RESP_SZ,
+};
+
+enum uverbs_attr_install_handle_mapping {
+	UVERBS_ATTR_VHANDLE,
+	UVERBS_ATTR_HANDLE,
+};
+
 enum uverbs_attrs_modify_flow_action_esp {
 	UVERBS_ATTR_MODIFY_FLOW_ACTION_ESP_HANDLE =
 		UVERBS_ATTR_CREATE_FLOW_ACTION_ESP_HANDLE,
@@ -364,6 +379,27 @@ enum uverbs_method_async_event {
 	UVERBS_METHOD_ASYNC_EVENT_ALLOC,
 };
 
+enum uverbs_method_rdma_footprint {
+	UVERBS_METHOD_INSTALL_FOOTPRINT,
+	UVERBS_METHOD_INSTALL_CTX_RESP,
+	UVERBS_METHOD_REGISTER_ASYNC_FD,
+	UVERBS_METHOD_INSTALL_PD_HANDLE_MAPPING,
+	UVERBS_METHOD_INSTALL_CQ_HANDLE_MAPPING,
+	UVERBS_METHOD_INSTALL_MR_HANDLE_MAPPING,
+	UVERBS_METHOD_INSTALL_QP_HANDLE_MAPPING,
+	UVERBS_METHOD_INSTALL_QPN_DICT,
+	UVERBS_METHOD_INSTALL_SRQ_HANDLE_MAPPING,
+	UVERBS_METHOD_INSTALL_LQPN_MAPPING,
+	UVERBS_METHOD_INSTALL_LKEY_MAPPING,
+	UVERBS_METHOD_INSTALL_LOCAL_RKEY_MAPPING,
+	UVERBS_METHOD_DELETE_LOCAL_RKEY_MAPPING,
+	UVERBS_METHOD_DELETE_LQPN_MAPPING,
+	UVERBS_METHOD_DELETE_LKEY_MAPPING,
+	UVERBS_METHOD_REGISTER_REMOTE_GID_PID,
+	UVERBS_METHOD_GET_LOCAL_RDMA_PID,
+	UVERBS_METHOD_UPDATE_COMP_CHANNEL_FD,
+};
+
 enum uverbs_attrs_async_event_create {
 	UVERBS_ATTR_ASYNC_EVENT_ALLOC_FD_HANDLE,
 };
diff --git a/kernel-headers/rdma/ib_user_verbs.h b/kernel-headers/rdma/ib_user_verbs.h
index 7ee73a0..64d8e98 100644
--- a/kernel-headers/rdma/ib_user_verbs.h
+++ b/kernel-headers/rdma/ib_user_verbs.h
@@ -124,6 +124,7 @@ struct ib_uverbs_async_event_desc {
 
 struct ib_uverbs_comp_event_desc {
 	__aligned_u64 cq_handle;
+	__u64			flag;
 };
 
 struct ib_uverbs_cq_moderation_caps {
diff --git a/libibverbs/CMakeLists.txt b/libibverbs/CMakeLists.txt
index 3c486b9..02b0968 100644
--- a/libibverbs/CMakeLists.txt
+++ b/libibverbs/CMakeLists.txt
@@ -54,6 +54,9 @@ rdma_library(ibverbs "${CMAKE_CURRENT_BINARY_DIR}/libibverbs.map"
   static_driver.c
   sysfs.c
   verbs.c
+  rbtree.c
+  q_tree.c
+  my_mem_mgnt.c
   )
 target_link_libraries(ibverbs LINK_PRIVATE
   ${NL_LIBRARIES}
diff --git a/libibverbs/cmd_device.c b/libibverbs/cmd_device.c
index 8a48aea..e7418d4 100644
--- a/libibverbs/cmd_device.c
+++ b/libibverbs/cmd_device.c
@@ -106,6 +106,182 @@ int ibv_cmd_query_port(struct ibv_context *context, uint8_t port_num,
 	return 0;
 }
 
+int ibv_cmd_install_footprint(struct ibv_context *context) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_INSTALL_FOOTPRINT, 1);
+
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_FOOTPRINT_IN_FD,
+					&context->cmd_fd);
+
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_install_qpndict(struct ibv_context *context,
+				uint32_t real_qpn, uint32_t vqpn) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_INSTALL_QPN_DICT, 2);
+	fill_attr_in_ptr(cmdb, 0, &real_qpn);
+	fill_attr_in_ptr(cmdb, 1, &vqpn);
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_install_ctx_resp(struct ibv_context *context, void *resp, size_t size) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_INSTALL_CTX_RESP, 2);
+	
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_RESP_PTR, &resp);
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_RESP_SZ, &size);
+
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_register_async_fd(struct ibv_context *context, int async_fd) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_REGISTER_ASYNC_FD, 1);
+	
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_FOOTPRINT_IN_FD,
+					&async_fd);
+	
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_install_pd_handle_mapping(struct ibv_context *context,
+							int vhandle, int handle) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_INSTALL_PD_HANDLE_MAPPING, 2);
+	
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_VHANDLE, &vhandle);
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_HANDLE, &handle);
+
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_install_mr_handle_mapping(struct ibv_context *context,
+					int vhandle, int handle) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_INSTALL_MR_HANDLE_MAPPING, 2);
+	
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_VHANDLE, &vhandle);
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_HANDLE, &handle);
+
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_install_qp_handle_mapping(struct ibv_context *context,
+					int vhandle, int handle) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_INSTALL_QP_HANDLE_MAPPING, 2);
+	
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_VHANDLE, &vhandle);
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_HANDLE, &handle);
+
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_install_srq_handle_mapping(struct ibv_context *context,
+					int vhandle, int handle) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_INSTALL_SRQ_HANDLE_MAPPING, 2);
+	
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_VHANDLE, &vhandle);
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_HANDLE, &handle);
+
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_install_cq_handle_mapping(struct ibv_context *context,
+							int vhandle, int handle) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_INSTALL_CQ_HANDLE_MAPPING, 2);
+	
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_VHANDLE, &vhandle);
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_HANDLE, &handle);
+
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_install_lkey_mapping(struct ibv_context *context,
+							uint32_t vlkey, uint32_t lkey) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_INSTALL_LKEY_MAPPING, 2);
+	
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_VHANDLE, &vlkey);
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_HANDLE, &lkey);
+
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_install_local_rkey_mapping(struct ibv_context *context,
+							uint32_t vrkey, uint32_t rkey) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_INSTALL_LOCAL_RKEY_MAPPING, 2);
+	
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_VHANDLE, &vrkey);
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_HANDLE, &rkey);
+
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_delete_local_rkey_mapping(struct ibv_context *context, uint32_t vrkey) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_DELETE_LOCAL_RKEY_MAPPING, 1);
+	
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_VHANDLE, &vrkey);
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_delete_lkey_mapping(struct ibv_context *context, uint32_t vlkey) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_DELETE_LKEY_MAPPING, 1);
+	
+	fill_attr_in_ptr(cmdb, UVERBS_ATTR_VHANDLE, &vlkey);
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_register_remote_gid_pid(struct ibv_context *context,
+					const union ibv_gid *gid, pid_t pid) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_REGISTER_REMOTE_GID_PID, 2);
+	
+	fill_attr_in_ptr(cmdb, 0, gid);
+	fill_attr_in_ptr(cmdb, 1, &pid);
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_update_comp_channel_fd(struct ibv_context *context,
+					struct ibv_comp_channel *channel) {
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_UPDATE_COMP_CHANNEL_FD, 1);
+
+	fill_attr_in_fd(cmdb, 0, channel->fd);
+	return execute_ioctl(context, cmdb);
+}
+
+int ibv_cmd_get_rdma_pid(struct ibv_context *context) {
+	pid_t rdma_pid;
+
+	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_FOOTPRINT,
+					UVERBS_METHOD_GET_LOCAL_RDMA_PID, 1);
+	
+	fill_attr_out_ptr(cmdb, 0, &rdma_pid);
+	if(execute_ioctl(context, cmdb)) {
+		return -1;
+	}
+
+	return rdma_pid;
+}
+
+#include "rdwr_flag.h"
+
+static inline int uint32_t_compare(uint32_t key1, uint32_t key2) {
+	if(key1 < key2)
+		return -1;
+	else if(key1 > key2)
+		return 1;
+	else
+		return 0;
+}
+
 int ibv_cmd_alloc_async_fd(struct ibv_context *context)
 {
 	DECLARE_COMMAND_BUFFER(cmdb, UVERBS_OBJECT_ASYNC_EVENT,
diff --git a/libibverbs/compiler.h b/libibverbs/compiler.h
new file mode 100644
index 0000000..bd3de01
--- /dev/null
+++ b/libibverbs/compiler.h
@@ -0,0 +1,149 @@
+#ifndef __CR_COMPILER_H__
+#define __CR_COMPILER_H__
+
+/*
+ * Various definitions for success build,
+ * picked from various places, mostly from
+ * the linux kernel.
+ */
+
+#define ARRAY_SIZE(x)		(sizeof(x) / sizeof((x)[0]))
+#define NELEMS_AS_ARRAY(x, y)	(sizeof(x) / sizeof((y)[0]))
+#define BUILD_BUG_ON(condition) ((void)sizeof(char[1 - 2 * !!(condition)]))
+
+#define ASSIGN_TYPED(a, b)            \
+	do {                          \
+		(a) = (typeof(a))(b); \
+	} while (0)
+#define ASSIGN_MEMBER(a, b, m)                \
+	do {                                  \
+		ASSIGN_TYPED((a)->m, (b)->m); \
+	} while (0)
+
+#define __stringify_1(x...) #x
+#define __stringify(x...)   __stringify_1(x)
+
+#define NORETURN	__attribute__((__noreturn__))
+#define __packed	__attribute__((__packed__))
+#define __used		__attribute__((__used__))
+#define __maybe_unused	__attribute__((unused))
+#define __always_unused __attribute__((unused))
+#define __must_check	__attribute__((__warn_unused_result__))
+
+#define __section(S) __attribute__((__section__(#S)))
+
+#ifndef __always_inline
+#define __always_inline inline __attribute__((always_inline))
+#endif
+
+#define likely(x)   __builtin_expect(!!(x), 1)
+#define unlikely(x) __builtin_expect(!!(x), 0)
+
+#ifndef always_inline
+#define always_inline __always_inline
+#endif
+
+#ifndef noinline
+#define noinline __attribute__((noinline))
+#endif
+
+#define __aligned(x) __attribute__((aligned(x)))
+
+/*
+ * Macro to define stack alignment.
+ * aarch64 requires stack to be aligned to 16 bytes.
+ */
+#define __stack_aligned__ __attribute__((aligned(16)))
+
+#ifndef offsetof
+#define offsetof(TYPE, MEMBER) ((size_t) & ((TYPE *)0)->MEMBER)
+#endif
+
+#define barrier() asm volatile("" ::: "memory")
+
+#define container_of(ptr, type, member)                            \
+	({                                                         \
+		const typeof(((type *)0)->member) *__mptr = (ptr); \
+		(type *)((char *)__mptr - offsetof(type, member)); \
+	})
+
+#ifndef FIELD_SIZEOF
+#define FIELD_SIZEOF(t, f) (sizeof(((t *)0)->f))
+#endif
+
+#define __round_mask(x, y) ((__typeof__(x))((y)-1))
+#define round_up(x, y)	   ((((x)-1) | __round_mask(x, y)) + 1)
+#define round_down(x, y)   ((x) & ~__round_mask(x, y))
+#define DIV_ROUND_UP(n, d) (((n) + (d)-1) / (d))
+#define ALIGN(x, a)	   (((x) + (a)-1) & ~((a)-1))
+
+#define min(x, y)                              \
+	({                                     \
+		typeof(x) _min1 = (x);         \
+		typeof(y) _min2 = (y);         \
+		(void)(&_min1 == &_min2);      \
+		_min1 < _min2 ? _min1 : _min2; \
+	})
+
+#define max(x, y)                              \
+	({                                     \
+		typeof(x) _max1 = (x);         \
+		typeof(y) _max2 = (y);         \
+		(void)(&_max1 == &_max2);      \
+		_max1 > _max2 ? _max1 : _max2; \
+	})
+
+#define min_t(type, x, y)                          \
+	({                                         \
+		type __min1 = (x);                 \
+		type __min2 = (y);                 \
+		__min1 < __min2 ? __min1 : __min2; \
+	})
+
+#define max_t(type, x, y)                          \
+	({                                         \
+		type __max1 = (x);                 \
+		type __max2 = (y);                 \
+		__max1 > __max2 ? __max1 : __max2; \
+	})
+
+#define SWAP(x, y)                     \
+	do {                           \
+		typeof(x) ____val = x; \
+		x = y;                 \
+		y = ____val;           \
+	} while (0)
+
+#define is_log2(v) (((v) & ((v)-1)) == 0)
+
+/*
+ * Use "__ignore_value" to avoid a warning when using a function declared with
+ * gcc's warn_unused_result attribute, but for which you really do want to
+ * ignore the result.  Traditionally, people have used a "(void)" cast to
+ * indicate that a function's return value is deliberately unused.  However,
+ * if the function is declared with __attribute__((warn_unused_result)),
+ * gcc issues a warning even with the cast.
+ *
+ * Caution: most of the time, you really should heed gcc's warning, and
+ * check the return value.  However, in those exceptional cases in which
+ * you're sure you know what you're doing, use this function.
+ *
+ * Normally casting an expression to void discards its value, but GCC
+ * versions 3.4 and newer have __attribute__ ((__warn_unused_result__))
+ * which may cause unwanted diagnostics in that case.  Use __typeof__
+ * and __extension__ to work around the problem, if the workaround is
+ * known to be needed.
+ * Written by Jim Meyering, Eric Blake and PÃ¡draig Brady.
+ * (See https://gcc.gnu.org/bugzilla/show_bug.cgi?id=66425 for the details)
+ */
+#if 3 < __GNUC__ + (4 <= __GNUC_MINOR__)
+#define __ignore_value(x)                \
+	({                               \
+		__typeof__(x) __x = (x); \
+		(void)__x;               \
+	})
+#else
+#define __ignore_value(x) ((void)(x))
+#endif
+
+#endif /* __CR_COMPILER_H__ */
diff --git a/libibverbs/device.c b/libibverbs/device.c
index 2f0b3b7..315f4ff 100644
--- a/libibverbs/device.c
+++ b/libibverbs/device.c
@@ -320,12 +320,64 @@ static void set_lib_ops(struct verbs_context *vctx)
 		(void (*)(void))vctx->ibv_destroy_flow;
 }
 
+#include "rdwr_flag.h"
+
+#define construct_mmap(context_ex, dir_fd, info_fd, map_field,						\
+					need_resume, mmap_fd, mmap_vaddr, ret)							\
+	(context_ex)->context.map_field##_fd = openat(dir_fd,							\
+							#map_field "_map", map_field##_FLAG);					\
+	if((context_ex)->context.map_field##_fd < 0) {									\
+		close(dir_fd);																\
+		ibv_close_device(&(context_ex)->context);									\
+		return ret;																	\
+	}																				\
+																					\
+	if(need_resume && mmap_fd != (context_ex)->context.map_field##_fd &&			\
+				dup2((context_ex)->context.map_field##_fd, mmap_fd) < 0) {			\
+		close(dir_fd);																\
+		ibv_close_device(&(context_ex)->context);									\
+		return ret;																	\
+	}																				\
+																					\
+	if(need_resume && mmap_fd != (context_ex)->context.map_field##_fd) {			\
+		close((context_ex)->context.map_field##_fd);								\
+		(context_ex)->context.map_field##_fd = mmap_fd;								\
+	}																				\
+																					\
+	(context_ex)->context.map_field##_mapping =										\
+					mmap((need_resume)? mmap_vaddr: NULL,							\
+					getpagesize(), map_field##_PROT, MAP_SHARED,					\
+					(context_ex)->context.map_field##_fd, 0);						\
+	if((context_ex)->context.map_field##_mapping == MAP_FAILED) {					\
+		close(dir_fd);																\
+		ibv_close_device(&(context_ex)->context);									\
+		return ret;																	\
+	}																				\
+																					\
+	info_fd = openat(dir_fd, #map_field "_mmap_fd", O_WRONLY);						\
+	if(info_fd < 0) {																\
+		close(dir_fd);																\
+		ibv_close_device(&(context_ex)->context);									\
+		return ret;																	\
+	}																				\
+																					\
+	if(write(info_fd, &(context_ex)->context.map_field##_fd, sizeof(int)) < 0) {	\
+		close(info_fd);																\
+		close(dir_fd);																\
+		ibv_close_device(&(context_ex)->context);									\
+		return ret;																	\
+	}																				\
+																					\
+	close(info_fd)
+
 struct ibv_context *verbs_open_device(struct ibv_device *device, void *private_data)
 {
 	struct verbs_device *verbs_device = verbs_get_device(device);
 	int cmd_fd;
 	struct verbs_context *context_ex;
-	int ret;
+	int ret, context_dir_fd;
+	char fname[128];
+	int info_fd;
 
 	/*
 	 * We'll only be doing writes, but we need O_RDWR in case the
@@ -353,14 +405,1397 @@ struct ibv_context *verbs_open_device(struct ibv_device *device, void *private_d
 		}
 	}
 
+	ret = ibv_cmd_register_async_fd(&context_ex->context, 
+					context_ex->context.async_fd);
+	if(ret) {
+		ibv_close_device(&context_ex->context);
+		return NULL;
+	}
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d", rdma_getpid(&context_ex->context),
+						context_ex->context.cmd_fd);
+	context_dir_fd = open(fname, O_DIRECTORY);
+	if(context_dir_fd < 0) {
+		ibv_close_device(&context_ex->context);
+		return NULL;
+	}
+
+	if(dup2(context_dir_fd, context_dir_fd + 1000) < 0) {
+		close(context_dir_fd);
+		ibv_close_device(&context_ex->context);
+		return NULL;
+	}
+
+	close(context_dir_fd);
+	context_dir_fd = context_dir_fd + 1000;
+
+	info_fd = openat(context_dir_fd, "ctx_uaddr", O_WRONLY);
+	if(info_fd < 0) {
+		ibv_close_device(&context_ex->context);
+		return NULL;
+	}
+
+	if(write(info_fd, &context_ex, sizeof(context_ex)) < 0) {
+		close(info_fd);
+		ibv_close_device(&context_ex->context);
+		return NULL;
+	}
+
+	close(info_fd);
+
+	construct_mmap(context_ex, context_dir_fd, info_fd, lkey, false, 0, 0, NULL);
+	construct_mmap(context_ex, context_dir_fd, info_fd, rkey, false, 0, 0, NULL);
+
+	close(context_dir_fd);
+
 	return &context_ex->context;
 }
 
+#include <signal.h>
+
+int signal_flag = 1;
+
+int ibv_get_signal(void) {
+	return signal_flag;
+}
+
+#include <sys/time.h>
+#include <unistd.h>
+
+static int iter_context_munmap(struct ibv_context *ctx, void *entry, void *in_param) {
+	return 0;
+}
+
+static int iter_context_mmap(struct ibv_context *ctx, void *entry, void *in_param) {
+	return 0;
+}
+
+int __rdma_pid__;
+
+static int iter_cq_uwrite(struct ibv_cq *cq, void *entry, void *in_param) {
+	cq->wc = NULL;
+	cq->qps = NULL;
+	cq->srqs = NULL;
+	return get_ops(cq->context)->uwrite_cq(cq, 0);
+}
+
+static int iter_qp_uwrite(struct ibv_qp *qp, void *entry, void *in_param) {
+	return get_ops(qp->context)->uwrite_qp(qp, 0);
+}
+
+static int iter_srq_uwrite(struct ibv_srq *srq, void *entry, void *in_param) {
+	return get_ops(srq->context)->uwrite_srq(srq, 0);
+}
+
+static int iter_add_old_qpndict(struct ibv_qp *qp, void *entry, void *in_param) {
+	add_old_dict_node(qp, qp->real_qpn, qp->qp_num);
+	return 0;
+}
+
+struct rdma_mmap_item {
+	unsigned long					start;
+	unsigned long					end;
+	int								prot;
+	int								flag;
+};
+
+static struct rdma_mmap_item rdma_mmaps[128];
+static int n_rdma_mmaps = 0;
+
+static void do_sigtstp(int signo);
+
+static inline void atomic_set(int *n, int val) {
+	*n = val;
+}
+
+static inline int atomic_get(int *n) {
+	return *(volatile int *)n;
+}
+
+static inline void atomic_dec(int *n) {
+	asm volatile( "\n\tlock; decl %0" : "+m"(*n));
+}
+
+#define __xchg_op(ptr, arg, op, lock)                                                                        \
+	({                                                                                                   \
+		__typeof__(*(ptr)) __ret = (arg);                                                            \
+		switch (sizeof(*(ptr))) {                                                                    \
+		case 1:                                                                           \
+			asm volatile(lock #op "b %b0, %1\n" : "+q"(__ret), "+m"(*(ptr)) : : "memory", "cc"); \
+			break;                                                                               \
+		case 2:                                                                           \
+			asm volatile(lock #op "w %w0, %1\n" : "+r"(__ret), "+m"(*(ptr)) : : "memory", "cc"); \
+			break;                                                                               \
+		case 4:                                                                           \
+			asm volatile(lock #op "l %0, %1\n" : "+r"(__ret), "+m"(*(ptr)) : : "memory", "cc");  \
+			break;                                                                               \
+		case 8:                                                                           \
+			asm volatile(lock #op "q %q0, %1\n" : "+r"(__ret), "+m"(*(ptr)) : : "memory", "cc"); \
+			break;                                                                               \
+		}                                                                                            \
+		__ret;                                                                                       \
+	})
+
+#define __xadd(ptr, inc, lock) __xchg_op((ptr), (inc), xadd, lock)
+#define xadd(ptr, inc)	       __xadd((ptr), (inc), "lock ;")
+
+static pthread_rwlock_t tstp_rwlock = PTHREAD_RWLOCK_INITIALIZER;
+static int tstp_n_threads = 0;
+
+static void sigtstp_handler(int signo) {
+	if(getpid() == gettid()) {
+	int sig_0 = 0;
+	int fd;
+	char fname[128];
+	FILE *fp;
+	char strln[1024];
+
+		sprintf(fname, "/proc/rdma_uwrite/%d/to_frm", __rdma_pid__);
+		fd = open(fname, O_RDWR);
+		read(fd, &sig_0, sizeof(int));
+		close(fd);
+
+		/* Make sure all calling threads exit only after the main thread exits */
+		pthread_rwlock_wrlock(&tstp_rwlock);
+		atomic_set(&tstp_n_threads, sig_0);
+
+		/* Wait until all threads enter handler function */
+		while(1) {
+			int tmp = atomic_get(&tstp_n_threads);
+			if(tmp == 0)
+				break;
+		}
+
+		do_sigtstp(signo);
+}
+	else {
+		/* Wait for main thread to finish prepare work */
+		while(1) {
+			int tmp = atomic_get(&tstp_n_threads);
+			if(tmp > 0) {
+				break;
+			}
+		}
+
+		atomic_dec(&tstp_n_threads);
+		/* Wait for the main thread to finish */
+		pthread_rwlock_rdlock(&tstp_rwlock);
+	}
+
+	pthread_rwlock_unlock(&tstp_rwlock);
+	atomic_set(&tstp_n_threads, 0);
+}
+
+static int iter_cq_start_poll(struct ibv_cq *cq,
+				void *entry, void *in_param) {
+	get_ops(cq->context)->migrrdma_start_poll(cq);
+	cq->stop_flag = 1;
+	return 0;
+}
+
+static int iter_cq_end_poll(struct ibv_cq *cq,
+				void *entry, void *in_param) {
+	get_ops(cq->context)->migrrdma_end_poll(cq);
+	return 0;
+}
+
+struct wait_qp_ent {
+	struct ibv_qp			*qp;
+	struct list_head		ent;
+};
+
+struct wait_srq_ent {
+	struct ibv_srq			*srq;
+	struct list_head		ent;
+};
+
+static int iter_qp_prepare_for_migr(struct ibv_qp *qp,
+				void *entry, void *in_param) {
+	struct list_head *wait_qp_list =
+						(struct list_head *)in_param;
+	struct wait_qp_ent *wait_qp_ent;
+
+	wait_qp_ent = my_malloc(sizeof(*wait_qp_ent));
+	if(!wait_qp_ent) {
+		return -ENOMEM;
+	}
+	memset(wait_qp_ent, 0, sizeof(*wait_qp_ent));
+
+	wait_qp_ent->qp = qp;
+	qp->wait_qp_node = wait_qp_ent;
+
+	pthread_rwlock_wrlock(&qp->rwlock);
+	qp->pause_flag = 1;
+	get_ops(qp->context)->migrrdma_start_inspect_qp_v2(qp);
+	list_add(wait_qp_list, &wait_qp_ent->ent);
+	pthread_rwlock_unlock(&qp->rwlock);
+
+	if(qp->srq)
+		qp->srq->cnt = 0;
+	qp->touched = 0;
+
+	return 0;
+}
+
+static int iter_qp_add_wait_list_r2(struct ibv_qp *qp,
+				void *entry, void *in_param) {
+	struct list_head *wait_srq_list =
+						(struct list_head *)in_param;
+	struct wait_srq_ent *wait_srq_ent;
+
+	if(!qp->srq || qp->srq->wait_srq_node) {
+		return 0;
+	}
+	
+	wait_srq_ent = my_malloc(sizeof(*wait_srq_ent));
+	if(!wait_srq_ent) {
+		return -ENOMEM;
+	}
+	memset(wait_srq_ent, 0, sizeof(*wait_srq_ent));
+
+	wait_srq_ent->srq = qp->srq;
+	list_add(wait_srq_list, &wait_srq_ent->ent);
+	qp->srq->wait_srq_node = wait_srq_ent;
+
+	return 0;
+}
+
+static uint64_t get_n_posted_from_dest_qpn(uint32_t dest_qpn);
+
+static int iter_cq_poll_cq(struct ibv_cq *cq,
+				void *entry, void *in_param) {
+	int ne;
+
+	if(!cq->wc) {
+		cq->wc = my_malloc(cq->cqe * sizeof(struct ibv_wc));
+		if(!cq->wc)
+			return -ENOMEM;
+	}
+
+	if(!cq->qps) {
+		cq->qps = my_malloc(cq->cqe * sizeof(struct ibv_qp *));
+		if(!cq->qps)
+			return -ENOMEM;
+	}
+
+	memset(cq->wc, 0, cq->cqe * sizeof(*cq->wc));
+	memset(cq->qps, 0, cq->cqe * sizeof(struct ibv_qp *));
+
+	ne = get_ops(cq->context)->migrrdma_poll_cq(cq, cq->cqe, cq->wc, cq->qps, NULL);
+	if(ne < 0)
+		return -1;
+
+	for(int i = 0; i < ne; i++) {
+		struct ibv_qp *qp = cq->qps[i];
+		if(!qp)
+			continue;
+		uint64_t n_posted = get_n_posted_from_dest_qpn(qp->dest_qpn);
+		uint64_t n_acked = get_ops(qp->context)->qp_get_n_acked(qp);
+		if(!qp->touched) {
+			qp->touched = 1;
+			if(qp->srq) {
+				if(n_posted != (uint64_t)-1)
+					qp->srq->cnt += n_posted;
+			}
+		}
+
+		if(n_posted == (uint64_t)-1)
+			n_posted = 0;
+
+		if(get_ops(qp->context)->migrrdma_is_q_empty(qp) &&
+					(qp->srq || n_posted <= n_acked)) {
+			if(qp->wait_qp_node) {
+				struct wait_qp_ent *wait_ent = qp->wait_qp_node;
+				list_del(&wait_ent->ent);
+				qp->wait_qp_node = NULL;
+			}
+		}
+	}
+
+	return 0;
+}
+
+static int iter_cq_poll_cq_r2(struct ibv_cq *cq,
+				void *entry, void *in_param) {
+	int ne;
+
+	if(!cq->wc) {
+		cq->wc = my_malloc(cq->cqe * sizeof(struct ibv_wc));
+		if(!cq->wc)
+			return -ENOMEM;
+	}
+
+	if(!cq->srqs) {
+		cq->srqs = my_malloc(cq->cqe * sizeof(struct ibv_srq *));
+		if(!cq->srqs)
+			return -ENOMEM;
+	}
+
+	memset(cq->wc, 0, cq->cqe * sizeof(*cq->wc));
+	memset(cq->srqs, 0, cq->cqe * sizeof(struct ibv_srq *));
+
+	ne = get_ops(cq->context)->migrrdma_poll_cq(cq, cq->cqe, cq->wc, NULL, cq->srqs);
+	if(ne < 0)
+		return -1;
+
+	for(int i = 0; i < ne; i++) {
+		uint64_t n_acked;
+		struct ibv_srq *srq = cq->srqs[i];
+		if(!srq)
+			continue;
+
+		n_acked = get_ops(srq->context)->srq_get_n_acked(srq);
+		if(srq->cnt <= n_acked) {
+			if(srq->wait_srq_node) {
+				struct wait_srq_ent *wait_ent = srq->wait_srq_node;
+				list_del(&wait_ent->ent);
+				srq->wait_srq_node = NULL;
+			}
+		}
+	}
+
+	return 0;
+}
+
+struct reply_hdr_fmt {
+	int								cnt;
+	char							msg[0];
+};
+
+struct reply_item_fmt {
+	uint32_t						qpn;
+	uint64_t						n_posted;
+};
+
+#include "rbtree.h"
+
+static declare_and_init_rbtree(n_posted_tree);
+
+struct n_posted_entry {
+	uint32_t				dest_qpn;
+	uint64_t				n_posted;
+	struct rb_node			rb_node;
+};
+
+static inline struct n_posted_entry *to_n_posted_entry(struct rb_node *n) {
+	return n? container_of(n, struct n_posted_entry, rb_node): NULL;
+}
+
+static inline int n_posted_entry_compare(const struct rb_node *n1, const struct rb_node *n2) {
+	struct n_posted_entry *ent1 = to_n_posted_entry(n1);
+	struct n_posted_entry *ent2 = to_n_posted_entry(n2);
+
+	if(ent1->dest_qpn < ent2->dest_qpn) {
+		return -1;
+	}
+	else if(ent1->dest_qpn > ent2->dest_qpn) {
+		return 1;
+	}
+	else
+		return 0;
+}
+
+static struct n_posted_entry *search_n_posted_entry(uint32_t dest_qpn,
+				struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct n_posted_entry target = {.dest_qpn = dest_qpn};
+	struct rb_node *match = ___search(&target.rb_node, &n_posted_tree, p_parent, p_insert,
+							SEARCH_EXACTLY, n_posted_entry_compare);
+	return to_n_posted_entry(match);
+}
+
+static uint64_t get_n_posted_from_dest_qpn(uint32_t dest_qpn) {
+	struct n_posted_entry *ent;
+	ent = search_n_posted_entry(dest_qpn, NULL, NULL);
+	if(!ent)
+		return -1;
+
+	return ent->n_posted;
+}
+
+static int add_n_posted_entry(uint32_t dest_qpn, uint64_t n_posted) {
+	struct n_posted_entry *ent;
+	struct rb_node *parent, **insert;
+
+	ent = search_n_posted_entry(dest_qpn, &parent, &insert);
+	if(ent) {
+		return 0;
+	}
+
+	ent = my_malloc(sizeof(*ent));
+	if(!ent)
+		return 0;
+
+	ent->dest_qpn = dest_qpn;
+	ent->n_posted = n_posted;
+	rbtree_add_node(&ent->rb_node, parent, insert, &n_posted_tree);
+
+	return 0;
+}
+
+static void free_n_posted_entry(struct rb_node *node) {
+	return;
+}
+
+static int count_qp(struct ibv_qp *qp,
+				void *entry, void *in_param) {
+	int *n_qp = (int *)in_param;
+	(*n_qp)++;
+	return 0;
+}
+
+static int fill_reply_buf(struct ibv_qp *qp,
+				void *entry, void *in_param) {
+	struct reply_hdr_fmt *reply_hdr = in_param;
+	int idx = reply_hdr->cnt;
+	struct reply_item_fmt *arr = &reply_hdr->msg;
+
+	arr[idx].qpn = qp->qp_num;
+	arr[idx].n_posted = get_ops(qp->context)->qp_get_n_posted(qp);
+	reply_hdr->cnt++;
+
+	return 0;
+}
+
+static void *migrside_wait_local_wqes(void *arg) {
+	struct list_head wait_qp_list;
+	struct list_head wait_srq_list;
+	struct wait_qp_ent *wait_qp_ent;
+	struct wait_qp_ent *wait_qp_tmp;
+	struct wait_srq_ent *wait_srq_ent;
+	struct wait_srq_ent *wait_srq_tmp;
+	int partner_buf_fd;
+	char fname[128];
+	void *buf = NULL;
+	char read_buf[1024];
+	ssize_t read_size = 0;
+	ssize_t cur_size;
+	struct reply_hdr_fmt *reply_hdr;
+	struct reply_item_fmt *arr;
+	struct n_posted_entry *n_posted_ent;
+	int total_cnt = 0;
+	int sig_0 = 0;
+	int fd;
+	FILE *fp;
+	char strln[1024];
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/partner_buf", __rdma_pid__);
+	partner_buf_fd = open(fname, O_RDONLY);
+
+	while(1) {
+		void *tmp_buf;
+
+		memset(read_buf, 0, 1024);
+		cur_size = read(partner_buf_fd, read_buf, 1024);
+		if(cur_size <= 0) {
+			break;
+		}
+
+		tmp_buf = my_malloc(read_size + cur_size);
+		memcpy(tmp_buf, buf, read_size);
+		memcpy(tmp_buf + read_size, read_buf, cur_size);
+
+		buf = tmp_buf;
+		tmp_buf = NULL;
+
+		read_size += cur_size;
+	}
+
+	close(partner_buf_fd);
+
+	reply_hdr = buf;
+	arr = (struct reply_item_fmt *)&reply_hdr->msg;
+	for(int i = 0; i < reply_hdr->cnt; i++) {
+		add_n_posted_entry(arr[i].qpn, arr[i].n_posted);
+	}
+
+	list_head_init(&wait_qp_list);
+	list_head_init(&wait_srq_list);
+	rbtree_traverse_qp(iter_qp_prepare_for_migr, &wait_qp_list);
+
+	list_for_each_safe(&wait_qp_list, wait_qp_ent, wait_qp_tmp, ent) {
+		struct ibv_qp *qp = wait_qp_ent->qp;
+		uint64_t n_posted = get_n_posted_from_dest_qpn(qp->dest_qpn);
+		uint64_t n_acked = get_ops(qp->context)->qp_get_n_acked(qp);
+		if(!qp->touched) {
+			qp->touched = 1;
+			if(qp->srq) {
+				if(n_posted != (uint64_t)-1)
+					qp->srq->cnt += n_posted;
+			}
+		}
+
+		if(n_posted == (uint64_t)-1)
+			n_posted = 0;
+
+		if(get_ops(qp->context)->migrrdma_is_q_empty(qp) &&
+						(qp->srq || n_posted <= n_acked)) {
+			list_del(&wait_qp_ent->ent);
+			qp->wait_qp_node = NULL;
+		}
+	}
+
+	while(!list_empty(&wait_qp_list)) {
+		rbtree_traverse_cq(iter_cq_poll_cq, NULL);
+	}
+
+	rbtree_traverse_qp(iter_qp_add_wait_list_r2, &wait_srq_list);
+	list_for_each_safe(&wait_srq_list, wait_srq_ent, wait_srq_tmp, ent) {
+		uint64_t n_acked;
+		struct ibv_srq *srq = wait_srq_ent->srq;
+
+
+		n_acked = get_ops(srq->context)->srq_get_n_acked(srq);
+		if(srq->cnt <= n_acked) {
+			list_del(&wait_srq_ent->ent);
+			srq->wait_srq_node = NULL;
+		}
+	}
+
+	while(!list_empty(&wait_srq_list)) {
+		rbtree_traverse_cq(iter_cq_poll_cq_r2, NULL);
+	}
+
+	rbtree_traverse_cq(iter_cq_end_poll, NULL);
+
+	clean_rbtree(&n_posted_tree, free_n_posted_entry);
+
+	rbtree_traverse_qp(count_qp, &total_cnt);
+	reply_hdr = my_malloc(sizeof(struct reply_hdr_fmt) +
+				total_cnt * sizeof(struct reply_item_fmt));
+	reply_hdr->cnt = 0;
+	rbtree_traverse_qp(fill_reply_buf, reply_hdr);
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/frm_buf", __rdma_pid__);
+	partner_buf_fd = open(fname, O_WRONLY);
+	write(partner_buf_fd, reply_hdr, sizeof(struct reply_hdr_fmt) +
+				reply_hdr->cnt * sizeof(struct reply_item_fmt));
+	close(partner_buf_fd);
+
+	rbtree_traverse_cq(iter_cq_uwrite, NULL);
+	rbtree_traverse_qp(iter_qp_uwrite, NULL);
+	rbtree_traverse_srq(iter_srq_uwrite, NULL);
+
+	clear_old_qpndict();
+	rbtree_traverse_qp(iter_add_old_qpndict, NULL);
+
+	for(int i = 0; i < n_rdma_mmaps; i++) {
+		munmap(rdma_mmaps[i].start,
+				rdma_mmaps[i].end - rdma_mmaps[i].start);
+	}
+	n_rdma_mmaps = 0;
+
+	fp = fopen("/proc/self/smaps", "r");
+	while(fgets(strln, 1024, fp) != NULL) {
+		unsigned long start, end;
+		off_t off;
+		char prots_and_flags[16];
+		char file[1024];
+		char arg[4][256];
+		int len = strlen(strln);
+
+		strln[len-1] = 0;
+		len--;
+		if(sscanf(strln, "%lx-%lx%ln", &start, &end, &off) < 2) {
+			continue;
+		}
+
+		if(sscanf(strln + off, "%s%s%s%s%s", prots_and_flags,
+					arg[0], arg[1], arg[2], file) < 5) {
+			continue;
+		}
+
+		if(strncmp(file, "/dev/infiniband/uverbs",
+					strlen("/dev/infiniband/uverbs"))) {
+			continue;
+		}
+
+		rdma_mmaps[n_rdma_mmaps].start		= start;
+		rdma_mmaps[n_rdma_mmaps].end		= end;
+		rdma_mmaps[n_rdma_mmaps].prot		= 0;
+		rdma_mmaps[n_rdma_mmaps].flag		= 0;
+		n_rdma_mmaps++;
+	}
+	fclose(fp);
+
+	free_all_my_memory();
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/to_frm", __rdma_pid__);
+	fd = open(fname, O_WRONLY);
+	if(fd < 0)
+		return;
+	
+	write(fd, &sig_0, sizeof(int));
+	close(fd);
+}
+
+static void do_sigtstp(int signo) {
+	pthread_t thread_id;
+	rbtree_traverse_cq(iter_cq_start_poll, NULL);
+	pthread_create(&thread_id, NULL, migrside_wait_local_wqes, NULL);
+}
+
+#define mv_fd(old_fd, new_fd) {									\
+	struct stat statbuf;										\
+	int __new_fd = (new_fd);									\
+	for(; !fstat(__new_fd, &statbuf); __new_fd++);				\
+																\
+	if(dup2(old_fd, __new_fd) < 0) {							\
+		if(old_fd >= 0)											\
+			close(old_fd);										\
+		old_fd = -1;											\
+	}															\
+	else {														\
+		close(old_fd);											\
+		old_fd = __new_fd;										\
+	}															\
+}
+
+#include <dirent.h>
+
+enum rdma_notify_ops {
+	RDMA_NOTIFY_PRE_ESTABLISH,
+	RDMA_NOTIFY_PRE_PAUSE,
+	RDMA_NOTIFY_RESTORE,
+};
+
+struct notify_message_fmt {
+	enum rdma_notify_ops			ops;
+	char							msg[0];
+};
+
+struct msg_fmt {
+	union ibv_gid					gid;
+	int								cnt;
+	char							msg[0];
+};
+
+static int switch_qp_cb(struct ibv_qp *orig_qp, struct ibv_qp *new_qp,
+				void *param) {
+//	struct ibv_qp *tmp;
+	void (*copy_qp)(struct ibv_qp *qp1, struct ibv_qp *qp2, void *param);
+	struct ibv_qp *(*calloc_qp)(void);
+
+	copy_qp = get_ops(orig_qp->context)->copy_qp;
+	calloc_qp = get_ops(orig_qp->context)->calloc_qp;
+
+//	tmp = calloc_qp();
+//	if(!tmp) {
+//		return -ENOMEM;
+//	}
+
+//	copy_qp(tmp, orig_qp);
+	pthread_rwlock_wrlock(&orig_qp->rwlock);
+	copy_qp(orig_qp, new_qp, param);
+	orig_qp->pause_flag = 0;
+	pthread_rwlock_unlock(&orig_qp->rwlock);
+//	ibv_resume_free_qp(new_qp);
+//	ibv_destroy_qp(tmp);
+
+	return 0;
+}
+
+static pid_t tid_list[8];
+static int curp = 0;
+static pthread_rwlock_t tid_list_rwlock = PTHREAD_RWLOCK_INITIALIZER;
+
+static void *pthread_pre_establish_qp(void *arg) {
+	struct msg_fmt *msg_fmt = arg;
+	uint32_t *qpn_arr;
+	int i, k;
+	struct ibv_qp **qp_list;
+	enum ibv_qp_state *qp_state_list;
+	struct ibv_qp_attr *attr_list;
+	int *attr_mask_list;
+
+	pthread_rwlock_wrlock(&tid_list_rwlock);
+	tid_list[curp] = gettid();
+//	dprintf(1, "cur tid: %d\n", gettid());
+	curp++;
+	pthread_rwlock_unlock(&tid_list_rwlock);
+
+	if(msg_fmt->cnt) {
+		qp_list = malloc(sizeof(*qp_list) * msg_fmt->cnt);
+		qp_state_list = malloc(sizeof(*qp_state_list) * msg_fmt->cnt);
+		attr_list = malloc(sizeof(*attr_list) * msg_fmt->cnt * 3);
+		attr_mask_list = malloc(sizeof(*attr_mask_list) * msg_fmt->cnt * 3);
+	}
+
+	qpn_arr = (uint32_t*)&msg_fmt->msg;
+	for(k = 0; k < msg_fmt->cnt; k++) {
+		struct ibv_qp 				*qp_meta;
+		struct ibv_qp 				*qp;
+		struct ibv_qp_init_attr		qp_init_attr;
+		enum ibv_qp_state			qp_state;
+
+		uint32_t					pqpn;
+		int							cmd_fd;
+		int							qp_vhandle;
+
+		char fname[128];
+		int fd;
+		int i;
+
+		pqpn = qpn_arr[k];
+		if(get_qpn_dict(pqpn, &cmd_fd, &qp_vhandle)) {
+			perror("get_qpn_dict");
+			continue;
+		}
+
+		sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/meta_uaddr",
+						__rdma_pid__, cmd_fd, qp_vhandle);
+		fd = open(fname, O_RDONLY);
+		read(fd, &qp_meta, sizeof(qp_meta));
+		close(fd);
+
+		sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/init_attr",
+						__rdma_pid__, cmd_fd, qp_vhandle);
+		fd = open(fname, O_RDONLY);
+		read(fd, &qp_init_attr, sizeof(qp_init_attr));
+		close(fd);
+
+		qp = ibv_pre_create_qp(qp_meta->pd, &qp_init_attr, qp_meta->qp_num);
+		if(!qp) {
+			perror("ibv_pre_create_qp");
+			continue;
+		}
+
+		if(add_switch_list_node(qp->real_qpn, qp_meta, qp)) {
+			perror("add_switch_list_node");
+			continue;
+		}
+
+		sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/cur_qp_state",
+						__rdma_pid__, cmd_fd, qp_vhandle);
+		fd = open(fname, O_RDONLY);
+		read(fd, &qp_state, sizeof(qp_state));
+		close(fd);
+
+		qp_list[k] = qp;
+		qp_state_list[k] = qp_state;
+		for(i = 0; i < qp_state; i++) {
+			struct ibv_qp_attr attr;
+			int attr_mask;
+
+			sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/attr_%d",
+							__rdma_pid__, cmd_fd, qp_vhandle, i);
+			fd = open(fname, O_RDONLY);
+			read(fd, &attr, sizeof(attr));
+			close(fd);
+
+			sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/mask_%d",
+							__rdma_pid__, cmd_fd, qp_vhandle, i);
+			fd = open(fname, O_RDONLY);
+			read(fd, &attr_mask, sizeof(attr_mask));
+			close(fd);
+
+			memcpy(attr_list + k * 3 + i, &attr, sizeof(attr));
+			attr_mask_list[k*3+i] = attr_mask;
+		}
+	}
+
+	for(k = 0; k < msg_fmt->cnt; k++) {
+		struct ibv_qp 				*qp;
+		enum ibv_qp_state			qp_state;
+
+		qp = qp_list[k];
+		qp_state = qp_state_list[k];
+		for(i = 0; i < qp_state; i++) {
+			struct ibv_qp_attr attr;
+			int attr_mask;
+
+			memcpy(&attr, attr_list + k * 3 + i, sizeof(attr));
+			attr_mask = attr_mask_list[k*3+i];
+
+			if(i+1 == IBV_QPS_RTR) {
+				memcpy(&attr.ah_attr.grh.dgid, &msg_fmt->gid, sizeof(union ibv_gid));
+			}
+
+			if(ibv_modify_qp(qp, &attr, attr_mask)) {
+				perror("ibv_modify_qp");
+				break;
+			}
+		}
+
+#if 0
+		if(i >= qp_state) {
+			printf("In %s(%d): Pre-create QP finished\n", __FILE__, __LINE__);
+		}
+#endif
+	}
+
+	if(msg_fmt->cnt) {
+		free(qp_list);
+		free(qp_state_list);
+		free(attr_list);
+		free(attr_mask_list);
+	}
+}
+
+static int iter_migrrdma_poll_cq(struct ibv_cq *cq, void *entry, void *in_param) {
+	struct ibv_wc *wc;
+	struct ibv_qp **qps;
+	int ne;
+
+	wc = calloc(cq->cqe, sizeof(*wc));
+	if(!wc)
+		return -ENOMEM;
+
+	qps = calloc(cq->cqe, sizeof(struct ibv_qp *));
+	if(!qps)
+		return -ENOMEM;
+
+	ne = get_ops(cq->context)->migrrdma_poll_cq(cq, cq->cqe, wc, qps, NULL);
+	free(wc);
+
+	if(ne < 0) {
+		free(qps);
+		return -1;
+	}
+
+	for(int i = 0; i < ne; i++) {
+		struct ibv_qp *qp = qps[i];
+		if(!qp)
+			continue;
+		if(get_ops(qp->context)->migrrdma_is_q_empty(qp)) {
+			if(qp->wait_qp_node) {
+				struct wait_qp_ent *wait_ent;
+				wait_ent = qp->wait_qp_node;
+				list_del(&wait_ent->ent);
+				qp->wait_qp_node = NULL;
+				free(wait_ent);
+			}
+		}
+	}
+
+	free(qps);
+	return 0;
+}
+
+static void *pthread_suspend_qp(void *arg) {
+	struct msg_fmt *msg_fmt = arg;
+	uint32_t *qpn_arr;
+	int k;
+	struct reply_hdr_fmt *reply_buf;
+	struct reply_item_fmt *reply_arr;
+
+	int sig_0 = 0;
+	int fd;
+	char fname[128];
+	int partner_buf_fd;
+
+	struct list_head wait_qp_list;
+	struct wait_qp_ent *wait_qp_ent;
+	struct wait_qp_ent *wait_qp_tmp;
+
+	pthread_rwlock_wrlock(&tid_list_rwlock);
+	tid_list[curp] = gettid();
+//	dprintf(1, "cur tid: %d\n", gettid());
+	curp++;
+	pthread_rwlock_unlock(&tid_list_rwlock);
+
+	list_head_init(&wait_qp_list);
+
+	reply_buf = malloc(sizeof(struct reply_hdr_fmt) +
+					msg_fmt->cnt * sizeof(struct reply_item_fmt));
+	reply_buf->cnt = msg_fmt->cnt;
+	reply_arr = (struct reply_item_fmt *)&reply_buf->msg;
+
+	qpn_arr = (uint32_t*)&msg_fmt->msg;
+	for(k = 0; k < msg_fmt->cnt; k++) {
+		struct ibv_qp 		*qp;
+		uint32_t			pqpn;
+		int					cmd_fd;
+		int					qp_vhandle;
+
+		char fname[128];
+		int fd;
+
+		pqpn = qpn_arr[k];
+		if(get_qpn_dict(pqpn, &cmd_fd, &qp_vhandle)) {
+			perror("get_qpn_dict");
+			continue;
+		}
+
+		sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/meta_uaddr",
+					__rdma_pid__, cmd_fd, qp_vhandle);
+		fd = open(fname, O_RDONLY);
+		read(fd, &qp, sizeof(qp));
+		close(fd);
+
+		wait_qp_ent = calloc(1, sizeof(*wait_qp_ent));
+		wait_qp_ent->qp = qp;
+
+		pthread_rwlock_wrlock(&qp->rwlock);
+		qp->pause_flag = 1;
+		get_ops(qp->context)->migrrdma_start_inspect_qp(qp);
+		list_add(&wait_qp_list, &wait_qp_ent->ent);
+		qp->wait_qp_node = wait_qp_ent;
+		pthread_rwlock_unlock(&qp->rwlock);
+	}
+
+	list_for_each_safe(&wait_qp_list, wait_qp_ent, wait_qp_tmp, ent) {
+		if(get_ops(wait_qp_ent->qp->context)->migrrdma_is_q_empty(wait_qp_ent->qp)) {
+			list_del(&wait_qp_ent->ent);
+			wait_qp_ent->qp->wait_qp_node = NULL;
+			free(wait_qp_ent);
+		}
+	}
+
+	while(!list_empty(&wait_qp_list)) {
+		rbtree_traverse_cq(iter_migrrdma_poll_cq, NULL);
+	}
+
+	qpn_arr = (uint32_t*)&msg_fmt->msg;
+	for(k = 0; k < msg_fmt->cnt; k++) {
+		struct ibv_qp 		*qp;
+		uint32_t			pqpn;
+		int					cmd_fd;
+		int					qp_vhandle;
+
+		char fname[128];
+		int fd;
+
+		pqpn = qpn_arr[k];
+		if(get_qpn_dict(pqpn, &cmd_fd, &qp_vhandle)) {
+			perror("get_qpn_dict");
+			continue;
+		}
+
+		sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/meta_uaddr",
+					__rdma_pid__, cmd_fd, qp_vhandle);
+		fd = open(fname, O_RDONLY);
+		read(fd, &qp, sizeof(qp));
+		close(fd);
+
+		get_ops(qp->context)->migrrdma_end_poll(qp->send_cq);
+		get_ops(qp->context)->migrrdma_end_poll(qp->recv_cq);
+
+		reply_arr[k].qpn = pqpn;
+		reply_arr[k].n_posted = get_ops(qp->context)->qp_get_n_posted(qp);
+	}
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/frm_buf", __rdma_pid__);
+	partner_buf_fd = open(fname, O_WRONLY);
+	write(partner_buf_fd, reply_buf, sizeof(struct reply_hdr_fmt) +
+				msg_fmt->cnt * sizeof(struct reply_item_fmt));
+	close(partner_buf_fd);
+	free(reply_buf);
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/to_frm", __rdma_pid__);
+	fd = open(fname, O_WRONLY);
+	if(fd < 0)
+		return;
+
+	write(fd, &sig_0, sizeof(int));
+	close(fd);
+}
+
+static void *pthread_switch_qp(void *arg) {
+	struct msg_fmt *msg_fmt = arg;
+	struct reply_item_fmt *qpn_arr;
+	int k;
+
+	pthread_rwlock_wrlock(&tid_list_rwlock);
+	tid_list[curp] = gettid();
+//	dprintf(1, "cur tid: %d\n", gettid());
+	curp++;
+	pthread_rwlock_unlock(&tid_list_rwlock);
+
+	qpn_arr = (struct reply_item_fmt *)&msg_fmt->msg;
+	for(k = 0; k < msg_fmt->cnt; k++) {
+		switch_to_new_qp(qpn_arr[k].qpn, &qpn_arr[k].n_posted,
+							switch_qp_cb);
+	}
+
+	pthread_rwlock_wrlock(&tid_list_rwlock);
+	curp = 0;
+	pthread_rwlock_unlock(&tid_list_rwlock);
+}
+
+static void migrrdma_start_poll(struct ibv_cq *ibcq) {
+	get_ops(ibcq->context)->migrrdma_start_poll(ibcq);
+}
+
+static int iter_start_poll_cq(struct ibv_cq *cq, void *entry, void *in_param) {
+	migrrdma_start_poll(cq);
+	return 0;
+}
+
+static void do_sigusr2(int signo, int partner_buf_fd);
+
+static pthread_rwlock_t sigusr2_rwlock = PTHREAD_RWLOCK_INITIALIZER;
+static int sigusr2_n_threads = 0;
+
+static void sigusr2_handler(int signo) {
+	if(getpid() == gettid()) {
+	int sig;
+	int fd;
+	int sig_fd;
+	int partner_buf_fd;
+	char fname[128];
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/to_frm", __rdma_pid__);
+	sig_fd = open(fname, O_RDWR);
+	sprintf(fname, "/proc/rdma_uwrite/%d/partner_buf", __rdma_pid__);
+	partner_buf_fd = open(fname, O_RDONLY);
+	if(sig_fd < 0 || partner_buf_fd < 0) {
+		if(sig_fd >= 0)
+			close(sig_fd);
+		return;
+	}
+
+	read(sig_fd, &sig, sizeof(int));
+	close(sig_fd);
+
+		/* Make sure all calling threads exit only after the main thread exit */
+		pthread_rwlock_wrlock(&sigusr2_rwlock);
+		xadd(&sigusr2_n_threads, sig);
+
+		/* Wait until all threads enter handler function */
+		while(1) {
+			int tmp = atomic_get(&sigusr2_n_threads);
+			if(tmp == 0)
+				break;
+		}
+
+		do_sigusr2(signo, partner_buf_fd);
+}
+	else {
+//		dprintf(1, "%s: cur tid: %d\n", __func__, gettid());
+		pthread_rwlock_rdlock(&tid_list_rwlock);
+		for(int i = 0; i < curp; i++) {
+			if(tid_list[i] == gettid()) {
+				pthread_rwlock_unlock(&tid_list_rwlock);
+				xadd(&sigusr2_n_threads, -1);
+				return;
+			}
+		}
+		pthread_rwlock_unlock(&tid_list_rwlock);
+
+		/* Wait for main thread to finish prepare work */
+		for(int i = 0; i < curp; i++)
+		while(1) {
+			int tmp = atomic_get(&sigusr2_n_threads);
+			if(tmp > 0) {
+				break;
+			}
+		}
+
+		atomic_dec(&sigusr2_n_threads);
+		/* Wait for the main thread to finish */
+		pthread_rwlock_rdlock(&sigusr2_rwlock);
+	}
+
+	pthread_rwlock_unlock(&sigusr2_rwlock);
+	atomic_set(&sigusr2_n_threads, 0);
+}
+
+static void do_sigusr2(int signo, int partner_buf_fd) {
+	int sig;
+	int fd;
+	int sig_fd;
+	char fname[128];
+	struct ibv_qp *qp;
+	enum ibv_qp_state qp_state;
+	struct ibv_qp_attr qp_attr;
+	int attr_mask;
+	uint32_t qpn;
+	void *buf = NULL;
+	void *read_buf = NULL;
+	ssize_t read_size = 0;
+	ssize_t cur_size;
+	struct notify_message_fmt *header;
+	struct msg_fmt *msg_fmt;
+
+	while(1) {
+		void *tmp_buf;
+
+		read_buf = malloc(1024);
+		if(!read_buf) {
+			if(buf)
+				free(buf);
+			close(partner_buf_fd);
+			return -1;
+		}
+
+		memset(read_buf, 0, 1024);
+		cur_size = read(partner_buf_fd, read_buf, 1024);
+		if(cur_size < 0) {
+			free(read_buf);
+			if(buf)
+				free(buf);
+			close(partner_buf_fd);
+			perror("read");
+			return;
+		}
+		if(cur_size == 0) {
+			free(read_buf);
+			break;
+		}
+
+		tmp_buf = malloc(read_size + cur_size);
+		if(!tmp_buf) {
+			free(read_buf);
+			if(buf)
+				free(buf);
+			close(partner_buf_fd);
+			perror("malloc");
+			return;
+		}
+
+		memcpy(tmp_buf, buf, read_size);
+		memcpy(tmp_buf + read_size, read_buf, cur_size);
+
+		free(read_buf);
+		if(buf)
+			free(buf);
+		buf = tmp_buf;
+		tmp_buf = NULL;
+	
+		read_size += cur_size;
+	}
+
+	close(partner_buf_fd);
+
+	header = (struct notify_message_fmt *)buf;
+	msg_fmt = (struct msg_fmt *)(header + 1);
+
+	if(header->ops == RDMA_NOTIFY_PRE_ESTABLISH) {
+		pthread_t thread_id;
+		if(pthread_create(&thread_id, NULL,
+					pthread_pre_establish_qp, msg_fmt)) {
+			perror("pthread_create");
+		}
+	}
+	else if(header->ops == RDMA_NOTIFY_PRE_PAUSE) {
+		pthread_t thread_id;
+		uint32_t *qpn_arr;
+
+		qpn_arr = (uint32_t *)&msg_fmt->msg;
+		for(int k = 0; k < msg_fmt->cnt; k++) {
+			struct ibv_qp 		*qp;
+			uint32_t			pqpn;
+			int					cmd_fd;
+			int					qp_vhandle;
+
+			char fname[128];
+			int fd;
+
+			pqpn = qpn_arr[k];
+			if(get_qpn_dict(pqpn, &cmd_fd, &qp_vhandle)) {
+				perror("get_qpn_dict");
+				continue;
+			}
+
+			sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/meta_uaddr",
+						__rdma_pid__, cmd_fd, qp_vhandle);
+			fd = open(fname, O_RDONLY);
+			read(fd, &qp, sizeof(qp));
+			close(fd);
+
+			get_ops(qp->context)->migrrdma_start_poll(qp->send_cq);
+			get_ops(qp->context)->migrrdma_start_poll(qp->recv_cq);
+		}
+
+		if(pthread_create(&thread_id, NULL,
+					pthread_suspend_qp, msg_fmt)) {
+			perror("pthread_create");
+		}
+	}
+	else if(header->ops == RDMA_NOTIFY_RESTORE) {
+		pthread_t thread_id;
+		if(pthread_create(&thread_id, NULL,
+					pthread_switch_qp, msg_fmt)) {
+			perror("pthread_create");
+		}
+	}
+}
+
+LATEST_SYMVER_FUNC(rdma_getpid, 1_1, "IBVERBS_1.1",
+			pid_t, struct ibv_context *context) {
+	return ibv_cmd_get_rdma_pid(context);
+}
+
 LATEST_SYMVER_FUNC(ibv_open_device, 1_1, "IBVERBS_1.1",
 		   struct ibv_context *,
 		   struct ibv_device *device)
 {
-	return verbs_open_device(device, NULL);
+	struct ibv_context *ctx;
+
+	signal(SIGTSTP, sigtstp_handler);
+	signal(SIGUSR2, sigusr2_handler);
+	ctx = verbs_open_device(device, NULL);
+
+	if(ctx)
+		__rdma_pid__ = rdma_getpid(ctx);
+
+#if 0
+	if(rbtree_add_context(ctx)) {
+		ibv_close_device(ctx);
+		return NULL;
+	}
+#endif
+
+	return ctx;
+}
+
+static int get_abs_path(char *abs_path, size_t bufsiz, int cmd_fd) {
+	int err;
+	char slink_name[128];
+
+	sprintf(slink_name, "/proc/self/fd/%d", cmd_fd);
+	err = readlink(slink_name, abs_path, bufsiz);
+	if(err < 0) {
+		return -1;
+	}
+
+	abs_path[err] = 0;
+
+	return 0;
+}
+
+struct verbs_device *get_verbs_device(struct ibv_device **p_ib_dev,
+				struct ibv_device **dev_list, int cmd_fd) {
+	char abs_path[128], dev_path[128];
+	struct verbs_device *verbs_device;
+	struct ibv_device *ib_dev = NULL;
+	int err;
+
+	err = get_abs_path(abs_path, sizeof(abs_path), cmd_fd);
+	if(err)
+		return NULL;
+
+	for(; (ib_dev = *dev_list); ++dev_list) {
+		verbs_device = verbs_get_device(ib_dev);
+		sprintf(dev_path, "/dev/infiniband/%s",
+					verbs_device->sysfs->sysfs_name);
+		if(!strcmp(dev_path, abs_path)) {
+			break;
+		}
+	}
+
+	if(!ib_dev) {
+		free(dev_list);
+		return NULL;
+	}
+
+	if(p_ib_dev)
+		*p_ib_dev = ib_dev;
+
+	return verbs_device;
+}
+
+LATEST_SYMVER_FUNC(ibv_free_tmp_context, 1_1, "IBVERBS_1.1",
+			void, struct ibv_context *context) {
+	struct verbs_device *verbs_device;
+
+	verbs_device = verbs_get_device(context->device);
+	verbs_device->ops->free_tmp_context(context);
+}
+
+LATEST_SYMVER_FUNC(ibv_resume_context, 1_1, "IBVERBS_1.1",
+			struct ibv_context *, struct ibv_device **dev_list,
+			const struct ibv_resume_context_param *context_param) {
+	int ctx_cmd_fd = -1;
+	struct verbs_device *verbs_device;
+	struct ibv_device *ib_dev = NULL;
+	struct verbs_context *context_ex;
+	int context_dir_fd;
+	int info_fd;
+	char fname[128];
+	int ctx_async_fd = -1;
+	int ret;
+
+	signal_flag = 0;
+
+	sprintf(fname, "/dev/infiniband/%s", context_param->cdev);
+	ctx_cmd_fd = open(fname, O_RDWR | O_CLOEXEC);
+	if(ctx_cmd_fd < 0) {
+		return NULL;
+	}
+
+	if(ctx_cmd_fd != context_param->cmd_fd &&
+					dup2(ctx_cmd_fd, context_param->cmd_fd) < 0) {
+		close(ctx_cmd_fd);
+		return NULL;
+	}
+
+	if(ctx_cmd_fd != context_param->cmd_fd) {
+		close(ctx_cmd_fd);
+	}
+
+	verbs_device = get_verbs_device(&ib_dev, dev_list, context_param->cmd_fd);
+	if(!verbs_device) {
+		close(context_param->cmd_fd);
+		return NULL;
+	}
+
+	context_ex = verbs_device->ops->pre_resume_context(ib_dev, context_param->cmd_fd);
+	if(!context_ex) {
+		return NULL;
+	}
+
+	sprintf(fname, "/proc/rdma/%d/%d", rdma_getpid(&context_ex->context), context_ex->context.cmd_fd);
+	context_dir_fd = open(fname, O_DIRECTORY);
+	if(context_dir_fd < 0) {
+		ibv_close_device(&context_ex->context);
+		return -1;
+	}
+
+	if(dup2(context_dir_fd, context_dir_fd + 1000) < 0) {
+		close(context_dir_fd);
+		ibv_close_device(&context_ex->context);
+		return -1;
+	}
+
+	close(context_dir_fd);
+	context_dir_fd = context_dir_fd + 1000;
+
+#if 0
+	resume_info(context_param, context_dir_fd, info_fd, gid_table,									\
+					ibv_close_device(&(context_ex)->context), -1);
+#endif
+
+	close(context_dir_fd);
+	ibv_free_tmp_context(&context_ex->context);
+	context_ex = verbs_device->ops->resume_context(ib_dev,
+					context_param->cmd_fd, &ctx_async_fd,
+					context_param->ctx_uaddr);
+
+	if(ctx_async_fd < 0) {
+		close(context_param->cmd_fd);
+		return NULL;
+	}
+
+	set_lib_ops(context_ex);
+
+	if(ctx_async_fd != context_param->async_fd &&
+					dup2(ctx_async_fd, context_param->async_fd) < 0) {
+		ibv_close_device(&context_ex->context);
+		return NULL;
+	}
+
+	if(ctx_async_fd != context_param->async_fd)
+		close(ctx_async_fd);
+
+	context_ex->context.async_fd = context_param->async_fd;
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/ctx_uaddr",
+			rdma_getpid(&context_ex->context), context_ex->context.cmd_fd);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_close_device(&context_ex->context);
+		return NULL;
+	}
+
+	if(write(info_fd, &context_param->ctx_uaddr, sizeof(context_ex)) < 0) {
+		close(info_fd);
+		ibv_close_device(&context_ex->context);
+		return NULL;
+	}
+
+	close(info_fd);
+	return &context_ex->context;
 }
 
 struct ibv_context *ibv_import_device(int cmd_fd)
@@ -440,6 +1875,8 @@ LATEST_SYMVER_FUNC(ibv_close_device, 1_1, "IBVERBS_1.1",
 {
 	const struct verbs_context_ops *ops = get_ops(context);
 
+	rbtree_del_context(context);
+
 	ops->free_context(context);
 	return 0;
 }
diff --git a/libibverbs/driver.h b/libibverbs/driver.h
index 8b2e045..811cd19 100644
--- a/libibverbs/driver.h
+++ b/libibverbs/driver.h
@@ -47,8 +47,14 @@
 #include <infiniband/cmd_ioctl.h>
 #include <sys/types.h>
 
+extern int __rdma_pid__;
+extern int signal_flag;
+
 struct verbs_device;
 
+extern void free_all_my_memory(void);
+extern void *my_malloc(size_t size);
+
 enum verbs_xrcd_mask {
 	VERBS_XRCD_HANDLE	= 1 << 0,
 	VERBS_XRCD_RESERVED	= 1 << 1
@@ -234,6 +240,13 @@ struct verbs_device_ops {
 
 	struct verbs_device *(*alloc_device)(struct verbs_sysfs_dev *sysfs_dev);
 	void (*uninit_device)(struct verbs_device *device);
+
+	struct verbs_context *(*resume_context)(struct ibv_device *ibdev, int cmd_fd,
+						int *async_fd, struct verbs_context *orig_ctx);
+	struct verbs_context *(*pre_resume_context)(struct ibv_device *ibdev, int cmd_fd);
+	void (*free_tmp_context)(struct ibv_context *context);
+	struct ibv_context *(*dup_context)(struct ibv_context *context, struct ibv_qp *qp);
+	struct verbs_context *(*get_ops_context)(struct ibv_device *ibdev, int cmd_fd);
 };
 
 /* Must change the PRIVATE IBVERBS_PRIVATE_ symbol if this is changed */
@@ -395,6 +408,44 @@ struct verbs_context_ops {
 	void (*unimport_dm)(struct ibv_dm *dm);
 	void (*unimport_mr)(struct ibv_mr *mr);
 	void (*unimport_pd)(struct ibv_pd *pd);
+
+	int (*uwrite_cq)(struct ibv_cq *cq, int cq_dir_fd);
+	struct ibv_cq *(*resume_cq)(struct ibv_context *context, struct ibv_cq *cq_meta,
+			int cqe, struct ibv_comp_channel *channel, int comp_vector,
+			void *buf_addr, void *db_addr, int vhandle);
+	int (*get_cons_index)(struct ibv_cq *cq);
+	void (*set_cons_index)(struct ibv_cq *cq, int cons_index);
+	void (*copy_cqe_to_shaded)(struct ibv_cq *cq);
+
+	int (*uwrite_qp)(struct ibv_qp *qp, struct ibv_qp *new_qp);
+	struct ibv_qp *(*resume_qp)(struct ibv_context *context, int pd_handle, int qp_handle,
+					struct ibv_qp_init_attr *attr, void *buf_addr, void *db_addr,
+					int32_t usr_idx, struct ibv_qp *orig_qp, unsigned long long *bf_reg);
+	void (*free_qp)(struct ibv_qp *qp);
+	int (*is_q_empty)(struct ibv_qp *qp);
+	void (*copy_qp)(struct ibv_qp *qp1, struct ibv_qp *qp2, void *param);
+	struct ibv_qp *(*calloc_qp)(void);
+	int (*replay_recv_wr)(struct ibv_qp *qp);
+	int (*prepare_qp_recv_replay)(struct ibv_qp *qp, struct ibv_qp *new_qp);
+	void (*record_qp_index)(struct ibv_qp *qp);
+
+	struct ibv_srq *(*resume_srq)(struct ibv_pd *pd, struct ibv_resume_srq_param *param);
+
+	void (*migrrdma_start_poll)(struct ibv_cq *cq);
+	void (*migrrdma_end_poll)(struct ibv_cq *cq);
+	int (*migrrdma_poll_cq)(struct ibv_cq *cq, int ne,
+				struct ibv_wc *wc,
+				struct ibv_qp **qps, struct ibv_srq **srqs);
+	void (*migrrdma_start_inspect_qp)(struct ibv_qp *qp);
+	void (*migrrdma_start_inspect_qp_v2)(struct ibv_qp *qp);
+	int (*migrrdma_is_q_empty)(struct ibv_qp *qp);
+	uint64_t (*qp_get_n_posted)(struct ibv_qp *qp);
+	uint64_t (*qp_get_n_acked)(struct ibv_qp *qp);
+	uint64_t (*srq_get_n_acked)(struct ibv_srq *srq);
+
+	int (*uwrite_srq)(struct ibv_srq *srq, struct ibv_srq *new_srq);
+	int (*replay_srq_recv_wr)(struct ibv_srq *srq, int head, int tail);
+	int (*prepare_srq_replay)(struct ibv_srq *srq, struct ibv_srq *new_srq, int *head, int *tail);
 };
 
 static inline struct verbs_device *
@@ -453,6 +504,8 @@ void verbs_init_cq(struct ibv_cq *cq, struct ibv_context *context,
 		       struct ibv_comp_channel *channel,
 		       void *cq_context);
 
+int ibv_get_signal(void);
+
 struct ibv_context *verbs_open_device(struct ibv_device *device,
 				      void *private_data);
 int ibv_cmd_get_context(struct verbs_context *context,
@@ -475,6 +528,125 @@ int ibv_cmd_query_device_any(struct ibv_context *context,
 int ibv_cmd_query_port(struct ibv_context *context, uint8_t port_num,
 		       struct ibv_port_attr *port_attr,
 		       struct ibv_query_port *cmd, size_t cmd_size);
+
+#define resume_info(resume_param, dir_fd, info_fd, info_name, do_destroy, ret)						\
+	info_fd = openat(dir_fd, #info_name, O_WRONLY);													\
+	if(info_fd < 0) {																				\
+		close(dir_fd);																				\
+		do_destroy;																					\
+		return ret;																					\
+	}																								\
+																									\
+	if(write(info_fd, &(resume_param)->info_name, sizeof((resume_param)->info_name)) < 0) {			\
+		close(info_fd);																				\
+		close(dir_fd);																				\
+		do_destroy;																					\
+		return ret;																					\
+	}																								\
+																									\
+	close(info_fd)
+
+struct ibv_qp *ibv_pre_create_qp(struct ibv_pd *pd,
+		struct ibv_qp_init_attr *qp_init_attr, uint32_t vqpn);
+
+int add_bf_addr_map_entry(void *new_bf, void *alloc_bf);
+void *get_alloc_bf(void *new_bf);
+
+int ibv_cmd_install_footprint(struct ibv_context *context);
+int ibv_cmd_install_qpndict(struct ibv_context *context,
+				uint32_t real_qpn, uint32_t vqpn);
+int ibv_cmd_install_ctx_resp(struct ibv_context *context, void *resp, size_t size);
+int ibv_cmd_register_async_fd(struct ibv_context *context, int async_fd);
+int ibv_cmd_install_pd_handle_mapping(struct ibv_context *context,
+							int vhandle, int handle);
+int ibv_cmd_install_cq_handle_mapping(struct ibv_context *context,
+							int vhandle, int handle);
+int ibv_cmd_install_mr_handle_mapping(struct ibv_context *context,
+							int vhandle, int handle);
+int ibv_cmd_install_qp_handle_mapping(struct ibv_context *context,
+							int vhandle, int handle);
+int ibv_cmd_install_srq_handle_mapping(struct ibv_context *context,
+							int vhandle, int handle);
+int ibv_cmd_install_lkey_mapping(struct ibv_context *context,
+							uint32_t vlkey, uint32_t lkey);
+int ibv_cmd_install_local_rkey_mapping(struct ibv_context *context,
+							uint32_t vrkey, uint32_t rkey);
+int ibv_cmd_delete_local_rkey_mapping(struct ibv_context *context, uint32_t vrkey);
+int ibv_cmd_delete_lkey_mapping(struct ibv_context *context, uint32_t vlkey);
+int ibv_cmd_register_remote_gid_pid(struct ibv_context *context,
+					const union ibv_gid *gid, pid_t pid);
+int ibv_cmd_update_comp_channel_fd(struct ibv_context *context,
+					struct ibv_comp_channel *channel);
+int ibv_cmd_get_rdma_pid(struct ibv_context *context);
+//int restore_rdma(void);
+
+struct gid_vid_key_value_type {
+	union ibv_gid					gid;
+	uint32_t						vid;
+};
+
+static inline int gid_vid_compare(struct gid_vid_key_value_type o1, struct gid_vid_key_value_type o2) {
+	int i;
+	for(i = 0; i < 16; i++) {
+		if(o1.gid.raw[i] < o2.gid.raw[i])
+			return -1;
+		else if(o1.gid.raw[i] > o2.gid.raw[i])
+			return 1;
+	}
+
+	if(o1.vid < o2.vid)
+		return -1;
+	else if(o1.vid > o2.vid)
+		return 1;
+	else
+		return 0;
+}
+
+int rbtree_search_cq(struct ibv_cq *cq);
+int rbtree_add_cq(struct ibv_cq *cq);
+void rbtree_del_cq(struct ibv_cq *cq);
+int rbtree_search_qp(struct ibv_qp *qp);
+int rbtree_add_qp(struct ibv_qp *qp);
+void rbtree_del_qp(struct ibv_qp *qp);
+int rbtree_search_srq(struct ibv_srq *srq);
+int rbtree_add_srq(struct ibv_srq *srq);
+void rbtree_del_srq(struct ibv_srq *srq);
+int rbtree_traverse_cq(int (*iter_cq_fn)(struct ibv_cq *cq,
+				void *entry, void *in_param), void *in_param);
+int rbtree_traverse_qp(int (*iter_qp_fn)(struct ibv_qp *qp,
+				void *entry, void *in_param), void *in_param);
+int rbtree_traverse_srq(int (*iter_srq_fn)(struct ibv_srq *srq,
+				void *entry, void *in_param), void *in_param);
+int rbtree_search_context(struct ibv_context *ctx);
+int rbtree_add_context(struct ibv_context *ctx);
+void rbtree_del_context(struct ibv_context *ctx);
+int rbtree_traverse_context(int (*iter_context_fn)(struct ibv_context *ctx,
+					void *entry, void *in_param), void *in_param);
+int add_qpn_dict_node(struct ibv_qp *qp);
+int get_qpn_dict(uint32_t pqpn, int *cmd_fd, int *qp_vhandle);
+void del_qpn_dict_node(struct ibv_qp *qp);
+int add_switch_list_node(uint32_t pqpn, struct ibv_qp *orig_qp, struct ibv_qp *new_qp);
+int switch_to_new_qp(uint32_t pqpn, void *param,
+				int (*switch_cb)(struct ibv_qp *orig_qp,
+				struct ibv_qp *new_qp,
+				void *param));
+int switch_all_qps(int (*switch_cb)(struct ibv_qp *orig_qp, struct ibv_qp *new_qp),
+				int (*load_cb)(struct ibv_qp *orig_qp, void *replay_fn));
+int add_srq_switch_node(struct ibv_srq *new_srq, struct ibv_srq *orig_srq);
+int switch_all_srqs(int (*switch_cb)(struct ibv_srq *orig_srq, struct ibv_srq *new_srq, int *head, int *tail),
+			int (*srq_load_cb)(struct ibv_srq *orig_srq, void *replay_fn, int head, int tail));
+int add_old_dict_node(struct ibv_qp *qp,
+				uint32_t real_qpn, uint32_t virt_qpn);
+int get_vqpn_from_old(uint32_t real_qpn, uint32_t *vqpn);
+void clear_old_qpndict(void);
+struct ibv_comp_channel *get_comp_channel_from_fd(int fd);
+int add_comp_channel(int fd, struct ibv_comp_channel *channel);
+int register_update_mem(void *ptr, size_t size, void *content_p);
+int register_keep_mmap_region(void *ptr, size_t size);
+int update_all_mem(int (*update_mem_fn)(void *ptr, size_t size,
+								void *content_p));
+int keep_all_mmap(int (*keep_mmap_fn)(unsigned long long start,
+								unsigned long long end));
 int ibv_cmd_alloc_async_fd(struct ibv_context *context);
 int ibv_cmd_alloc_pd(struct ibv_context *context, struct ibv_pd *pd,
 		     struct ibv_alloc_pd *cmd, size_t cmd_size,
diff --git a/libibverbs/dummy_ops.c b/libibverbs/dummy_ops.c
index bf70775..7ac74d1 100644
--- a/libibverbs/dummy_ops.c
+++ b/libibverbs/dummy_ops.c
@@ -137,6 +137,79 @@ static struct ibv_cq *create_cq(struct ibv_context *context, int cqe,
 	return NULL;
 }
 
+static struct ibv_cq *resume_cq(struct ibv_context *context, struct ibv_cq *cq_meta,
+			int cqe, struct ibv_comp_channel *channel, int comp_vector,
+			void *buf_addr, void *db_addr, int vhandle) {
+	errno = EOPNOTSUPP;
+	return NULL;
+}
+
+static int uwrite_cq(struct ibv_cq *cq, int cq_dir_fd) {
+	errno = EOPNOTSUPP;
+	return -1;
+}
+
+static int uwrite_qp(struct ibv_qp *qp, struct ibv_qp *new_qp) {
+	errno = EOPNOTSUPP;
+	return -1;
+}
+
+static int uwrite_srq(struct ibv_srq *srq, struct ibv_srq *new_srq) {
+	errno = EOPNOTSUPP;
+	return -1;
+}
+
+static int get_cons_index(struct ibv_cq *cq) {
+	return -1;
+}
+
+static void set_cons_index(struct ibv_cq *cq, int cons_index) {
+	return;
+}
+
+static void copy_cqe_to_shaded(struct ibv_cq *cq) {
+	return;
+}
+
+static void record_qp_index(struct ibv_qp *qp) {
+	return;
+}
+
+static void migrrdma_start_poll(struct ibv_cq *cq) {
+	return;
+}
+
+static void migrrdma_end_poll(struct ibv_cq *cq) {
+	return;
+}
+
+static uint64_t qp_get_n_posted(struct ibv_qp *qp) {
+	return -1;
+}
+
+static uint64_t qp_get_n_acked(struct ibv_qp *qp) {
+	return -1;
+}
+
+static uint64_t srq_get_n_acked(struct ibv_srq *srq) {
+	return -1;
+}
+
+void migrrdma_start_inspect_qp(struct ibv_qp *qp) {
+	return;
+}
+
+void migrrdma_start_inspect_qp_v2(struct ibv_qp *qp) {
+	return;
+}
+
+static int migrrdma_poll_cq(struct ibv_cq *cq, int ne,
+			struct ibv_wc *wc, struct ibv_qp **qps,
+			struct ibv_srq **srqs) {
+	errno = EOPNOTSUPP;
+	return -1;
+}
+
 static struct ibv_cq_ex *create_cq_ex(struct ibv_context *context,
 				      struct ibv_cq_init_attr_ex *init_attr)
 {
@@ -165,6 +238,55 @@ static struct ibv_qp *create_qp(struct ibv_pd *pd,
 	return NULL;
 }
 
+static struct ibv_qp *resume_qp(struct ibv_context *context, int pd_handle, int qp_handle,
+					struct ibv_qp_init_attr *attr, void *buf_addr, void *db_addr,
+					int32_t usr_idx, struct ibv_qp *orig_qp, unsigned long long *bf_reg) {
+	errno = EOPNOTSUPP;
+	return NULL;
+}
+
+static void free_qp(struct ibv_qp *qp) {
+	return;
+}
+
+static int is_q_empty(struct ibv_qp *qp) {
+	return 1;
+}
+
+static struct ibv_srq *resume_srq(struct ibv_pd *pd, struct ibv_resume_srq_param *param) {
+	errno = EOPNOTSUPP;
+	return NULL;
+}
+
+static int migrrdma_is_q_empty(struct ibv_qp *qp) {
+	return 1;
+}
+
+static void copy_qp(struct ibv_qp *qp1, struct ibv_qp *qp2,
+					void *param) {
+	return;
+}
+
+static struct ibv_qp *calloc_qp(void) {
+	return NULL;
+}
+
+static int replay_recv_wr(struct ibv_qp *qp) {
+	return -1;
+}
+
+static int prepare_qp_recv_replay(struct ibv_qp *qp, struct ibv_qp *new_qp) {
+	return -1;
+}
+
+static int replay_srq_recv_wr(struct ibv_srq *srq, int head, int tail) {
+	return -1;
+}
+
+static int prepare_srq_replay(struct ibv_srq *srq, struct ibv_srq *new_srq, int *head, int *tail) {
+	return -1;
+}
+
 static struct ibv_qp *create_qp_ex(struct ibv_context *context,
 				   struct ibv_qp_init_attr_ex *qp_init_attr_ex)
 {
@@ -579,6 +701,33 @@ const struct verbs_context_ops verbs_dummy_ops = {
 	unimport_dm,
 	unimport_mr,
 	unimport_pd,
+	uwrite_cq,
+	resume_cq,
+	get_cons_index,
+	set_cons_index,
+	copy_cqe_to_shaded,
+	uwrite_qp,
+	resume_qp,
+	free_qp,
+	is_q_empty,
+	copy_qp,
+	calloc_qp,
+	replay_recv_wr,
+	prepare_qp_recv_replay,
+	record_qp_index,
+	resume_srq,
+	migrrdma_start_poll,
+	migrrdma_end_poll,
+	migrrdma_poll_cq,
+	migrrdma_start_inspect_qp,
+	migrrdma_start_inspect_qp_v2,
+	migrrdma_is_q_empty,
+	qp_get_n_posted,
+	qp_get_n_acked,
+	srq_get_n_acked,
+	uwrite_srq,
+	replay_srq_recv_wr,
+	prepare_srq_replay,
 };
 
 /*
@@ -705,6 +854,38 @@ void verbs_set_ops(struct verbs_context *vctx,
 	SET_PRIV_OP_IC(vctx, unimport_mr);
 	SET_PRIV_OP_IC(vctx, unimport_pd);
 
+	SET_PRIV_OP(ctx, uwrite_cq);
+	SET_PRIV_OP(ctx, resume_cq);
+	SET_PRIV_OP(ctx, get_cons_index);
+	SET_PRIV_OP(ctx, set_cons_index);
+	SET_PRIV_OP(ctx, copy_cqe_to_shaded);
+
+	SET_PRIV_OP(ctx, uwrite_qp);
+	SET_PRIV_OP(ctx, resume_qp);
+	SET_PRIV_OP(ctx, free_qp);
+	SET_PRIV_OP(ctx, is_q_empty);
+	SET_PRIV_OP(ctx, copy_qp);
+	SET_PRIV_OP(ctx, calloc_qp);
+	SET_PRIV_OP(ctx, replay_recv_wr);
+	SET_PRIV_OP(ctx, prepare_qp_recv_replay);
+	SET_PRIV_OP(ctx, record_qp_index);
+
+	SET_PRIV_OP(ctx, resume_srq);
+
+	SET_PRIV_OP(ctx, migrrdma_start_poll);
+	SET_PRIV_OP(ctx, migrrdma_end_poll);
+	SET_PRIV_OP(ctx, migrrdma_poll_cq);
+	SET_PRIV_OP(ctx, migrrdma_start_inspect_qp);
+	SET_PRIV_OP(ctx, migrrdma_start_inspect_qp_v2);
+	SET_PRIV_OP(ctx, migrrdma_is_q_empty);
+	SET_PRIV_OP(ctx, qp_get_n_posted);
+	SET_PRIV_OP(ctx, qp_get_n_acked);
+	SET_PRIV_OP(ctx, srq_get_n_acked);
+
+	SET_PRIV_OP(ctx, uwrite_srq);
+	SET_PRIV_OP(ctx, replay_srq_recv_wr);
+	SET_PRIV_OP(ctx, prepare_srq_replay);
+
 #undef SET_OP
 #undef SET_OP2
 }
diff --git a/libibverbs/libibverbs.map.in b/libibverbs/libibverbs.map.in
index 0e39428..75d1acc 100644
--- a/libibverbs/libibverbs.map.in
+++ b/libibverbs/libibverbs.map.in
@@ -19,6 +19,7 @@ IBVERBS_1.0 {
 		ibv_reg_mr;
 		ibv_dereg_mr;
 		ibv_create_comp_channel;
+		ibv_resume_comp_channel;
 		ibv_destroy_comp_channel;
 		ibv_create_cq;
 		ibv_resize_cq;
@@ -178,6 +179,7 @@ IBVERBS_PRIVATE_@IBVERBS_PABI_VERSION@ {
 		ibv_cmd_alloc_dm;
 		ibv_cmd_alloc_mw;
 		ibv_cmd_alloc_pd;
+		ibv_cmd_alloc_async_fd;
 		ibv_cmd_attach_mcast;
 		ibv_cmd_close_xrcd;
 		ibv_cmd_create_ah;
@@ -208,6 +210,20 @@ IBVERBS_PRIVATE_@IBVERBS_PABI_VERSION@ {
 		ibv_cmd_detach_mcast;
 		ibv_cmd_free_dm;
 		ibv_cmd_get_context;
+		ibv_cmd_install_footprint;
+		ibv_cmd_register_async_fd;
+		ibv_cmd_install_cq_handle_mapping;
+		ibv_cmd_install_qp_handle_mapping;
+		ibv_cmd_install_srq_handle_mapping;
+		get_vqpn_from_old;
+		del_qpn_dict_node_2;
+		add_qpn_dict_node;
+		switch_to_new_qp;
+		ibv_cmd_install_ctx_resp;
+		register_update_mem;
+		register_keep_mmap_region;
+		add_bf_addr_map_entry;
+		get_alloc_bf;
 		ibv_cmd_modify_flow_action_esp;
 		ibv_cmd_modify_qp;
 		ibv_cmd_modify_qp_ex;
@@ -242,4 +258,5 @@ IBVERBS_PRIVATE_@IBVERBS_PABI_VERSION@ {
 		verbs_uninit_context;
 		verbs_init_cq;
 		ibv_cmd_modify_cq;
+		ibv_get_signal;
 };
diff --git a/libibverbs/my_mem_mgnt.c b/libibverbs/my_mem_mgnt.c
new file mode 100644
index 0000000..89b862e
--- /dev/null
+++ b/libibverbs/my_mem_mgnt.c
@@ -0,0 +1,49 @@
+#define _GNU_SOURCE
+#include "driver.h"
+#include "sys/mman.h"
+
+#define BLOCK_LOG					16
+#define BLOCK_SZ					(1UL << 16)
+#define BLOCK_MASK					(~(BLOCK_SZ - 1))
+
+static inline size_t block_aligned_up(size_t size) {
+	return (size & BLOCK_MASK) + ((!!(size & (~BLOCK_MASK))) << 16);
+}
+
+static void *start_addr = NULL;
+static off_t offset = 0;
+static size_t alloc_size = 0;
+
+void free_all_my_memory(void) {
+	munmap(start_addr, alloc_size);
+	start_addr = NULL;
+	offset = 0;
+	alloc_size = 0;
+}
+
+void *my_malloc(size_t size) {
+	void *ret;
+
+	if(offset + size > alloc_size) {
+		/* alloc new one, or expand the existing one */
+		if(!start_addr) {
+			start_addr = mmap(NULL, 16777216 * 64, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS,
+							-1, 0);
+			if(start_addr == MAP_FAILED)
+				return NULL;
+			alloc_size = 16777216 * 64;
+		}
+		else {
+			void *tmp_addr = mremap(start_addr, alloc_size,
+						block_aligned_up(offset + size), 0);
+			if(tmp_addr == MAP_FAILED || tmp_addr != start_addr)
+				return NULL;
+			alloc_size = block_aligned_up(offset + size);
+		}
+	}
+
+	ret = start_addr + offset;
+	offset += size;
+
+	return ret;
+}
diff --git a/libibverbs/q_tree.c b/libibverbs/q_tree.c
new file mode 100644
index 0000000..a681292
--- /dev/null
+++ b/libibverbs/q_tree.c
@@ -0,0 +1,1170 @@
+#include "verbs.h"
+#include "rbtree.h"
+#include "driver.h"
+
+struct cq_entry {
+	struct ibv_cq			*cq;
+	int						cons_index;
+	struct rb_node			rb_node;
+};
+
+static inline struct cq_entry *to_cq_entry(struct rb_node *n) {
+	return n? container_of(n, struct cq_entry, rb_node): NULL;
+}
+
+static inline int cq_entry_compare(const struct rb_node *n1, const struct rb_node *n2) {
+	struct cq_entry *cq_ent1 = to_cq_entry(n1);
+	struct cq_entry *cq_ent2 = to_cq_entry(n2);
+
+	if(cq_ent1->cq < cq_ent2->cq)
+		return -1;
+	else if(cq_ent1->cq > cq_ent2->cq)
+		return 1;
+	else
+		return 0;
+}
+
+struct qp_entry {
+	struct ibv_qp			*qp;
+	struct rb_node			rb_node;
+};
+
+static inline struct qp_entry *to_qp_entry(struct rb_node *n) {
+	return n? container_of(n, struct qp_entry, rb_node): NULL;
+}
+
+static inline int qp_entry_compare(const struct rb_node *n1, const struct rb_node *n2) {
+	struct qp_entry *qp_ent1 = to_qp_entry(n1);
+	struct qp_entry *qp_ent2 = to_qp_entry(n2);
+
+	if(qp_ent1->qp->qp_num < qp_ent2->qp->qp_num)
+		return -1;
+	else if(qp_ent1->qp->qp_num > qp_ent2->qp->qp_num)
+		return 1;
+	else
+		return 0;
+}
+
+static struct cq_entry *search_cq_entry(struct ibv_cq *cq, const struct rbtree_struct *rbtree,
+						struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct cq_entry target = {.cq = cq};
+	struct rb_node *match = ___search(&target.rb_node, rbtree, p_parent, p_insert,
+					SEARCH_EXACTLY, cq_entry_compare);
+	return to_cq_entry(match);
+}
+
+static struct qp_entry *search_qp_entry(struct ibv_qp *qp, const struct rbtree_struct *rbtree,
+						struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct qp_entry target = {.qp = qp};
+	struct rb_node *match = ___search(&target.rb_node, rbtree, p_parent, p_insert,
+					SEARCH_EXACTLY, qp_entry_compare);
+	return to_qp_entry(match);
+}
+
+static declare_and_init_rbtree(qp_tree);
+static declare_and_init_rbtree(cq_tree);
+
+int rbtree_search_cq(struct ibv_cq *cq) {
+	struct cq_entry *cq_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_rdlock(&cq_tree.rwlock);
+	cq_entry = search_cq_entry(cq, &cq_tree, NULL, NULL);
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&cq_tree.rwlock);
+	
+	if(cq_entry)
+		return 0;
+	else
+		return -ENOENT;
+}
+
+int rbtree_add_cq(struct ibv_cq *cq) {
+	struct cq_entry *cq_entry;
+	struct rb_node *parent, **insert;
+	struct cq_entry *new_cq_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_wrlock(&cq_tree.rwlock);
+	cq_entry = search_cq_entry(cq, &cq_tree, &parent, &insert);
+	if(cq_entry) {
+		if(ibv_get_signal())
+			pthread_rwlock_unlock(&cq_tree.rwlock);
+		return -EEXIST;
+	}
+
+	new_cq_entry = malloc(sizeof(*new_cq_entry));
+	if(!new_cq_entry) {
+		if(ibv_get_signal())
+			pthread_rwlock_unlock(&cq_tree.rwlock);
+		return -ENOMEM;
+	}
+
+	new_cq_entry->cq = cq;
+	rbtree_add_node(&new_cq_entry->rb_node, parent, insert, &cq_tree);
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&cq_tree.rwlock);
+
+	return 0;
+}
+
+void rbtree_del_cq(struct ibv_cq *cq) {
+	struct cq_entry *cq_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_wrlock(&cq_tree.rwlock);
+	cq_entry = search_cq_entry(cq, &cq_tree, NULL, NULL);
+	if(!cq_entry) {
+		if(ibv_get_signal())
+			pthread_rwlock_unlock(&cq_tree.rwlock);
+		return;
+	}
+
+	rbtree_rm_node(&cq_entry->rb_node, &cq_tree);
+	free(cq_entry);
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&cq_tree.rwlock);
+}
+
+int rbtree_search_qp(struct ibv_qp *qp) {
+	struct qp_entry *qp_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_rdlock(&qp_tree.rwlock);
+	qp_entry = search_qp_entry(qp, &qp_tree, NULL, NULL);
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&qp_tree.rwlock);
+
+	if(qp_entry)
+		return 0;
+	else
+		return -ENOENT;
+}
+
+int rbtree_add_qp(struct ibv_qp *qp) {
+	struct qp_entry *qp_entry;
+	struct rb_node *parent, **insert;
+	struct qp_entry *new_qp_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_wrlock(&qp_tree.rwlock);
+	qp_entry = search_qp_entry(qp, &qp_tree, &parent, &insert);
+	if(qp_entry) {
+		if(ibv_get_signal())
+			pthread_rwlock_unlock(&qp_tree.rwlock);
+		return -EEXIST;
+	}
+
+	new_qp_entry = malloc(sizeof(*new_qp_entry));
+	if(!new_qp_entry) {
+		if(ibv_get_signal())
+			pthread_rwlock_unlock(&qp_tree.rwlock);
+		return -ENOMEM;
+	}
+
+	new_qp_entry->qp = qp;
+	rbtree_add_node(&new_qp_entry->rb_node, parent, insert, &qp_tree);
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&qp_tree.rwlock);
+
+	return 0;
+}
+
+void rbtree_del_qp(struct ibv_qp *qp) {
+	struct qp_entry *qp_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_wrlock(&qp_tree.rwlock);
+	qp_entry = search_qp_entry(qp, &qp_tree, NULL, NULL);
+	if(!qp_entry) {
+		if(ibv_get_signal())
+			pthread_rwlock_unlock(&qp_tree.rwlock);
+		return;
+	}
+
+	rbtree_rm_node(&qp_entry->rb_node, &qp_tree);
+	free(qp_entry);
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&qp_tree.rwlock);
+}
+
+int rbtree_traverse_cq(int (*iter_cq_fn)(struct ibv_cq *cq,
+					void *entry, void *in_param), void *in_param) {
+	struct cq_entry *cq_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_rdlock(&cq_tree.rwlock);
+	for_each_rbtree_entry(cq_entry, &cq_tree, to_cq_entry, rb_node) {
+		if(iter_cq_fn(cq_entry->cq, cq_entry, in_param)) {
+			if(ibv_get_signal())
+				pthread_rwlock_unlock(&cq_tree.rwlock);
+			return -1;
+		}
+	}
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&cq_tree.rwlock);
+	return 0;
+}
+
+int rbtree_traverse_qp(int (*iter_qp_fn)(struct ibv_qp *qp,
+					void *entry, void *in_param), void *in_param) {
+	struct qp_entry *qp_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_rdlock(&qp_tree.rwlock);
+	for_each_rbtree_entry(qp_entry, &qp_tree, to_qp_entry, rb_node) {
+		if(iter_qp_fn(qp_entry->qp, qp_entry, in_param)) {
+			if(ibv_get_signal())
+				pthread_rwlock_unlock(&qp_tree.rwlock);
+			return -1;
+		}
+	}
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&qp_tree.rwlock);
+	return 0;
+}
+
+struct context_entry {
+	struct ibv_context				*context;
+	struct rb_node					rb_node;
+};
+
+static inline struct context_entry *to_context_entry(struct rb_node *n) {
+	return n? container_of(n, struct context_entry, rb_node): NULL;
+}
+
+static inline int context_entry_compare(const struct rb_node *n1, const struct rb_node *n2) {
+	struct context_entry *ctx_ent1 = to_context_entry(n1);
+	struct context_entry *ctx_ent2 = to_context_entry(n2);
+
+	if(ctx_ent1->context < ctx_ent2->context)
+		return -1;
+	else if(ctx_ent1->context > ctx_ent2->context)
+		return 1;
+	else
+		return 0;
+}
+
+static struct context_entry *search_context_entry(struct ibv_context *ctx, const struct rbtree_struct *rbtree,
+					struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct context_entry target = {.context = ctx};
+	struct rb_node *match = ___search(&target.rb_node, rbtree, p_parent, p_insert,
+					SEARCH_EXACTLY, context_entry_compare);
+	return to_context_entry(match);
+}
+
+static declare_and_init_rbtree(ctx_tree);
+
+int rbtree_search_context(struct ibv_context *ctx) {
+	struct context_entry *ctx_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_rdlock(&ctx_tree.rwlock);
+	ctx_entry = search_context_entry(ctx, &ctx_tree, NULL, NULL);
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&ctx_tree.rwlock);
+	
+	if(ctx_entry)
+		return 0;
+	else
+		return -ENOENT;
+}
+
+int rbtree_add_context(struct ibv_context *ctx) {
+	struct context_entry *ctx_entry;
+	struct rb_node *parent, **insert;
+	struct context_entry *new_ctx_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_wrlock(&ctx_tree.rwlock);
+	ctx_entry = search_context_entry(ctx, &ctx_tree, &parent, &insert);
+	if(ctx_entry) {
+		if(ibv_get_signal())
+			pthread_rwlock_unlock(&ctx_tree.rwlock);
+		return -EEXIST;
+	}
+
+	new_ctx_entry = malloc(sizeof(*new_ctx_entry));
+	if(!new_ctx_entry) {
+		if(ibv_get_signal())
+			pthread_rwlock_unlock(&ctx_tree.rwlock);
+		return -ENOMEM;
+	}
+
+	new_ctx_entry->context = ctx;
+	rbtree_add_node(&new_ctx_entry->rb_node, parent, insert, &ctx_tree);
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&ctx_tree.rwlock);
+	
+	return 0;
+}
+
+void rbtree_del_context(struct ibv_context *ctx) {
+	struct context_entry *ctx_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_wrlock(&ctx_tree.rwlock);
+	ctx_entry = search_context_entry(ctx, &ctx_tree, NULL, NULL);
+	if(!ctx_entry) {
+		if(ibv_get_signal())
+			pthread_rwlock_unlock(&ctx_tree.rwlock);
+		return;
+	}
+
+	rbtree_rm_node(&ctx_entry->rb_node, &ctx_tree);
+	free(ctx_entry);
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&ctx_tree.rwlock);
+}
+
+int rbtree_traverse_context(int (*iter_context_fn)(struct ibv_context *ctx,
+					void *entry, void *in_param), void *in_param) {
+	struct context_entry *ctx_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_rdlock(&ctx_tree.rwlock);
+	for_each_rbtree_entry(ctx_entry, &ctx_tree, to_context_entry, rb_node) {
+		if(iter_context_fn(ctx_entry->context, ctx_entry, in_param)) {
+			if(ibv_get_signal())
+				pthread_rwlock_unlock(&ctx_tree.rwlock);
+			return -1;
+		}
+	}
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&ctx_tree.rwlock);
+	return 0;
+}
+
+static declare_and_init_rbtree(qpn_dict);
+
+struct qpn_dict_node {
+	uint32_t				pqpn;
+	int						cmd_fd;
+	int						qp_vhandle;
+	struct rb_node			node;
+};
+
+static inline struct qpn_dict_node *to_qpn_dict_node(struct rb_node *node) {
+	return node? container_of(node, struct qpn_dict_node, node): NULL;
+}
+
+static int qpn_dict_node_compare(const struct rb_node *n1, const struct rb_node *n2) {
+	struct qpn_dict_node *node1 = to_qpn_dict_node((struct rb_node *)n1);
+	struct qpn_dict_node *node2 = to_qpn_dict_node((struct rb_node *)n2);
+
+	if(node1->pqpn < node2->pqpn)
+		return -1;
+	else if(node1->pqpn > node2->pqpn)
+		return 1;
+	else
+		return 0;
+}
+
+static struct qpn_dict_node *search_qpn_dict_node(uint32_t pqpn,
+					struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct qpn_dict_node target = {.pqpn = pqpn};
+	struct rb_node *match = ___search(&target.node, &qpn_dict, p_parent, p_insert,
+								SEARCH_EXACTLY, qpn_dict_node_compare);
+	return to_qpn_dict_node(match);
+}
+
+int add_qpn_dict_node(struct ibv_qp *qp) {
+	struct qpn_dict_node	*this_node;
+	struct rb_node			*parent;
+	struct rb_node			**insert;
+	uint32_t				pqpn;
+	int						cmd_fd;
+	int						qp_vhandle;
+
+	pqpn = qp->real_qpn;
+	cmd_fd = qp->context->cmd_fd;
+	qp_vhandle = qp->handle;
+
+	pthread_rwlock_wrlock(&qpn_dict.rwlock);
+	this_node = search_qpn_dict_node(pqpn, &parent, &insert);
+	if(this_node) {
+		pthread_rwlock_unlock(&qpn_dict.rwlock);
+		return -EEXIST;
+	}
+
+	this_node = calloc(1, sizeof(*this_node));
+	if(!this_node) {
+		pthread_rwlock_unlock(&qpn_dict.rwlock);
+		return -ENOMEM;
+	}
+
+	this_node->pqpn = pqpn;
+	this_node->cmd_fd = cmd_fd;
+	this_node->qp_vhandle = qp_vhandle;
+	rbtree_add_node(&this_node->node, parent, insert, &qpn_dict);
+	pthread_rwlock_unlock(&qpn_dict.rwlock);
+
+	return 0;
+}
+
+int get_qpn_dict(uint32_t pqpn, int *cmd_fd, int *qp_vhandle) {
+	struct qpn_dict_node	*this_node;
+
+	pthread_rwlock_rdlock(&qpn_dict.rwlock);
+	this_node = search_qpn_dict_node(pqpn, NULL, NULL);
+	if(!this_node) {
+		pthread_rwlock_unlock(&qpn_dict.rwlock);
+		return -ENOENT;
+	}
+
+	if(cmd_fd) {
+		*cmd_fd = this_node->cmd_fd;
+	}
+
+	if(qp_vhandle) {
+		*qp_vhandle = this_node->qp_vhandle;
+	}
+
+	pthread_rwlock_unlock(&qpn_dict.rwlock);
+	return 0;
+}
+
+void del_qpn_dict_node(struct ibv_qp *qp) {
+	struct qpn_dict_node	*this_node;
+
+	pthread_rwlock_wrlock(&qpn_dict.rwlock);
+	this_node = search_qpn_dict_node(qp->real_qpn, NULL, NULL);
+	if(!this_node) {
+		pthread_rwlock_unlock(&qpn_dict.rwlock);
+		return;
+	}
+
+	rbtree_rm_node(&this_node->node, &qpn_dict);
+	free(this_node);
+	pthread_rwlock_unlock(&qpn_dict.rwlock);
+}
+
+void del_qpn_dict_node_2(struct ibv_qp *qp) {
+	struct qpn_dict_node	*this_node;
+
+	pthread_rwlock_wrlock(&qpn_dict.rwlock);
+	this_node = search_qpn_dict_node(qp->orig_real_qpn, NULL, NULL);
+	if(!this_node) {
+		pthread_rwlock_unlock(&qpn_dict.rwlock);
+		return;
+	}
+
+	rbtree_rm_node(&this_node->node, &qpn_dict);
+	free(this_node);
+	pthread_rwlock_unlock(&qpn_dict.rwlock);
+}
+
+static declare_and_init_rbtree(switch_list);
+
+struct switch_list_node {
+	uint32_t				new_pqpn;
+	struct ibv_qp			*orig_qp;
+	struct ibv_qp			*new_qp;
+	struct rb_node			node;
+};
+
+static inline struct switch_list_node *to_switch_list_node(struct rb_node *node) {
+	return node? container_of(node, struct switch_list_node, node): NULL;
+}
+
+static int switch_list_node_compare(const struct rb_node *n1, const struct rb_node *n2) {
+	struct switch_list_node *node1 = to_switch_list_node((struct rb_node *)n1);
+	struct switch_list_node *node2 = to_switch_list_node((struct rb_node *)n2);
+
+	if(node1->new_pqpn < node2->new_pqpn)
+		return -1;
+	else if(node1->new_pqpn > node2->new_pqpn)
+		return 1;
+	else
+		return 0;
+}
+
+static struct switch_list_node *search_switch_list_node(uint32_t pqpn,
+						struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct switch_list_node target = {.new_pqpn = pqpn};
+	struct rb_node *match = ___search(&target.node, &switch_list, p_parent, p_insert,
+								SEARCH_EXACTLY, switch_list_node_compare);
+	return to_switch_list_node(match);
+}
+
+int add_switch_list_node(uint32_t pqpn, struct ibv_qp *orig_qp, struct ibv_qp *new_qp) {
+	struct switch_list_node		*this_node;
+	struct rb_node				*parent;
+	struct rb_node				**insert;
+
+	pthread_rwlock_wrlock(&switch_list.rwlock);
+	this_node = search_switch_list_node(pqpn, &parent, &insert);
+	if(this_node) {
+		pthread_rwlock_unlock(&switch_list.rwlock);
+		return -EEXIST;
+	}
+
+	this_node = calloc(1, sizeof(*this_node));
+	if(!this_node) {
+		pthread_rwlock_unlock(&switch_list.rwlock);
+		return -ENOMEM;
+	}
+
+	this_node->new_pqpn = pqpn;
+	this_node->orig_qp = orig_qp;
+	this_node->new_qp = new_qp;
+	rbtree_add_node(&this_node->node, parent, insert, &switch_list);
+	pthread_rwlock_unlock(&switch_list.rwlock);
+
+	return 0;
+}
+
+int switch_to_new_qp(uint32_t pqpn, void *param,
+				int (*switch_cb)(struct ibv_qp *orig_qp,
+				struct ibv_qp *new_qp,
+				void *param)) {
+	struct switch_list_node		*this_node;
+	int err;
+
+	pthread_rwlock_wrlock(&switch_list.rwlock);
+	this_node = search_switch_list_node(pqpn, NULL, NULL);
+	if(!this_node) {
+		pthread_rwlock_unlock(&switch_list.rwlock);
+		return -ENOENT;
+	}
+
+	err = switch_cb(this_node->orig_qp, this_node->new_qp, param);
+	if(err) {
+		pthread_rwlock_unlock(&switch_list.rwlock);
+		return err;
+	}
+
+	rbtree_rm_node(&this_node->node, &switch_list);
+	free(this_node);
+	pthread_rwlock_unlock(&switch_list.rwlock);
+
+	return 0;
+}
+
+#include "ibverbs.h"
+
+int switch_all_qps(int (*switch_cb)(struct ibv_qp *orig_qp, struct ibv_qp *new_qp),
+				int (*load_cb)(struct ibv_qp *orig_qp, void *replay_fn)) {
+	struct switch_list_node *this_node;
+	struct switch_list_node *tmp;
+	int err;
+
+	pthread_rwlock_wrlock(&switch_list.rwlock);
+	for_each_rbtree_entry_safe(this_node, tmp, &switch_list,
+					to_switch_list_node, node) {
+		err = switch_cb(this_node->orig_qp, this_node->new_qp);
+		if(err) {
+			pthread_rwlock_unlock(&switch_list.rwlock);
+			return err;
+		}
+
+		err = load_cb(this_node->orig_qp, get_ops(this_node->orig_qp->context)->replay_recv_wr);
+		if(err) {
+			pthread_rwlock_unlock(&switch_list.rwlock);
+			return err;
+		}
+
+		rbtree_rm_node(&this_node->node, &switch_list);
+		free(this_node);
+	}
+	pthread_rwlock_unlock(&switch_list.rwlock);
+
+	return 0;
+}
+
+struct srq_entry {
+	struct ibv_srq			*srq;
+	struct rb_node			rb_node;
+};
+
+static inline struct srq_entry *to_srq_entry(struct rb_node *n) {
+	return n? container_of(n, struct srq_entry, rb_node): NULL;
+}
+
+static inline int srq_entry_compare(const struct rb_node *n1, const struct rb_node *n2) {
+	struct srq_entry *srq_ent1 = to_srq_entry(n1);
+	struct srq_entry *srq_ent2 = to_srq_entry(n2);
+
+	if(srq_ent1->srq < srq_ent2->srq)
+		return -1;
+	else if(srq_ent1->srq > srq_ent2->srq)
+		return 1;
+	else
+		return 0;
+}
+
+static struct srq_entry *search_srq_entry(struct ibv_srq *srq, const struct rbtree_struct *rbtree,
+						struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct srq_entry target = {.srq = srq};
+	struct rb_node *match = ___search(&target.rb_node, rbtree, p_parent, p_insert,
+							SEARCH_EXACTLY, srq_entry_compare);
+	return to_srq_entry(match);
+}
+
+static declare_and_init_rbtree(srq_tree);
+
+int rbtree_search_srq(struct ibv_srq *srq) {
+	struct srq_entry *srq_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_rdlock(&srq_tree.rwlock);
+	srq_entry = search_srq_entry(srq, &srq_tree, NULL, NULL);
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&srq_tree.rwlock);
+
+	if(srq_entry)
+		return 0;
+	else
+		return -ENOENT;
+}
+
+int rbtree_add_srq(struct ibv_srq *srq) {
+	struct srq_entry *srq_entry;
+	struct rb_node *parent, **insert;
+
+	if(ibv_get_signal())
+		pthread_rwlock_wrlock(&srq_tree.rwlock);
+	srq_entry = search_srq_entry(srq, &srq_tree, &parent, &insert);
+	if(srq_entry) {
+		if(ibv_get_signal())
+			pthread_rwlock_unlock(&srq_tree.rwlock);
+		return -EEXIST;
+	}
+
+	srq_entry = malloc(sizeof(*srq_entry));
+	if(!srq_entry) {
+		if(ibv_get_signal())
+			pthread_rwlock_unlock(&srq_tree.rwlock);
+		return -ENOMEM;
+	}
+
+	srq_entry->srq = srq;
+	rbtree_add_node(&srq_entry->rb_node, parent, insert, &srq_tree);
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&srq_tree.rwlock);
+
+	return 0;
+}
+
+void rbtree_del_srq(struct ibv_srq *srq) {
+	struct srq_entry *srq_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_wrlock(&srq_tree.rwlock);
+	srq_entry = search_srq_entry(srq, &srq_tree, NULL, NULL);
+	if(!srq_entry) {
+		if(ibv_get_signal())
+			pthread_rwlock_unlock(&srq_tree.rwlock);
+		return;
+	}
+
+	rbtree_rm_node(&srq_entry->rb_node, &srq_tree);
+	free(srq_entry);
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&srq_tree.rwlock);
+}
+
+int rbtree_traverse_srq(int (*iter_srq_fn)(struct ibv_srq *srq,
+						void *entry, void *in_param), void *in_param) {
+	struct srq_entry *srq_entry;
+
+	if(ibv_get_signal())
+		pthread_rwlock_rdlock(&srq_tree.rwlock);
+	for_each_rbtree_entry(srq_entry, &srq_tree, to_srq_entry, rb_node) {
+		if(iter_srq_fn(srq_entry->srq, srq_entry, in_param)) {
+			if(ibv_get_signal())
+				pthread_rwlock_unlock(&srq_tree.rwlock);
+			return -1;
+		}
+	}
+	if(ibv_get_signal())
+		pthread_rwlock_unlock(&srq_tree.rwlock);
+	return 0;
+}
+
+static declare_and_init_rbtree(srq_switch_list);
+
+struct srq_switch_node {
+	struct ibv_srq				*new_srq;
+	struct ibv_srq				*orig_srq;
+	struct rb_node				node;
+};
+
+static inline struct srq_switch_node *to_srq_switch_node(struct rb_node *node) {
+	return node? container_of(node, struct srq_switch_node, node): NULL;
+}
+
+static int srq_switch_node_compare(const struct rb_node *n1, const struct rb_node *n2) {
+	struct srq_switch_node *node1 = to_srq_switch_node(n1);
+	struct srq_switch_node *node2 = to_srq_switch_node(n2);
+
+	if(node1->new_srq < node2->new_srq)
+		return -1;
+	else if(node1->new_srq > node2->new_srq)
+		return 1;
+	else
+		return 0;
+}
+
+static struct srq_switch_node *search_srq_switch_node(struct ibv_srq *new_srq,
+					struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct srq_switch_node target = {.new_srq = new_srq};
+	struct rb_node *match = ___search(&target.node, &srq_switch_list, p_parent, p_insert,
+							SEARCH_EXACTLY, srq_switch_node_compare);
+	return to_srq_switch_node(match);
+}
+
+int add_srq_switch_node(struct ibv_srq *new_srq, struct ibv_srq *orig_srq) {
+	struct srq_switch_node *this_node;
+	struct rb_node *parent;
+	struct rb_node **insert;
+
+	pthread_rwlock_wrlock(&srq_switch_list.rwlock);
+	this_node = search_srq_switch_node(new_srq, &parent, &insert);
+	if(this_node) {
+		pthread_rwlock_unlock(&srq_switch_list.rwlock);
+		return -EEXIST;
+	}
+
+	this_node = calloc(1, sizeof(*this_node));
+	if(!this_node) {
+		pthread_rwlock_unlock(&srq_switch_list.rwlock);
+		return -ENOMEM;
+	}
+
+	this_node->new_srq = new_srq;
+	this_node->orig_srq = orig_srq;
+	rbtree_add_node(&this_node->node, parent, insert, &srq_switch_list);
+	pthread_rwlock_unlock(&srq_switch_list.rwlock);
+
+	return 0;
+}
+
+int switch_all_srqs(int (*switch_cb)(struct ibv_srq *orig_srq, struct ibv_srq *new_srq, int *head, int *tail),
+			int (*srq_load_cb)(struct ibv_srq *orig_srq, void *replay_fn, int head, int tail)) {
+	struct srq_switch_node *this_node, *tmp;
+	int err;
+	int head, tail;
+
+	pthread_rwlock_wrlock(&srq_switch_list.rwlock);
+	for_each_rbtree_entry_safe(this_node, tmp, &srq_switch_list,
+						to_srq_switch_node, node) {
+		err = switch_cb(this_node->orig_srq, this_node->new_srq, &head, &tail);
+		if(err) {
+			pthread_rwlock_unlock(&srq_switch_list.rwlock);
+			return err;
+		}
+
+		err = srq_load_cb(this_node->orig_srq, get_ops(this_node->orig_srq->context)->replay_srq_recv_wr, head, tail);
+		if(err) {
+			pthread_rwlock_unlock(&srq_switch_list.rwlock);
+			return err;
+		}
+
+		rbtree_rm_node(&this_node->node, &srq_switch_list);
+		free(this_node);
+	}
+	pthread_rwlock_unlock(&srq_switch_list.rwlock);
+	return 0;
+}
+
+static declare_and_init_rbtree(old_qpndict);
+
+struct old_dict_node {
+	uint32_t			real_qpn;
+	uint32_t			virt_qpn;
+	struct rb_node		node;
+};
+
+static inline struct old_dict_node *to_old_dict_node(struct rb_node *node) {
+	return node? container_of(node, struct old_dict_node, node): NULL;
+}
+
+static int old_dict_node_compare(const struct rb_node *n1, const struct rb_node *n2) {
+	struct old_dict_node *node1 = to_old_dict_node(n1);
+	struct old_dict_node *node2 = to_old_dict_node(n2);
+
+	if(node1->real_qpn < node2->real_qpn)
+		return -1;
+	else if(node1->real_qpn > node2->real_qpn)
+		return 1;
+	else
+		return 0;
+}
+
+static struct old_dict_node *search_old_dict_node(uint32_t real_qpn,
+					struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct old_dict_node target = {.real_qpn = real_qpn};
+	struct rb_node *match = ___search(&target.node, &old_qpndict, p_parent, p_insert,
+						SEARCH_EXACTLY, old_dict_node_compare);
+	return to_old_dict_node(match);
+}
+
+int get_vqpn_from_old(uint32_t real_qpn, uint32_t *vqpn) {
+	struct old_dict_node *this_node;
+
+	pthread_rwlock_rdlock(&old_qpndict.rwlock);
+	this_node = search_old_dict_node(real_qpn, NULL, NULL);
+	if(!this_node) {
+		pthread_rwlock_unlock(&old_qpndict.rwlock);
+		return -ENOENT;
+	}
+
+	if(vqpn)
+		*vqpn = this_node->virt_qpn;
+	pthread_rwlock_unlock(&old_qpndict.rwlock);
+	return 0;
+}
+
+int add_old_dict_node(struct ibv_qp *qp,
+				uint32_t real_qpn, uint32_t virt_qpn) {
+	struct old_dict_node *this_node;
+	struct rb_node *parent;
+	struct rb_node **insert;
+
+	pthread_rwlock_wrlock(&old_qpndict.rwlock);
+	this_node = search_old_dict_node(real_qpn, &parent, &insert);
+	if(this_node) {
+		pthread_rwlock_unlock(&old_qpndict.rwlock);
+		return 0;
+	}
+
+	this_node = (struct old_dict_node *)&qp->old_dict_node;
+	this_node->real_qpn = real_qpn;
+	this_node->virt_qpn = virt_qpn;
+	rbtree_add_node(&this_node->node, parent, insert, &old_qpndict);
+	pthread_rwlock_unlock(&old_qpndict.rwlock);
+	return 0;
+}
+
+static void free_old_qpn_dict(struct rb_node *node) {
+	return;
+}
+
+void clear_old_qpndict(void) {
+	clean_rbtree(&old_qpndict, free_old_qpn_dict);
+}
+
+static declare_and_init_rbtree(comp_channel_tree);
+
+struct comp_channel_node {
+	int									fd;
+	struct ibv_comp_channel				*channel;
+	struct rb_node						node;
+};
+
+static inline struct comp_channel_node *to_comp_channel_node(struct rb_node *node) {
+	return node? container_of(node, struct comp_channel_node, node): NULL;
+}
+
+static int comp_channel_node_compare(const struct rb_node *n1, const struct rb_node *n2) {
+	struct comp_channel_node *node1 = to_comp_channel_node(n1);
+	struct comp_channel_node *node2 = to_comp_channel_node(n2);
+
+	return node1->fd - node2->fd;
+}
+
+static struct comp_channel_node *search_comp_channel_node(int fd,
+					struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct comp_channel_node target = {.fd = fd};
+	struct rb_node *match = ___search(&target.node, &comp_channel_tree, p_parent, p_insert,
+							SEARCH_EXACTLY, comp_channel_node_compare);
+	return to_comp_channel_node(match);
+}
+
+struct ibv_comp_channel *get_comp_channel_from_fd(int fd) {
+	struct comp_channel_node *this_node;
+
+	pthread_rwlock_rdlock(&comp_channel_tree.rwlock);
+	this_node = search_comp_channel_node(fd, NULL, NULL);
+	if(!this_node) {
+		pthread_rwlock_unlock(&comp_channel_tree.rwlock);
+		return NULL;
+	}
+
+	pthread_rwlock_unlock(&comp_channel_tree.rwlock);
+	return this_node->channel;
+}
+
+int add_comp_channel(int fd, struct ibv_comp_channel *channel) {
+	struct comp_channel_node *this_node;
+	struct rb_node *parent;
+	struct rb_node **insert;
+
+	pthread_rwlock_wrlock(&comp_channel_tree.rwlock);
+	this_node = search_comp_channel_node(fd, &parent, &insert);
+	if(this_node) {
+		pthread_rwlock_unlock(&comp_channel_tree.rwlock);
+		return -1;
+	}
+
+	this_node = calloc(1, sizeof(*this_node));
+	if(!this_node) {
+		pthread_rwlock_unlock(&comp_channel_tree.rwlock);
+		return -1;
+	}
+
+	this_node->fd				= fd;
+	this_node->channel			= channel;
+	rbtree_add_node(&this_node->node, parent, insert, &comp_channel_tree);
+	pthread_rwlock_unlock(&comp_channel_tree.rwlock);
+	return 0;
+}
+
+static declare_and_init_rbtree(update_mem);
+
+struct update_mem_node {
+	void					*ptr;
+	size_t					size;
+	void					*content_p;
+	struct rb_node			node;
+};
+
+static inline struct update_mem_node *to_update_mem_node(struct rb_node *node) {
+	return node? container_of(node, struct update_mem_node, node): NULL;
+}
+
+static int comp_update_mem_node(const struct rb_node *n1, const struct rb_node *n2) {
+	struct update_mem_node *node1 = to_update_mem_node(n1);
+	struct update_mem_node *node2 = to_update_mem_node(n2);
+
+	if(node1->ptr < node2->ptr)
+		return -1;
+	else if(node1->ptr > node2->ptr)
+		return 1;
+	else
+		return 0;
+}
+
+static struct update_mem_node *search_update_mem_node(void *ptr,
+					struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct update_mem_node target = {.ptr = ptr};
+	struct rb_node *match = ___search(&target.node, &update_mem, p_parent, p_insert,
+					SEARCH_LAST_PRECURSOR_INC_ITSELF, comp_update_mem_node);
+	return to_update_mem_node(match);
+}
+
+static inline
+struct update_mem_node *get_next_update_mem_node(struct update_mem_node *this_node) {
+	struct rb_node *next = rb_next(&this_node->node);
+	return to_update_mem_node(next);
+}
+
+static inline
+struct update_mem_node *get_first_update_mem_node(void) {
+	struct rb_node *first = rb_first(&update_mem.tree);
+	return to_update_mem_node(first);
+}
+
+int register_update_mem(void *ptr, size_t size, void *content_p) {
+	struct update_mem_node *this_node;
+	struct rb_node *parent;
+	struct rb_node **insert;
+
+	pthread_rwlock_wrlock(&update_mem.rwlock);
+	this_node = search_update_mem_node(ptr, &parent, &insert);
+	if(this_node && this_node->ptr + this_node->size > ptr) {
+		pthread_rwlock_unlock(&update_mem.rwlock);
+		return -EEXIST;
+	}
+
+	this_node = this_node?
+				get_next_update_mem_node(this_node):
+				get_first_update_mem_node();
+	if(this_node && ptr + size > this_node->ptr) {
+		pthread_rwlock_unlock(&update_mem.rwlock);
+		return -EEXIST;
+	}
+
+	this_node = malloc(sizeof(*this_node));
+	if(!this_node) {
+		pthread_rwlock_unlock(&update_mem.rwlock);
+		return -ENOMEM;
+	}
+
+	this_node->ptr				= ptr;
+	this_node->size				= size;
+	this_node->content_p		= content_p;
+	rbtree_add_node(&this_node->node, parent, insert, &update_mem);
+	pthread_rwlock_unlock(&update_mem.rwlock);
+
+	return 0;
+}
+
+int update_all_mem(int (*update_mem_fn)(void *ptr, size_t size,
+								void *content_p)) {
+	struct update_mem_node *this_node;
+
+	for_each_rbtree_entry(this_node, &update_mem,
+					to_update_mem_node, node) {
+		if(update_mem_fn(this_node->ptr, this_node->size,
+						this_node->content_p)) {
+			return -1;
+		}
+	}
+
+	return 0;
+}
+
+static declare_and_init_rbtree(keep_mmap);
+
+struct keep_mmap_node {
+	void						*ptr;
+	size_t						size;
+	struct rb_node				node;
+};
+
+static inline struct keep_mmap_node *to_keep_mmap_node(struct rb_node *node) {
+	return node? container_of(node, struct keep_mmap_node, node): NULL;
+}
+
+static int comp_keep_mmap_node(const struct rb_node *n1, const struct rb_node *n2) {
+	struct keep_mmap_node *node1 = to_keep_mmap_node(n1);
+	struct keep_mmap_node *node2 = to_keep_mmap_node(n2);
+
+	if(node1->ptr < node2->ptr)
+		return -1;
+	else if(node1->ptr > node2->ptr)
+		return 1;
+	else
+		return 0;
+}
+
+static struct keep_mmap_node *search_keep_mmap_node(void *ptr,
+					struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct keep_mmap_node target = {.ptr = ptr};
+	struct rb_node *match = ___search(&target.node, &keep_mmap, p_parent, p_insert,
+						SEARCH_LAST_PRECURSOR_INC_ITSELF, comp_keep_mmap_node);
+	return to_keep_mmap_node(match);
+}
+
+static inline
+struct keep_mmap_node *get_next_keep_mmap_node(struct keep_mmap_node *this_node) {
+	struct rb_node *next = rb_next(&this_node->node);
+	return to_keep_mmap_node(next);
+}
+
+static inline
+struct keep_mmap_node *get_first_keep_mmap_node(void) {
+	struct rb_node *first = rb_first(&keep_mmap.tree);
+	return to_keep_mmap_node(first);
+}
+
+int register_keep_mmap_region(void *ptr, size_t size) {
+	struct keep_mmap_node *this_node;
+	struct rb_node *parent;
+	struct rb_node **insert;
+
+	pthread_rwlock_wrlock(&keep_mmap.rwlock);
+	this_node = search_keep_mmap_node(ptr, &parent, &insert);
+	if(this_node && this_node->ptr + this_node->size > ptr) {
+		pthread_rwlock_unlock(&keep_mmap.rwlock);
+		return -EEXIST;
+	}
+
+	this_node = this_node?
+				get_next_keep_mmap_node(this_node):
+				get_first_keep_mmap_node();
+	if(this_node && ptr + size > this_node->ptr) {
+		pthread_rwlock_unlock(&keep_mmap.rwlock);
+		return -EEXIST;
+	}
+
+	this_node = malloc(sizeof(*this_node));
+	if(!this_node) {
+		pthread_rwlock_unlock(&keep_mmap.rwlock);
+		return -ENOMEM;
+	}
+
+	this_node->ptr			= ptr;
+	this_node->size			= size;
+	rbtree_add_node(&this_node->node, parent, insert, &keep_mmap);
+	pthread_rwlock_unlock(&keep_mmap.rwlock);
+
+	return 0;
+}
+
+int keep_all_mmap(int (*keep_mmap_fn)(unsigned long long start,
+								unsigned long long end)) {
+	struct keep_mmap_node *this_node;
+
+	for_each_rbtree_entry(this_node, &keep_mmap,
+					to_keep_mmap_node, node) {
+		keep_mmap_fn((unsigned long long)this_node->ptr,
+					(unsigned long long)(this_node->ptr + this_node->size));
+	}
+
+	return 0;
+}
+
+static declare_and_init_rbtree(bf_addr_map);
+
+struct bf_addr_map_entry {
+	void					*new_bf;
+	void					*alloc_bf;
+	struct rb_node			node;
+};
+
+static inline struct bf_addr_map_entry *to_bf_addr_map_entry(struct rb_node *node) {
+	return node? container_of(node, struct bf_addr_map_entry, node): NULL;
+}
+
+static int bf_addr_map_entry_compare(const struct rb_node *n1, const struct rb_node *n2) {
+	struct bf_addr_map_entry *ent1 = to_bf_addr_map_entry(n1);
+	struct bf_addr_map_entry *ent2 = to_bf_addr_map_entry(n2);
+
+	if(ent1->new_bf < ent2->new_bf)
+		return -1;
+	else if(ent1->new_bf > ent2->new_bf)
+		return 1;
+	else
+		return 0;
+}
+
+static struct bf_addr_map_entry *search_bf_addr_map_entry(void *new_bf,
+					struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct bf_addr_map_entry target = {.new_bf = new_bf};
+	struct rb_node *match = ___search(&target.node, &bf_addr_map, p_parent, p_insert,
+						SEARCH_EXACTLY, bf_addr_map_entry_compare);
+	return to_bf_addr_map_entry(match);
+}
+
+int add_bf_addr_map_entry(void *new_bf, void *alloc_bf) {
+	struct bf_addr_map_entry *ent;
+	struct rb_node *parent, **insert;
+
+	pthread_rwlock_wrlock(&bf_addr_map.rwlock);
+	ent = search_bf_addr_map_entry(new_bf, &parent, &insert);
+	if(ent) {
+		pthread_rwlock_unlock(&bf_addr_map.rwlock);
+		return -EEXIST;
+	}
+
+	ent = malloc(sizeof(*ent));
+	if(!ent) {
+		pthread_rwlock_unlock(&bf_addr_map.rwlock);
+		return -ENOMEM;
+	}
+
+	ent->new_bf = new_bf;
+	ent->alloc_bf = alloc_bf;
+	rbtree_add_node(&ent->node, parent, insert, &bf_addr_map);
+	pthread_rwlock_unlock(&bf_addr_map.rwlock);
+	return 0;
+}
+
+void *get_alloc_bf(void *new_bf) {
+	struct bf_addr_map_entry *ent;
+	void *ret;
+
+	pthread_rwlock_rdlock(&bf_addr_map.rwlock);
+	ent = search_bf_addr_map_entry(new_bf, NULL, NULL);
+	if(!ent) {
+		pthread_rwlock_unlock(&bf_addr_map.rwlock);
+		return NULL;
+	}
+
+	ret = ent->alloc_bf;
+	pthread_rwlock_unlock(&bf_addr_map.rwlock);
+	return ret;
+}
diff --git a/libibverbs/rbtree.c b/libibverbs/rbtree.c
new file mode 100644
index 0000000..d186625
--- /dev/null
+++ b/libibverbs/rbtree.c
@@ -0,0 +1,441 @@
+/*
+ * RBtree implementation adopted from the Linux kernel sources.
+ */
+
+#include <sys/types.h>
+#include "rbtree.h"
+
+#define RB_RED			0
+#define RB_BLACK		1
+#define RB_MASK			3
+
+static void __rb_rotate_left(struct rb_node *node, struct rb_root *root)
+{
+	struct rb_node *right = node->rb_right;
+	struct rb_node *parent = rb_parent(node);
+
+	node->rb_right = right->rb_left;
+	if (node->rb_right)
+		rb_set_parent(right->rb_left, node);
+	right->rb_left = node;
+
+	rb_set_parent(right, parent);
+
+	if (parent) {
+		if (node == parent->rb_left)
+			parent->rb_left = right;
+		else
+			parent->rb_right = right;
+	} else
+		root->rb_node = right;
+	rb_set_parent(node, right);
+}
+
+static void __rb_rotate_right(struct rb_node *node, struct rb_root *root)
+{
+	struct rb_node *left = node->rb_left;
+	struct rb_node *parent = rb_parent(node);
+
+	node->rb_left = left->rb_right;
+	if (node->rb_left)
+		rb_set_parent(left->rb_right, node);
+	left->rb_right = node;
+
+	rb_set_parent(left, parent);
+
+	if (parent) {
+		if (node == parent->rb_right)
+			parent->rb_right = left;
+		else
+			parent->rb_left = left;
+	} else
+		root->rb_node = left;
+	rb_set_parent(node, left);
+}
+
+void rb_insert_color(struct rb_node *node, struct rb_root *root)
+{
+	struct rb_node *parent, *gparent;
+
+	while ((parent = rb_parent(node)) && rb_is_red(parent)) {
+		gparent = rb_parent(parent);
+
+		if (parent == gparent->rb_left) {
+			{
+				register struct rb_node *uncle = gparent->rb_right;
+				if (uncle && rb_is_red(uncle)) {
+					rb_set_black(uncle);
+					rb_set_black(parent);
+					rb_set_red(gparent);
+					node = gparent;
+					continue;
+				}
+			}
+
+			if (parent->rb_right == node) {
+				register struct rb_node *tmp;
+				__rb_rotate_left(parent, root);
+				tmp = parent;
+				parent = node;
+				node = tmp;
+			}
+
+			rb_set_black(parent);
+			rb_set_red(gparent);
+			__rb_rotate_right(gparent, root);
+		} else {
+			{
+				register struct rb_node *uncle = gparent->rb_left;
+				if (uncle && rb_is_red(uncle)) {
+					rb_set_black(uncle);
+					rb_set_black(parent);
+					rb_set_red(gparent);
+					node = gparent;
+					continue;
+				}
+			}
+
+			if (parent->rb_left == node) {
+				register struct rb_node *tmp;
+				__rb_rotate_right(parent, root);
+				tmp = parent;
+				parent = node;
+				node = tmp;
+			}
+
+			rb_set_black(parent);
+			rb_set_red(gparent);
+			__rb_rotate_left(gparent, root);
+		}
+	}
+
+	rb_set_black(root->rb_node);
+}
+
+static void __rb_erase_color(struct rb_node *node, struct rb_node *parent, struct rb_root *root)
+{
+	struct rb_node *other;
+
+	while ((!node || rb_is_black(node)) && node != root->rb_node) {
+		if (parent->rb_left == node) {
+			other = parent->rb_right;
+			if (rb_is_red(other)) {
+				rb_set_black(other);
+				rb_set_red(parent);
+				__rb_rotate_left(parent, root);
+				other = parent->rb_right;
+			}
+			if ((!other->rb_left || rb_is_black(other->rb_left)) &&
+			    (!other->rb_right || rb_is_black(other->rb_right))) {
+				rb_set_red(other);
+				node = parent;
+				parent = rb_parent(node);
+			} else {
+				if (!other->rb_right || rb_is_black(other->rb_right)) {
+					rb_set_black(other->rb_left);
+					rb_set_red(other);
+					__rb_rotate_right(other, root);
+					other = parent->rb_right;
+				}
+				rb_set_color(other, rb_color(parent));
+				rb_set_black(parent);
+				rb_set_black(other->rb_right);
+				__rb_rotate_left(parent, root);
+				node = root->rb_node;
+				break;
+			}
+		} else {
+			other = parent->rb_left;
+			if (rb_is_red(other)) {
+				rb_set_black(other);
+				rb_set_red(parent);
+				__rb_rotate_right(parent, root);
+				other = parent->rb_left;
+			}
+			if ((!other->rb_left || rb_is_black(other->rb_left)) &&
+			    (!other->rb_right || rb_is_black(other->rb_right))) {
+				rb_set_red(other);
+				node = parent;
+				parent = rb_parent(node);
+			} else {
+				if (!other->rb_left || rb_is_black(other->rb_left)) {
+					rb_set_black(other->rb_right);
+					rb_set_red(other);
+					__rb_rotate_left(other, root);
+					other = parent->rb_left;
+				}
+				rb_set_color(other, rb_color(parent));
+				rb_set_black(parent);
+				rb_set_black(other->rb_left);
+				__rb_rotate_right(parent, root);
+				node = root->rb_node;
+				break;
+			}
+		}
+	}
+
+	if (node)
+		rb_set_black(node);
+}
+
+void rb_erase(struct rb_node *node, struct rb_root *root)
+{
+	struct rb_node *child, *parent;
+	int color;
+
+	if (!node->rb_left)
+		child = node->rb_right;
+	else if (!node->rb_right)
+		child = node->rb_left;
+	else {
+		struct rb_node *old = node, *left;
+
+		node = node->rb_right;
+		while ((left = node->rb_left))
+			node = left;
+
+		if (rb_parent(old)) {
+			if (rb_parent(old)->rb_left == old)
+				rb_parent(old)->rb_left = node;
+			else
+				rb_parent(old)->rb_right = node;
+		} else
+			root->rb_node = node;
+
+		child = node->rb_right;
+		parent = rb_parent(node);
+		color = rb_color(node);
+
+		if (parent == old) {
+			parent = node;
+		} else {
+			if (child)
+				rb_set_parent(child, parent);
+			parent->rb_left = child;
+
+			node->rb_right = old->rb_right;
+			rb_set_parent(old->rb_right, node);
+		}
+
+		node->rb_parent_color = old->rb_parent_color;
+		node->rb_left = old->rb_left;
+		rb_set_parent(old->rb_left, node);
+
+		goto color;
+	}
+
+	parent = rb_parent(node);
+	color = rb_color(node);
+
+	if (child)
+		rb_set_parent(child, parent);
+
+	if (parent) {
+		if (parent->rb_left == node)
+			parent->rb_left = child;
+		else
+			parent->rb_right = child;
+	} else
+		root->rb_node = child;
+
+color:
+	if (color == RB_BLACK)
+		__rb_erase_color(child, parent, root);
+}
+
+/*
+ * This function returns the first node (in sort order) of the tree.
+ */
+struct rb_node *rb_first(const struct rb_root *root)
+{
+	struct rb_node *n;
+
+	n = root->rb_node;
+	if (!n)
+		return NULL;
+
+	while (n->rb_left)
+		n = n->rb_left;
+
+	return n;
+}
+
+struct rb_node *rb_last(const struct rb_root *root)
+{
+	struct rb_node *n;
+
+	n = root->rb_node;
+	if (!n)
+		return NULL;
+
+	while (n->rb_right)
+		n = n->rb_right;
+
+	return n;
+}
+
+struct rb_node *rb_next(const struct rb_node *node)
+{
+	struct rb_node *parent;
+
+	if (rb_parent(node) == node)
+		return NULL;
+
+	/*
+	 * If we have a right-hand child, go down and
+	 * then left as far as we can.
+	 */
+	if (node->rb_right) {
+		node = node->rb_right;
+		while (node->rb_left)
+			node = node->rb_left;
+		return (struct rb_node *)node;
+	}
+
+	/*
+	 * No right-hand children.  Everything down and left is
+	 * smaller than us, so any 'next' node must be in the general
+	 * direction of our parent. Go up the tree; any time the
+	 * ancestor is a right-hand child of its parent, keep going
+	 * up. First time it's a left-hand child of its parent, said
+	 * parent is our 'next' node.
+	 */
+	while ((parent = rb_parent(node)) && node == parent->rb_right)
+		node = parent;
+
+	return parent;
+}
+
+struct rb_node *rb_prev(const struct rb_node *node)
+{
+	struct rb_node *parent;
+
+	if (rb_parent(node) == node)
+		return NULL;
+
+	/*
+	 * If we have a left-hand child, go down and
+	 * then right as far as we can.
+	 */
+	if (node->rb_left) {
+		node = node->rb_left;
+		while (node->rb_right)
+			node = node->rb_right;
+		return (struct rb_node *)node;
+	}
+
+	/*
+	 * No left-hand children. Go up till we find
+	 * an ancestor which is a right-hand child of its parent.
+	 */
+	while ((parent = rb_parent(node)) && node == parent->rb_left)
+		node = parent;
+
+	return parent;
+}
+
+void rb_replace_node(struct rb_node *victim, struct rb_node *new, struct rb_root *root)
+{
+	struct rb_node *parent = rb_parent(victim);
+
+	/* Set the surrounding nodes to point to the replacement */
+	if (parent) {
+		if (victim == parent->rb_left)
+			parent->rb_left = new;
+		else
+			parent->rb_right = new;
+	} else
+		root->rb_node = new;
+
+	if (victim->rb_left)
+		rb_set_parent(victim->rb_left, new);
+
+	if (victim->rb_right)
+		rb_set_parent(victim->rb_right, new);
+
+	/* Copy the pointers/colour from the victim to the replacement */
+	*new = *victim;
+}
+
+/* rbtree_wrap */
+enum TRAV_DIRECTION {
+	GO_LEFT,
+	GO_RIGHT,
+	NO_DIRECTION,
+};
+
+struct rb_node *___search(const struct rb_node *target, const struct rbtree_struct *rbtree,
+						struct rb_node **p_parent, struct rb_node ***p_insert, enum search_ops ops,
+						int (*compare)(const struct rb_node*, const struct rb_node*)) {
+	struct rb_node *parent = NULL;
+	struct rb_node **insert = &rbtree->tree.rb_node;
+	struct rb_node *node = rbtree->tree.rb_node;
+	enum TRAV_DIRECTION direction = NO_DIRECTION;
+
+	while(node) {
+		parent = node;
+
+		if(compare(target, node) < 0) {
+			node = node->rb_left;
+			insert = &(*insert)->rb_left;
+			direction = GO_LEFT;
+		}
+		else if(compare(target, node) > 0) {
+			node = node->rb_right;
+			insert = &(*insert)->rb_right;
+			direction = GO_RIGHT;
+		}
+		else {
+			switch(ops) {
+			case SEARCH_EXACTLY:
+			case SEARCH_LAST_PRECURSOR_INC_ITSELF:
+			case SEARCH_FIRST_SUCCESSOR_INC_ITSELF:
+				parent = NULL;
+				insert = NULL;
+				goto out;
+			case SEARCH_LAST_PRECURSOR:
+				node = node->rb_left;
+				insert = &(*insert)->rb_left;
+				direction = GO_LEFT;
+				break;
+			case SEARCH_FIRST_SUCCESSOR:
+				node = node->rb_right;
+				insert = &(*insert)->rb_right;
+				direction = GO_RIGHT;
+				break;
+			}
+		}
+	}
+
+	if((!parent) || (ops == SEARCH_EXACTLY))
+		goto out;
+
+	if(p_parent)
+		*p_parent = parent;
+
+	while(parent && (((ops & SEARCH_LAST_PRECURSOR) && direction == GO_LEFT) ||
+				((ops & SEARCH_FIRST_SUCCESSOR) && direction == GO_RIGHT))) {
+		struct rb_node *grandpa = rb_parent(parent);
+		if(!grandpa)
+			direction = NO_DIRECTION;
+		else if(grandpa->rb_left == parent)
+			direction = GO_LEFT;
+		else
+			direction = GO_RIGHT;
+		parent = grandpa;
+	}
+
+	node = parent;
+
+	if(p_insert)
+		*p_insert = insert;
+	
+	return node;
+
+out:
+	if(p_parent)
+		*p_parent = parent;
+	if(p_insert)
+		*p_insert = insert;
+	return node;
+}
diff --git a/libibverbs/rbtree.h b/libibverbs/rbtree.h
new file mode 100644
index 0000000..4538a94
--- /dev/null
+++ b/libibverbs/rbtree.h
@@ -0,0 +1,166 @@
+/*
+ * RBtree implementation adopted from the Linux kernel sources.
+ */
+
+#ifndef __MY_RBTREE_H__
+#define __MY_RBTREE_H__
+
+#include <stddef.h>
+
+#include "compiler.h"
+
+#define RB_RED			0
+#define RB_BLACK		1
+#define RB_MASK			3
+
+struct rb_node {
+	unsigned long rb_parent_color; /* Keeps both parent anc color */
+	struct rb_node *rb_right;
+	struct rb_node *rb_left;
+} __aligned(sizeof(long));
+
+struct rb_root {
+	struct rb_node *rb_node;
+};
+
+#define rb_parent(r)   ((struct rb_node *)((r)->rb_parent_color & ~RB_MASK))
+#define rb_color(r)    ((r)->rb_parent_color & RB_BLACK)
+#define rb_is_red(r)   (!rb_color(r))
+#define rb_is_black(r) (rb_color(r))
+#define rb_set_red(r)                              \
+	do {                                       \
+		(r)->rb_parent_color &= ~RB_BLACK; \
+	} while (0)
+#define rb_set_black(r)                           \
+	do {                                      \
+		(r)->rb_parent_color |= RB_BLACK; \
+	} while (0)
+
+static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)
+{
+	rb->rb_parent_color = (rb->rb_parent_color & RB_MASK) | (unsigned long)p;
+}
+
+static inline void rb_set_color(struct rb_node *rb, int color)
+{
+	rb->rb_parent_color = (rb->rb_parent_color & ~RB_BLACK) | color;
+}
+
+#define RB_ROOT          \
+	(struct rb_root) \
+	{                \
+		NULL,    \
+	}
+#define rb_entry(ptr, type, member) container_of(ptr, type, member)
+
+#define RB_EMPTY_ROOT(root) ((root)->rb_node == NULL)
+#define RB_EMPTY_NODE(node) (rb_parent(node) == node)
+#define RB_CLEAR_NODE(node) (rb_set_parent(node, node))
+
+static inline void rb_init_node(struct rb_node *node)
+{
+	*node = (struct rb_node){};
+
+	RB_CLEAR_NODE(node);
+}
+
+extern void rb_insert_color(struct rb_node *node, struct rb_root *root);
+extern void rb_erase(struct rb_node *node, struct rb_root *root);
+
+/* Find logical next and previous nodes in a tree */
+extern struct rb_node *rb_first(const struct rb_root *root);
+extern struct rb_node *rb_last(const struct rb_root *root);
+extern struct rb_node *rb_next(const struct rb_node *node);
+extern struct rb_node *rb_prev(const struct rb_node *node);
+
+/* Fast replacement of a single node without remove/rebalance/add/rebalance */
+extern void rb_replace_node(struct rb_node *victim, struct rb_node *new, struct rb_root *root);
+
+static inline void rb_link_node(struct rb_node *node, struct rb_node *parent, struct rb_node **rb_link)
+{
+	node->rb_parent_color = (unsigned long)parent;
+	node->rb_left = node->rb_right = NULL;
+
+	*rb_link = node;
+}
+
+static inline void rb_link_and_balance(struct rb_root *root, struct rb_node *node, struct rb_node *parent,
+				       struct rb_node **rb_link)
+{
+	rb_link_node(node, parent, rb_link);
+	rb_insert_color(node, root);
+}
+
+#undef RB_RED
+#undef RB_BLACK
+#undef RB_MASK
+
+/* rbtree_wrap */
+#include <pthread.h>
+
+struct rbtree_struct {
+	struct rb_root				tree;
+	pthread_rwlock_t			rwlock;
+};
+
+enum search_ops {
+	/* Search the element exactly the same as what is specified */
+	SEARCH_EXACTLY									= 0,
+	/* Search the last precursor of the specified item */
+	SEARCH_LAST_PRECURSOR							= 2,
+	/* Search the last precursor of the specified item.
+	 * If there is the item exactly the same as what is specified,
+	 * then return it */
+	SEARCH_LAST_PRECURSOR_INC_ITSELF				= 3,
+	/* Search the first successor of the specified item */
+	SEARCH_FIRST_SUCCESSOR							= 4,
+	/* Search the first successor of the specified item.
+	 * If there is the item exactly the same as what is specified,
+	 * then return it */
+	SEARCH_FIRST_SUCCESSOR_INC_ITSELF				= 5,
+};
+
+enum TRACE_DIRECTION {
+	LEFT,
+	RIGHT,
+};
+
+#define declare_and_init_rbtree(var)								\
+	struct rbtree_struct var = {									\
+		.tree				= RB_ROOT,								\
+		.rwlock				= PTHREAD_RWLOCK_INITIALIZER,			\
+	}
+
+extern struct rb_node *___search(const struct rb_node *target, const struct rbtree_struct *rbtree,
+						struct rb_node **p_parent, struct rb_node ***p_insert, enum search_ops ops,
+						int (*compare)(const struct rb_node*, const struct rb_node*));
+
+static inline void rbtree_add_node(struct rb_node *new_node, struct rb_node *parent,
+					struct rb_node **insert, struct rbtree_struct *rbtree) {
+	rb_link_node(new_node, parent, insert);
+	rb_insert_color(new_node, &rbtree->tree);
+}
+
+static inline void rbtree_rm_node(struct rb_node *target, struct rbtree_struct *rbtree) {
+	rb_erase(target, &rbtree->tree);
+}
+
+static inline void clean_rbtree(struct rbtree_struct *rbtree,
+				void (*free_fn)(struct rb_node *node)) {
+	struct rb_node *root_node;
+	while((root_node = rbtree->tree.rb_node)) {
+		rb_erase(root_node, &rbtree->tree);
+		free_fn(root_node);
+	}
+}
+
+#define for_each_rbtree_entry(entry, rbtree, to_entry_fn, member)					\
+	for(entry = to_entry_fn(rb_first(&(rbtree)->tree));								\
+			entry; entry = to_entry_fn(rb_next(&entry->member)))
+
+#define for_each_rbtree_entry_safe(entry, tmp, rbtree, to_entry_fn, member)			\
+	for(entry = to_entry_fn(rb_first(&(rbtree)->tree)),								\
+			tmp = entry? to_entry_fn(rb_next(&entry->member)): NULL;				\
+			entry; entry = tmp, tmp = entry? to_entry_fn(rb_next(&entry->member)): NULL)
+
+#endif /* __MY_RBTREE_H__ */
diff --git a/libibverbs/rdwr_flag.h b/libibverbs/rdwr_flag.h
new file mode 100644
index 0000000..f7ff649
--- /dev/null
+++ b/libibverbs/rdwr_flag.h
@@ -0,0 +1,9 @@
+#ifndef __RDWR_FLAG__
+#define __RDWR_FLAG__
+
+#define lkey_FLAG				O_RDONLY
+#define lkey_PROT				PROT_READ
+#define rkey_FLAG               O_RDONLY
+#define rkey_PROT               PROT_READ
+
+#endif
diff --git a/libibverbs/restore_core.c b/libibverbs/restore_core.c
new file mode 100644
index 0000000..6265d5e
--- /dev/null
+++ b/libibverbs/restore_core.c
@@ -0,0 +1,460 @@
+#if 0
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <unistd.h>
+#include <dirent.h>
+#include "driver.h"
+
+#define mv_fd(old_fd, new_fd) {									\
+	struct stat statbuf;										\
+	int __new_fd = (new_fd);									\
+	for(; !fstat(__new_fd, &statbuf); __new_fd++);				\
+																\
+	if(dup2(old_fd, __new_fd) < 0) {							\
+		if(old_fd >= 0)											\
+			close(old_fd);										\
+		old_fd = -1;											\
+	}															\
+	else {														\
+		close(old_fd);											\
+		old_fd = __new_fd;										\
+	}															\
+}
+
+#define def_restore(res, restore_info_fn, restore_sub_fn, free_fn)						\
+static int restore_rdma_##res(void *parent, int img_fd,									\
+						char *path, char *parent_path) {								\
+	void *(*__restore_info_fn)(void *, int, char *, char *, int *);						\
+	int (*__restore_sub_fn)(void *, int, char *);										\
+	void (*__free_fn)(void *);															\
+	int sub_img_fd;																		\
+	void *p_res;																		\
+	int err;																			\
+																						\
+	__restore_info_fn = restore_info_fn;												\
+	__restore_sub_fn = restore_sub_fn;													\
+	__free_fn = free_fn;																\
+																						\
+	if(!__restore_info_fn)																\
+		return -1;																		\
+																						\
+	sub_img_fd = openat(img_fd, path, O_DIRECTORY);										\
+	mv_fd(sub_img_fd, sub_img_fd + 1350);												\
+	if(sub_img_fd < 0) {																\
+		return -1;																		\
+	}																					\
+																						\
+	p_res = __restore_info_fn(parent, sub_img_fd, path, parent_path, &err);				\
+	if(err) {																			\
+		close(sub_img_fd);																\
+		return -1;																		\
+	}																					\
+																						\
+	if(__restore_sub_fn && __restore_sub_fn(p_res, sub_img_fd, path)) {					\
+		if(__free_fn)																	\
+			__free_fn(p_res);															\
+		close(sub_img_fd);																\
+		return -1;																		\
+	}																					\
+																						\
+	if(__free_fn)																		\
+		__free_fn(p_res);																\
+	close(sub_img_fd);																	\
+	return 0;																			\
+}
+
+#define dump_info(dir_fd, info_fd, param, info_name)									\
+	info_fd = openat(dir_fd, #info_name, O_RDONLY);										\
+	mv_fd(info_fd, info_fd + 1350);														\
+	if(info_fd < 0) {																	\
+		*p_err = -1;																	\
+		return NULL;																	\
+	}																					\
+																						\
+	if(read(info_fd, &(param)->info_name, sizeof((param)->info_name)) < 0) {			\
+		close(info_fd);																	\
+		*p_err = -1;																	\
+		return NULL;																	\
+	}																					\
+																						\
+	close(info_fd)
+
+#define dump_mmap(dir_fd, info_fd, param, map_field)									\
+	dump_info(dir_fd, info_fd, param, map_field##_mmap_fd);								\
+	/* dump_info(dir_fd, info_fd, param, map_field##_mmap_addr); */							\
+	dump_info(dir_fd, info_fd, param, map_field##_map)
+
+static void *restore_mr(void *parent, int mr_fd,
+						char *mr_path, char *parent_path, int *p_err) {
+	struct ibv_context *tmp_context = parent;
+	struct ibv_resume_mr_param mr_param;
+	int info_fd;
+	int pd_handle;
+	int mr_handle;
+
+	sscanf(parent_path, "pd_%d", &pd_handle);
+	sscanf(mr_path, "mr_%d", &mr_handle);
+	mr_param.pd_vhandle = pd_handle;
+	mr_param.mr_vhandle = mr_handle;
+	dump_info(mr_fd, info_fd, &mr_param, access_flags);
+	dump_info(mr_fd, info_fd, &mr_param, iova);
+	dump_info(mr_fd, info_fd, &mr_param, length);
+	dump_info(mr_fd, info_fd, &mr_param, vlkey);
+	dump_info(mr_fd, info_fd, &mr_param, vrkey);
+
+	*p_err = ibv_resume_mr(tmp_context, &mr_param);
+	return (*p_err)? NULL: parent;
+}
+
+def_restore(mr, restore_mr, NULL, NULL);
+
+#define get_cq_handle(qp_dir_fd, cq) ({													\
+	char linkbuf[128];																	\
+	char tmp_buf[128];																	\
+	ssize_t linkbuf_sz;																	\
+	int cq_handle = -1;																	\
+																						\
+	linkbuf_sz = readlinkat(qp_dir_fd, #cq, linkbuf, sizeof(linkbuf));					\
+	if(linkbuf_sz >= 0) {																\
+		sprintf(tmp_buf, "%.*s", (int)linkbuf_sz, linkbuf);								\
+		sscanf(tmp_buf, "../../cq_%d", &cq_handle);										\
+	}																					\
+	cq_handle;																			\
+})
+
+#define dump_info_var(dir_fd, info_fd, var, info_name)									\
+	info_fd = openat(dir_fd, info_name, O_RDONLY);										\
+	mv_fd(info_fd, info_fd + 1350);														\
+	if(info_fd < 0) {																	\
+		*p_err = -1;																	\
+		return NULL;																	\
+	}																					\
+																						\
+	if(read(info_fd, var, sizeof(*var)) < 0) {											\
+		close(info_fd);																	\
+		*p_err = -1;																	\
+		return NULL;																	\
+	}																					\
+																						\
+	close(info_fd)
+
+static void *restore_qp(void *parent, int qp_fd,
+						char *qp_path, char *parent_path, int *p_err) {
+	struct ibv_context *tmp_context = parent;
+	struct ibv_qp *tmp_qp;
+	struct ibv_resume_qp_param qp_param;
+	int info_fd;
+	int pd_handle;
+	int qp_handle;
+	char info_name[32];
+	int i;
+
+	memset(&qp_param, 0, sizeof(qp_param));
+	info_fd = openat(qp_fd, "qp_ctx", O_RDONLY);
+	if(info_fd < 0) {
+		*p_err = -1;
+		return NULL;
+	}
+
+	if(read(info_fd, &qp_param, sizeof(qp_param)) < 0) {
+		close(info_fd);
+		*p_err = -1;
+		return NULL;
+	}
+
+	close(info_fd);
+
+	sscanf(parent_path, "pd_%d", &pd_handle);
+	sscanf(qp_path, "qp_%d", &qp_handle);
+	qp_param.pd_vhandle = pd_handle;
+	qp_param.qp_vhandle = qp_handle;
+	qp_param.send_cq_vhandle = get_cq_handle(qp_fd, send_cq);
+	qp_param.recv_cq_vhandle = get_cq_handle(qp_fd, recv_cq);
+
+#if 0
+	dump_info(qp_fd, info_fd, &qp_param, qp_state);
+	dump_info(qp_fd, info_fd, &qp_param, init_attr);
+	dump_info(qp_fd, info_fd, &qp_param, meta_uaddr);
+	dump_info(qp_fd, info_fd, &qp_param, vqpn);
+	dump_info(qp_fd, info_fd, &qp_param, buf_addr);
+	dump_info(qp_fd, info_fd, &qp_param, db_addr);
+	dump_info(qp_fd, info_fd, &qp_param, usr_idx);
+	dump_info(qp_fd, info_fd, &qp_param, signal_fd);
+
+	for(i = 0; i < qp_param.qp_state; i++) {
+		sprintf(info_name, "attr_%d", i);
+		dump_info_var(qp_fd, info_fd, &qp_param.modify_qp_attr[i], info_name);
+		sprintf(info_name, "mask_%d", i);
+		dump_info_var(qp_fd, info_fd, &qp_param.modify_qp_mask[i], info_name);
+	}
+#endif
+
+	tmp_qp = ibv_resume_create_qp(tmp_context, &qp_param);
+	if(!tmp_qp) {
+		*p_err = -1;
+		return NULL;
+	}
+
+	for(i = 0; i < qp_param.qp_state; i++) {
+		if(ibv_modify_qp(tmp_qp, &qp_param.modify_qp_attr[i],
+							qp_param.modify_qp_mask[i])) {
+			*p_err = -1;
+			return NULL;
+		}
+	}
+
+	ibv_resume_free_qp(tmp_qp);
+	*p_err = 0;
+	return parent;
+}
+
+def_restore(qp, restore_qp, NULL, NULL);
+
+static void *restore_cq(void *parent, int cq_fd,
+						char *cq_path, char *parent_path, int *p_err) {
+	struct ibv_context *tmp_context = parent;
+	struct ibv_resume_cq_param cq_param;
+	int info_fd;
+	int cq_handle;
+
+	sscanf(cq_path, "cq_%d", &cq_handle);
+
+	cq_param.cq_vhandle = cq_handle;
+
+	dump_info(cq_fd, info_fd, &cq_param, cq_size);
+	dump_info(cq_fd, info_fd, &cq_param, meta_uaddr);
+	dump_info(cq_fd, info_fd, &cq_param, buf_addr);
+	dump_info(cq_fd, info_fd, &cq_param, db_addr);
+
+	*p_err = ibv_resume_cq(tmp_context, &cq_param);
+	return (*p_err)? NULL: parent;
+}
+
+def_restore(cq, restore_cq, NULL, NULL);
+
+static void *restore_pd(void *parent, int pd_fd,
+				char *pd_path, char *parent_path, int *p_err) {
+	struct ibv_context *tmp_context = parent;
+	int pd_handle;
+
+	sscanf(pd_path, "pd_%d", &pd_handle);
+	*p_err = ibv_resume_pd(tmp_context, pd_handle);
+	return (*p_err)? NULL: parent;
+}
+
+static int restore_pd_sub(void *parent, int pd_fd, char *path) {
+	struct ibv_context *tmp_context = parent;
+	DIR *pd_dir;
+	struct dirent *pd_dirent;
+
+	pd_dir = fdopendir(pd_fd);
+	if(!pd_dir)
+		return -1;
+	
+	while((pd_dirent = readdir(pd_dir)) != NULL) {
+		if(!strncmp(pd_dirent->d_name, "mr", strlen("mr"))) {
+			if(restore_rdma_mr(tmp_context, pd_fd, pd_dirent->d_name, path)) {
+				return -1;
+			}
+		}
+
+		if(!strncmp(pd_dirent->d_name, "qp", strlen("qp"))) {
+			if(restore_rdma_qp(tmp_context, pd_fd, pd_dirent->d_name, path)) {
+				return -1;
+			}
+		}
+	}
+
+	return 0;
+}
+
+def_restore(pd, restore_pd, restore_pd_sub, NULL);
+
+static void *restore_context(void *parent, int cmd_fd,
+					char *cmd_fd_path, char *parent_path, int *p_err) {
+	struct ibv_context *tmp_context;
+	struct ibv_resume_context_param context_param;
+	char fname[128];
+	int info_fd;
+
+	if(parent) {
+		*p_err = -1;
+		return NULL;
+	}
+
+	memset(&context_param, 0, sizeof(context_param));
+	context_param.cmd_fd = atoi(cmd_fd_path);
+
+	dump_info(cmd_fd, info_fd, &context_param, cdev);
+	dump_info(cmd_fd, info_fd, &context_param, async_fd);
+	dump_info(cmd_fd, info_fd, &context_param, gid_table);
+
+	dump_mmap(cmd_fd, info_fd, &context_param, lkey);
+	dump_mmap(cmd_fd, info_fd, &context_param, local_rkey);
+	dump_mmap(cmd_fd, info_fd, &context_param, lqpn);
+	dump_mmap(cmd_fd, info_fd, &context_param, rkey);
+	dump_mmap(cmd_fd, info_fd, &context_param, rqpn);
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/lkey_mmap_addr",
+					getpid(), context_param.cmd_fd);
+	info_fd = open(fname, O_RDONLY);
+	if(info_fd < 0) {
+		return -1;
+	}
+
+	if(read(info_fd, &context_param.lkey_mmap_addr,
+				sizeof(context_param.lkey_mmap_addr)) < 0) {
+		close(info_fd);
+		return -1;
+	}
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/local_rkey_mmap_addr",
+					getpid(), context_param.cmd_fd);
+	info_fd = open(fname, O_RDONLY);
+	if(info_fd < 0) {
+		return -1;
+	}
+
+	if(read(info_fd, &context_param.local_rkey_mmap_addr,
+				sizeof(context_param.local_rkey_mmap_addr)) < 0) {
+		close(info_fd);
+		return -1;
+	}
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/lqpn_mmap_addr",
+					getpid(), context_param.cmd_fd);
+	info_fd = open(fname, O_RDONLY);
+	if(info_fd < 0) {
+		return -1;
+	}
+
+	if(read(info_fd, &context_param.lqpn_mmap_addr,
+				sizeof(context_param.lqpn_mmap_addr)) < 0) {
+		close(info_fd);
+		return -1;
+	}
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/rkey_mmap_addr",
+					getpid(), context_param.cmd_fd);
+	info_fd = open(fname, O_RDONLY);
+	if(info_fd < 0) {
+		return -1;
+	}
+
+	if(read(info_fd, &context_param.rkey_mmap_addr,
+				sizeof(context_param.rkey_mmap_addr)) < 0) {
+		close(info_fd);
+		return -1;
+	}
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/rqpn_mmap_addr",
+					getpid(), context_param.cmd_fd);
+	info_fd = open(fname, O_RDONLY);
+	if(info_fd < 0) {
+		return -1;
+	}
+
+	if(read(info_fd, &context_param.rqpn_mmap_addr,
+				sizeof(context_param.rqpn_mmap_addr)) < 0) {
+		close(info_fd);
+		return -1;
+	}
+
+	tmp_context = ibv_resume_context(&context_param);
+	if(!tmp_context) {
+		*p_err = -1;
+		return NULL;
+	}
+
+	*p_err = 0;
+	return tmp_context;
+}
+
+static int restore_context_sub(void *g_tmp_context, int cmd_fd, char *path) {
+	struct ibv_context *tmp_context = g_tmp_context;
+	DIR *cmd_dir;
+	struct dirent *cmd_dirent;
+
+	cmd_dir = fdopendir(cmd_fd);
+	if(!cmd_dir)
+		return -1;
+	
+	while((cmd_dirent = readdir(cmd_dir)) != NULL) {
+		if(!strncmp(cmd_dirent->d_name, "cq", strlen("cq"))) {
+			if(restore_rdma_cq(tmp_context, cmd_fd, cmd_dirent->d_name, path)) {
+				return -1;
+			}
+		}
+	}
+
+	if(lseek(cmd_fd, 0, SEEK_SET) < 0) {
+		return -1;
+	}
+
+	while((cmd_dirent = readdir(cmd_dir)) != NULL) {
+		if(!strncmp(cmd_dirent->d_name, "pd", strlen("pd"))) {
+			if(restore_rdma_pd(tmp_context, cmd_fd, cmd_dirent->d_name, path)) {
+				return -1;
+			}
+		}
+	}
+
+	return 0;
+}
+
+static inline void free_context(void *g_tmp_context) {
+	struct ibv_context *tmp_context = g_tmp_context;
+	ibv_free_tmp_context(tmp_context);
+}
+
+def_restore(context, restore_context, restore_context_sub, free_context);
+
+int restore_rdma(void) {
+	char fname[128];
+	int img_pid_fd = 5000;
+	DIR *img_pid_DIR;
+	struct dirent *img_pid_dirent;
+	struct stat st;
+
+	if(fstat(img_pid_fd, &st))
+		return 0;
+	
+	sprintf(fname, "rdma_pid_%d", getpid());
+
+	img_pid_DIR = fdopendir(img_pid_fd);
+	if(!img_pid_DIR) {
+		close(img_pid_fd);
+		return -1;
+	}
+
+	if(lseek(img_pid_fd, 0, SEEK_SET) < 0) {
+		return -1;
+	}
+
+	while((img_pid_dirent = readdir(img_pid_DIR)) != NULL) {
+		struct stat st;
+
+		if(!strncmp(img_pid_dirent->d_name, ".", strlen(".")))
+			continue;
+
+		if(fstatat(img_pid_fd, img_pid_dirent->d_name, &st, 0)) {
+			close(img_pid_fd);
+			return -1;
+		}
+
+		if(!S_ISDIR(st.st_mode))
+			continue;
+
+		if(restore_rdma_context(NULL, img_pid_fd,
+						img_pid_dirent->d_name, fname)) {
+			close(img_pid_fd);
+			return -1;
+		}
+	}
+
+	close(img_pid_fd);
+
+	return 0;
+}
+#endif
diff --git a/libibverbs/verbs.c b/libibverbs/verbs.c
index ee26b1d..e3246bb 100644
--- a/libibverbs/verbs.c
+++ b/libibverbs/verbs.c
@@ -54,6 +54,11 @@
 #include <net/if_arp.h>
 #include "neigh.h"
 
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <dirent.h>
+
 #undef ibv_query_port
 
 int __attribute__((const)) ibv_rate_to_mult(enum ibv_rate rate)
@@ -220,11 +225,36 @@ LATEST_SYMVER_FUNC(ibv_query_port, 1_1, "IBVERBS_1.1",
 				sizeof(*port_attr));
 }
 
+static int read_gid_table_from_uwrite(struct ibv_context *context, int index,
+									union ibv_gid *gid) {
+	char fname[128];
+	struct footprint_gid_entry entries[256];
+	int fd;
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/gid_table", rdma_getpid(context), context->cmd_fd);
+	fd = open(fname, O_RDONLY);
+	if(fd < 0) {
+		return errno;
+	}
+
+	if(read(fd, entries, sizeof(entries)) < 0) {
+		close(fd);
+		return errno;
+	}
+
+	close(fd);
+	memcpy(gid, &entries[index].gid, sizeof(*gid));
+
+	return 0;
+}
+
 LATEST_SYMVER_FUNC(ibv_query_gid, 1_1, "IBVERBS_1.1",
 		   int,
 		   struct ibv_context *context, uint8_t port_num,
 		   int index, union ibv_gid *gid)
 {
+	return read_gid_table_from_uwrite(context, index, gid);
+#if 0
 	struct ibv_gid_entry entry = {};
 	int ret;
 
@@ -240,7 +270,10 @@ LATEST_SYMVER_FUNC(ibv_query_gid, 1_1, "IBVERBS_1.1",
 
 	memcpy(gid, &entry.gid, sizeof(entry.gid));
 
+	read_gid_table_from_uwrite(context);
+
 	return 0;
+#endif
 }
 
 LATEST_SYMVER_FUNC(ibv_query_pkey, 1_1, "IBVERBS_1.1",
@@ -289,6 +322,28 @@ LATEST_SYMVER_FUNC(ibv_alloc_pd, 1_1, "IBVERBS_1.1",
 	if (pd)
 		pd->context = context;
 
+	if(ibv_cmd_install_pd_handle_mapping(context, pd->handle, pd->handle)) {
+		get_ops(pd->context)->dealloc_pd(pd);
+		return NULL;
+	}
+
+	return pd;
+}
+
+LATEST_SYMVER_FUNC(ibv_resume_pd, 1_1, "IBVERBS_1.1",
+			struct ibv_pd *, struct ibv_context *context, int vhandle) {
+	struct ibv_pd *pd;
+
+	pd = get_ops(context)->alloc_pd(context);
+	if (pd)
+		pd->context = context;
+	
+	if(ibv_cmd_install_pd_handle_mapping(context, vhandle, pd->handle)) {
+		get_ops(pd->context)->dealloc_pd(pd);
+		return NULL;
+	}
+
+	pd->handle = vhandle;
 	return pd;
 }
 
@@ -299,12 +354,40 @@ LATEST_SYMVER_FUNC(ibv_dealloc_pd, 1_1, "IBVERBS_1.1",
 	return get_ops(pd->context)->dealloc_pd(pd);
 }
 
+static uint32_t get_first_empty_slot_for_lkey(struct ibv_context *context) {
+	uint32_t *lkey_arr = context->lkey_mapping;
+	uint32_t i;
+
+	for(i = 0; i < getpagesize() / sizeof(uint32_t) && lkey_arr[i]; i++);
+
+	if(i >= getpagesize() / sizeof(uint32_t))
+		return -1;
+
+	return i;
+}
+
+static uint32_t get_first_empty_slot_for_rkey(struct ibv_context *context) {
+	uint32_t *rkey_arr = context->rkey_mapping;
+	uint32_t i;
+
+	for(i = 0; i < getpagesize() / sizeof(uint32_t) && rkey_arr[i]; i++);
+
+	if(i >= getpagesize() / sizeof(uint32_t))
+		return -1;
+
+	return i;
+}
+
 struct ibv_mr *ibv_reg_mr_iova2(struct ibv_pd *pd, void *addr, size_t length,
 				uint64_t iova, unsigned int access)
 {
 	struct verbs_device *device = verbs_get_device(pd->context->device);
 	bool odp_mr = access & IBV_ACCESS_ON_DEMAND;
 	struct ibv_mr *mr;
+	char fname[128];
+	int mr_dir_fd;
+	int info_fd;
+	uint32_t vlkey, vrkey;
 
 	if (!(device->core_support & IB_UVERBS_CORE_SUPPORT_OPTIONAL_MR_ACCESS))
 		access &= ~IBV_ACCESS_OPTIONAL_RANGE;
@@ -321,8 +404,70 @@ struct ibv_mr *ibv_reg_mr_iova2(struct ibv_pd *pd, void *addr, size_t length,
 	} else {
 		if (!odp_mr)
 			ibv_dofork_range(addr, length);
+		return NULL;
+	}
+
+	if(ibv_cmd_install_mr_handle_mapping(pd->context, mr->handle, mr->handle)) {
+		ibv_dereg_mr(mr);
+		return NULL;
+	}
+
+	vlkey = get_first_empty_slot_for_lkey(pd->context);
+	if(ibv_cmd_install_lkey_mapping(pd->context, vlkey, mr->lkey)) {
+		ibv_dereg_mr(mr);
+		return NULL;
+	}
+
+	mr->lkey = vlkey;
+
+	vrkey = get_first_empty_slot_for_rkey(pd->context);
+	if(ibv_cmd_install_local_rkey_mapping(pd->context, vrkey, mr->rkey)) {
+		ibv_dereg_mr(mr);
+		return NULL;
+	}
+
+	mr->rkey = vrkey;
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/mr_%d/",
+				rdma_getpid(pd->context), pd->context->cmd_fd, mr->handle);
+	mr_dir_fd = open(fname, O_DIRECTORY);
+	if(mr_dir_fd < 0) {
+		ibv_dereg_mr(mr);
+		return NULL;
+	}
+
+	info_fd = openat(mr_dir_fd, "vlkey", O_WRONLY);
+	if(info_fd < 0) {
+		close(mr_dir_fd);
+		ibv_dereg_mr(mr);
+		return NULL;
+	}
+
+	if(write(info_fd, &mr->lkey, sizeof(mr->lkey)) < 0) {
+		close(info_fd);
+		close(mr_dir_fd);
+		ibv_dereg_mr(mr);
+		return NULL;
 	}
 
+	close(info_fd);
+	info_fd = openat(mr_dir_fd, "vrkey", O_WRONLY);
+	if(info_fd < 0) {
+		close(mr_dir_fd);
+		ibv_dereg_mr(mr);
+		return NULL;
+	}
+
+	if(write(info_fd, &mr->rkey, sizeof(mr->rkey)) < 0) {
+		close(info_fd);
+		close(mr_dir_fd);
+		ibv_dereg_mr(mr);
+		return NULL;
+	}
+
+	close(info_fd);
+	close(mr_dir_fd);
+
 	return mr;
 }
 
@@ -335,6 +480,116 @@ LATEST_SYMVER_FUNC(ibv_reg_mr, 1_1, "IBVERBS_1.1",
 	return ibv_reg_mr_iova2(pd, addr, length, (uintptr_t)addr, access);
 }
 
+static struct ibv_mr *__ibv_reg_mr_iova2(struct ibv_pd *pd, void *addr, size_t length,
+				uint64_t iova, unsigned int access, int mr_handle, uint32_t vlkey, uint32_t vrkey)
+{
+	struct verbs_device *device = verbs_get_device(pd->context->device);
+	bool odp_mr = access & IBV_ACCESS_ON_DEMAND;
+	struct ibv_mr *mr;
+
+	if (!(device->core_support & IB_UVERBS_CORE_SUPPORT_OPTIONAL_MR_ACCESS))
+		access &= ~IBV_ACCESS_OPTIONAL_RANGE;
+
+	if (!odp_mr && ibv_dontfork_range(addr, length))
+		return NULL;
+
+	mr = get_ops(pd->context)->reg_mr(pd, addr, length, iova, access);
+	if (mr) {
+		mr->context = pd->context;
+		mr->pd      = pd;
+		mr->addr    = addr;
+		mr->length  = length;
+	} else {
+		if (!odp_mr)
+			ibv_dofork_range(addr, length);
+	}
+
+	if(ibv_cmd_install_mr_handle_mapping(pd->context, mr_handle, mr->handle)) {
+		ibv_dereg_mr(mr);
+		return NULL;
+	}
+
+	if(ibv_cmd_install_lkey_mapping(pd->context, vlkey, mr->lkey)) {
+		ibv_dereg_mr(mr);
+		return NULL;
+	}
+
+	if(ibv_cmd_install_local_rkey_mapping(pd->context, vrkey, mr->rkey)) {
+		ibv_dereg_mr(mr);
+		return NULL;
+	}
+
+	return mr;
+}
+
+LATEST_SYMVER_FUNC(ibv_resume_mr, 1_1, "IBVERBS_1.1",
+			int, struct ibv_context *context, struct ibv_pd *pd,
+					const struct ibv_resume_mr_param *mr_param) {
+	struct ibv_mr *mr;
+	char fname[128];
+	int mr_dir_fd;
+	int info_fd;
+
+	pd->handle = mr_param->pd_vhandle;
+	mr = __ibv_reg_mr_iova2(pd, mr_param->iova, mr_param->length,
+							(uintptr_t)mr_param->iova,
+							mr_param->access_flags, mr_param->mr_vhandle,
+							mr_param->vlkey, mr_param->vrkey);
+	if(!mr) {
+		return -1;
+	}
+
+	mr->context			= context;
+	mr->pd				= pd;
+	mr->addr			= mr_param->iova;
+	mr->length			= mr_param->length;
+	mr->handle			= mr_param->mr_vhandle;
+	mr->lkey			= mr_param->vlkey;
+	mr->rkey			= mr_param->vrkey;
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/mr_%d/",
+				rdma_getpid(context), pd->context->cmd_fd, mr_param->mr_vhandle);
+	mr_dir_fd = open(fname, O_DIRECTORY);
+	if(mr_dir_fd < 0) {
+		ibv_dereg_mr(mr);
+		return -errno;
+	}
+
+	info_fd = openat(mr_dir_fd, "vlkey", O_WRONLY);
+	if(info_fd < 0) {
+		close(mr_dir_fd);
+		ibv_dereg_mr(mr);
+		return -1;
+	}
+
+	if(write(info_fd, &mr->lkey, sizeof(mr->lkey)) < 0) {
+		close(info_fd);
+		close(mr_dir_fd);
+		ibv_dereg_mr(mr);
+		return -1;
+	}
+
+	close(info_fd);
+
+	info_fd = openat(mr_dir_fd, "vrkey", O_WRONLY);
+	if(info_fd < 0) {
+		close(mr_dir_fd);
+		ibv_dereg_mr(mr);
+		return -1;
+	}
+
+	if(write(info_fd, &mr->rkey, sizeof(mr->rkey)) < 0) {
+		close(info_fd);
+		close(mr_dir_fd);
+		ibv_dereg_mr(mr);
+		return -1;
+	}
+
+	close(info_fd);
+	close(mr_dir_fd);
+	return 0;
+}
+
 #undef ibv_reg_mr_iova
 struct ibv_mr *ibv_reg_mr_iova(struct ibv_pd *pd, void *addr, size_t length,
 			       uint64_t iova, int access)
@@ -480,6 +735,9 @@ LATEST_SYMVER_FUNC(ibv_dereg_mr, 1_1, "IBVERBS_1.1",
 	enum ibv_mr_type type	= verbs_get_mr(mr)->mr_type;
 	int access = verbs_get_mr(mr)->access;
 
+	ibv_cmd_delete_lkey_mapping(mr->context, mr->lkey);
+	ibv_cmd_delete_local_rkey_mapping(mr->context, mr->rkey);
+
 	ret = get_ops(mr->context)->dereg_mr(verbs_get_mr(mr));
 	if (!ret && type == IBV_MR_TYPE_MR && !(access & IBV_ACCESS_ON_DEMAND))
 		ibv_dofork_range(addr, length);
@@ -511,6 +769,49 @@ struct ibv_comp_channel *ibv_create_comp_channel(struct ibv_context *context)
 	return channel;
 }
 
+int ibv_resume_comp_channel(struct ibv_context *context, int comp_fd) {
+	struct ibv_create_comp_channel req;
+	struct ib_uverbs_create_comp_channel_resp resp;
+	struct ibv_comp_channel            *channel;
+
+	channel = malloc(sizeof *channel);
+	if (!channel)
+		return -1;
+
+	req.core_payload = (struct ib_uverbs_create_comp_channel){};
+	if (execute_cmd_write(context, IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL,
+			      &req, sizeof(req), &resp, sizeof(resp))) {
+		free(channel);
+		return -1;
+	}
+
+	if(resp.fd != comp_fd && dup2(resp.fd, comp_fd) < 0) {
+		free(channel);
+		return -1;
+	}
+
+	if(resp.fd != comp_fd) {
+		close(resp.fd);
+		resp.fd = comp_fd;
+	}
+
+	channel->context = context;
+	channel->fd      = resp.fd;
+	channel->refcnt  = 0;
+
+	if(add_comp_channel(channel->fd, channel)) {
+		free(channel);
+		return -1;
+	}
+
+	if(ibv_cmd_update_comp_channel_fd(context, channel)) {
+		free(channel);
+		return -1;
+	}
+
+	return 0;
+}
+
 int ibv_destroy_comp_channel(struct ibv_comp_channel *channel)
 {
 	struct ibv_context *context;
@@ -534,6 +835,30 @@ out:
 	return ret;
 }
 
+static int write_cq_meta_uaddr(struct ibv_context *context,
+						struct ibv_cq *cq, void *cq_meta_addr) {
+	char meta_uaddr_fname[128];
+	int fd;
+	ssize_t size;
+
+	sprintf(meta_uaddr_fname, "/proc/rdma_uwrite/%d/%d/cq_%d/meta_uaddr",
+					rdma_getpid(context), context->cmd_fd, cq->handle);
+	
+	fd = open(meta_uaddr_fname, O_WRONLY);
+	if(fd < 0) {
+		return -1;
+	}
+
+	size = write(fd, &cq_meta_addr, sizeof(cq_meta_addr));
+	if(size < 0) {
+		close(fd);
+		return -1;
+	}
+
+	close(fd);
+	return 0;
+}
+
 LATEST_SYMVER_FUNC(ibv_create_cq, 1_1, "IBVERBS_1.1",
 		   struct ibv_cq *,
 		   struct ibv_context *context, int cqe, void *cq_context,
@@ -545,6 +870,70 @@ LATEST_SYMVER_FUNC(ibv_create_cq, 1_1, "IBVERBS_1.1",
 
 	if (cq)
 		verbs_init_cq(cq, context, channel, cq_context);
+	
+	if(!cq)
+		return NULL;
+
+	if(write_cq_meta_uaddr(context, cq, cq)) {
+		ibv_destroy_cq(cq);
+		return NULL;
+	}
+
+	if(rbtree_add_cq(cq)) {
+		ibv_destroy_cq(cq);
+		return NULL;
+	}
+
+	cq->arm_flag = 0;
+	cq->wc = NULL;
+	cq->qps = NULL;
+	cq->srqs = NULL;
+	cq->stop_flag = 0;
+
+	return cq;
+}
+
+LATEST_SYMVER_FUNC(ibv_resume_cq, 1_1, "IBVERBS_1.1",
+			struct ibv_cq *, struct ibv_context *context,
+			const struct ibv_resume_cq_param *cq_param) {
+	struct ibv_cq *cq;
+	struct ibv_comp_channel *channel;
+	struct ibv_cq *orig_cq = cq_param->meta_uaddr;
+
+	channel = get_comp_channel_from_fd(cq_param->comp_fd);
+
+	if(get_ops(context)->uwrite_cq((struct ibv_cq *)cq_param->meta_uaddr, 0)) {
+		return NULL;
+	}
+
+	cq = get_ops(context)->resume_cq(context, cq_param->meta_uaddr, cq_param->cq_size,
+				channel, 0, cq_param->buf_addr, cq_param->db_addr, cq_param->cq_vhandle);
+	if(!cq)
+		return NULL;
+
+	cq->context = context;
+	cq->handle = cq_param->cq_vhandle;
+	if(write_cq_meta_uaddr(context, cq, cq_param->meta_uaddr)) {
+		ibv_destroy_cq(cq);
+		return NULL;
+	}
+
+	if(rbtree_add_cq(cq_param->meta_uaddr)) {
+		ibv_destroy_cq(cq);
+		return NULL;
+	}
+
+	cq->arm_flag = 0;
+
+	{
+		typeof(orig_cq->stop_flag) *content_p;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = 0;
+		if(register_update_mem(&orig_cq->stop_flag, sizeof(*content_p), content_p)) {
+			ibv_destroy_cq(cq);
+			return NULL;
+		}
+	}
 
 	return cq;
 }
@@ -563,6 +952,8 @@ LATEST_SYMVER_FUNC(ibv_destroy_cq, 1_1, "IBVERBS_1.1",
 	struct ibv_comp_channel *channel = cq->channel;
 	int ret;
 
+	rbtree_del_cq(cq);
+
 	ret = get_ops(cq->context)->destroy_cq(cq);
 
 	if (channel) {
@@ -589,7 +980,14 @@ LATEST_SYMVER_FUNC(ibv_get_cq_event, 1_1, "IBVERBS_1.1",
 	*cq         = (struct ibv_cq *) (uintptr_t) ev.cq_handle;
 	*cq_context = (*cq)->cq_context;
 
-	get_ops((*cq)->context)->cq_event(*cq);
+	(*cq)->arm_flag = 0;
+	if(ev.flag)
+		get_ops((*cq)->context)->cq_event(*cq);
+	else {
+		pthread_mutex_lock(&(*cq)->mutex);
+		(*cq)->comp_events_completed--;
+		pthread_mutex_unlock(&(*cq)->mutex);
+	}
 
 	return 0;
 }
@@ -604,6 +1002,50 @@ LATEST_SYMVER_FUNC(ibv_ack_cq_events, 1_1, "IBVERBS_1.1",
 	pthread_mutex_unlock(&cq->mutex);
 }
 
+static int write_srq_umeta_addr(struct ibv_context *context, struct ibv_srq *srq,
+				void *srq_meta_addr, struct ibv_srq_init_attr *srq_init_attr) {
+	char srq_dir_name[128];
+	int srq_dir_fd;
+	int info_fd;
+
+	sprintf(srq_dir_name, "/proc/rdma_uwrite/%d/%d/srq_%d/",
+					rdma_getpid(context), context->cmd_fd, srq->handle);
+	srq_dir_fd = open(srq_dir_name, O_DIRECTORY);
+	if(srq_dir_fd < 0) {
+		return -1;
+	}
+
+	info_fd = openat(srq_dir_fd, "meta_uaddr", O_WRONLY);
+	if(info_fd < 0) {
+		close(srq_dir_fd);
+		return -1;
+	}
+
+	if(write(info_fd, &srq_meta_addr, sizeof(srq_meta_addr)) < 0) {
+		close(info_fd);
+		close(srq_dir_fd);
+		return -1;
+	}
+
+	close(info_fd);
+
+	info_fd = openat(srq_dir_fd, "srq_init_attr", O_WRONLY);
+	if(info_fd < 0) {
+		close(srq_dir_fd);
+		return -1;
+	}
+
+	if(write(info_fd, srq_init_attr, sizeof(*srq_init_attr)) < 0) {
+		close(info_fd);
+		close(srq_dir_fd);
+		return -1;
+	}
+
+	close(info_fd);
+	close(srq_dir_fd);
+	return 0;
+}
+
 LATEST_SYMVER_FUNC(ibv_create_srq, 1_1, "IBVERBS_1.1",
 		   struct ibv_srq *,
 		   struct ibv_pd *pd,
@@ -621,6 +1063,58 @@ LATEST_SYMVER_FUNC(ibv_create_srq, 1_1, "IBVERBS_1.1",
 		pthread_cond_init(&srq->cond, NULL);
 	}
 
+	srq->wait_srq_node = NULL;
+
+	if(write_srq_umeta_addr(pd->context, srq, srq, srq_init_attr)) {
+		ibv_destroy_srq(srq);
+		return NULL;
+	}
+
+	if(rbtree_add_srq(srq)) {
+		ibv_destroy_srq(srq);
+		return NULL;
+	}
+
+	return srq;
+}
+
+LATEST_SYMVER_FUNC(ibv_resume_srq, 1_1, "IBVERBS_1.1",
+		   struct ibv_srq *,
+		   struct ibv_pd *pd,
+		   struct ibv_resume_srq_param *srq_param) {
+	struct ibv_srq *srq;
+
+	if(get_ops(pd->context)->uwrite_srq(srq_param->meta_uaddr, 0)) {
+		return NULL;
+	}
+
+	pd->handle = srq_param->pd_vhandle;
+	srq = get_ops(pd->context)->resume_srq(pd, srq_param);
+	if(!srq) {
+		return NULL;
+	}
+
+	if (srq) {
+		srq->context          = pd->context;
+		srq->srq_context      = srq_param->init_attr.srq_context;
+		srq->pd               = pd;
+		srq->events_completed = 0;
+		pthread_mutex_init(&srq->mutex, NULL);
+		pthread_cond_init(&srq->cond, NULL);
+	}
+
+	if(write_srq_umeta_addr(pd->context, srq, srq_param->meta_uaddr,
+						&srq_param->init_attr)) {
+		ibv_destroy_srq(srq);
+		return NULL;
+	}
+
+	if(add_srq_switch_node(srq, srq_param->meta_uaddr)) {
+		perror("add_srq_switch_node");
+		ibv_destroy_srq(srq);
+		return NULL;
+	}
+
 	return srq;
 }
 
@@ -644,19 +1138,417 @@ LATEST_SYMVER_FUNC(ibv_destroy_srq, 1_1, "IBVERBS_1.1",
 		   int,
 		   struct ibv_srq *srq)
 {
+	rbtree_del_srq(srq);
 	return get_ops(srq->context)->destroy_srq(srq);
 }
 
+static int write_qp_umeta_addr(struct ibv_context *context, struct ibv_qp *qp,
+					void *qp_meta_addr, struct ibv_qp_init_attr *qp_init_attr,
+					uint32_t vqpn) {
+	char qp_dir_fname[128];
+	int qp_dir_fd;
+	int info_fd;
+
+	sprintf(qp_dir_fname, "/proc/rdma_uwrite/%d/%d/qp_%d/",
+						rdma_getpid(context), context->cmd_fd, qp->handle);
+	
+	qp_dir_fd = open(qp_dir_fname, O_DIRECTORY);
+	if(qp_dir_fd < 0) {
+		return -1;
+	}
+
+	info_fd = openat(qp_dir_fd, "meta_uaddr", O_WRONLY);
+	if(info_fd < 0) {
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	if(write(info_fd, &qp_meta_addr, sizeof(qp_meta_addr)) < 0) {
+		close(info_fd);
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	close(info_fd);
+
+	info_fd = openat(qp_dir_fd, "send_cq_handle", O_WRONLY);
+	if(info_fd < 0) {
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	if(write(info_fd, &qp->send_cq->handle, sizeof(qp->send_cq->handle)) < 0) {
+		close(info_fd);
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	close(info_fd);
+
+	info_fd = openat(qp_dir_fd, "recv_cq_handle", O_WRONLY);
+	if(info_fd < 0) {
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	if(write(info_fd, &qp->recv_cq->handle, sizeof(qp->recv_cq->handle)) < 0) {
+		close(info_fd);
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	close(info_fd);
+
+	info_fd = openat(qp_dir_fd, "init_attr", O_WRONLY);
+	if(info_fd < 0) {
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	if(write(info_fd, qp_init_attr, sizeof(*qp_init_attr)) < 0) {
+		close(info_fd);
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	close(info_fd);
+
+	info_fd = openat(qp_dir_fd, "vqpn", O_WRONLY);
+	if(info_fd < 0) {
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	if(write(info_fd, &vqpn, sizeof(vqpn)) < 0) {
+		close(info_fd);
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	close(info_fd);
+	close(qp_dir_fd);
+	return 0;
+}
+
+static void pid_service(void *arg) {
+	struct ibv_qp *qp = arg;
+	struct sockaddr_in local_addr, remote_addr;
+	socklen_t addrlen = sizeof(struct sockaddr_in);
+	int sock;
+	pid_t pid;
+	ssize_t size;
+	uint32_t qpn;
+	int rcv_buf_sz = 4;
+	int reuse = 1;
+
+	sock = socket(AF_INET, SOCK_DGRAM, 0);
+	if(sock < 0) {
+		return;
+	}
+
+	if(setsockopt(sock, SOL_SOCKET, SO_RCVBUF, &rcv_buf_sz, sizeof(int))) {
+		close(sock);
+		exit(1);
+	}
+
+	if(setsockopt(sock, SOL_SOCKET, SO_REUSEPORT, &reuse, sizeof(int))) {
+		close(sock);
+		exit(1);
+	}
+
+	local_addr.sin_family = AF_INET;
+	local_addr.sin_port = htons(qp->qp_num % 65536 + 8000);
+	local_addr.sin_addr.s_addr = htonl(INADDR_ANY);
+	if(bind(sock, (struct sockaddr*)&local_addr, sizeof(local_addr))) {
+		close(sock);
+		exit(1);
+	}
+
+	size = recvfrom(sock, &pid, sizeof(pid), 0, &remote_addr, &addrlen);
+	if(size != sizeof(pid)) {
+		close(sock);
+		exit(1);
+	}
+
+	pid = rdma_getpid(qp->context);
+	size = sendto(sock, &pid, sizeof(pid), 0, &remote_addr, sizeof(remote_addr));
+	if(size != sizeof(pid)) {
+		close(sock);
+		exit(1);
+	}
+
+	size = recvfrom(sock, &qpn, sizeof(qpn), 0, &remote_addr, &addrlen);
+	if(size != sizeof(qpn)) {
+		close(sock);
+		exit(1);
+	}
+
+	qpn = qp->real_qpn;
+	size = sendto(sock, &qpn, sizeof(qpn), 0, &remote_addr, sizeof(remote_addr));
+	if(size != sizeof(qpn)) {
+		close(sock);
+		exit(1);
+	}
+
+	close(sock);
+	return 0;
+}
+
+struct ibv_qp *ibv_pre_create_qp(struct ibv_pd *pd,
+				struct ibv_qp_init_attr *qp_init_attr, uint32_t vqpn) {
+	struct ibv_qp *qp = get_ops(pd->context)->create_qp(pd, qp_init_attr);
+	int info_fd;
+	char fname[128];
+	pthread_t thread_id;
+
+	if(!qp)
+		return NULL;
+
+	qp->real_qpn = qp->qp_num;
+
+	if(write_qp_umeta_addr(pd->context, qp, qp, qp_init_attr, qp->qp_num)) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	if(rbtree_add_qp(qp)) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	if(add_qpn_dict_node(qp)) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	qp->qp_num = vqpn;
+	if(qp->qp_type != IBV_QPT_UD)
+		pthread_create(&thread_id, NULL, pid_service, (void*)qp);
+
+	pthread_rwlock_init(&qp->rwlock, NULL);
+
+	if(ibv_cmd_install_qpndict(qp->context, qp->real_qpn,
+				qp->qp_num)) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	return qp;
+}
+
 LATEST_SYMVER_FUNC(ibv_create_qp, 1_1, "IBVERBS_1.1",
 		   struct ibv_qp *,
 		   struct ibv_pd *pd,
 		   struct ibv_qp_init_attr *qp_init_attr)
 {
 	struct ibv_qp *qp = get_ops(pd->context)->create_qp(pd, qp_init_attr);
+	int info_fd;
+	char fname[128];
+	pthread_t thread_id;
+
+	if(!qp)
+		return NULL;
 
+	qp->real_qpn = qp->qp_num;
+	qp->clear_qpndict_flag = 0;
+	qp->wait_qp_node = NULL;
+
+	if(write_qp_umeta_addr(pd->context, qp, qp, qp_init_attr, qp->qp_num)) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	if(rbtree_add_qp(qp)) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	if(add_qpn_dict_node(qp)) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	if(ibv_cmd_install_qpndict(qp->context, qp->real_qpn,
+				qp->qp_num)) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	if(qp->qp_type != IBV_QPT_UD)
+		pthread_create(&thread_id, NULL, pid_service, (void*)qp);
+
+	pthread_rwlock_init(&qp->rwlock, NULL);
 	return qp;
 }
 
+LATEST_SYMVER_FUNC(ibv_resume_create_qp, 1_1, "IBVERBS_1.1",
+			struct ibv_qp *, struct ibv_context *context, struct ibv_pd *pd,
+			struct ibv_cq *send_cq, struct ibv_cq *recv_cq, struct ibv_srq *srq,
+			const struct ibv_resume_qp_param *qp_param, unsigned long long *bf_reg) {
+	struct ibv_qp *qp_ptr = qp_param->meta_uaddr;
+	struct ibv_qp *qp;
+	struct ibv_qp_init_attr qp_init_attr;
+	char fname[128];
+	int info_fd;
+	int err;
+	pthread_t thread_id;
+
+	if(get_ops(context)->uwrite_qp(qp_ptr, qp)) {
+		return NULL;
+	}
+
+	memcpy(&qp_init_attr, &qp_param->init_attr, sizeof(qp_init_attr));
+
+	send_cq->handle = qp_param->send_cq_handle;
+	recv_cq->handle = qp_param->recv_cq_handle;
+	qp_init_attr.send_cq = send_cq;
+	qp_init_attr.recv_cq = recv_cq;
+	qp_init_attr.srq = srq;
+	
+	qp = get_ops(context)->resume_qp(context, qp_param->pd_vhandle, qp_param->qp_vhandle,
+					&qp_init_attr, qp_param->buf_addr, qp_param->db_addr,
+					qp_param->usr_idx, qp_param->meta_uaddr, bf_reg);
+	if(!qp) {
+		return NULL;
+	}
+
+	qp->pd = pd;
+	qp->send_cq = send_cq;
+	qp->recv_cq = recv_cq;
+	qp->real_qpn = qp->qp_num;
+	
+	qp->handle = qp_param->qp_vhandle;
+	if(write_qp_umeta_addr(context, qp, qp_param->meta_uaddr,
+						&qp_param->init_attr, qp_param->vqpn)) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	qp->qp_num = qp_param->vqpn;
+
+	qp_ptr->orig_real_qpn = qp_ptr->real_qpn;
+	{
+		typeof(qp_ptr->real_qpn) *content_p;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = qp_ptr->real_qpn;
+		if(register_update_mem(&qp_ptr->orig_real_qpn,
+						sizeof(*content_p), content_p)) {
+			ibv_destroy_qp(qp);
+			return NULL;
+		}
+	}
+	qp_ptr->real_qpn = qp->real_qpn;
+	{
+		typeof(qp->real_qpn) *content_p;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = qp->real_qpn;
+		if(register_update_mem(&qp_ptr->real_qpn,
+					sizeof(*content_p), content_p)) {
+			ibv_destroy_qp(qp);
+			return NULL;
+		}
+	}
+	memcpy(&qp_ptr->local_gid, &qp->local_gid, sizeof(union ibv_gid));
+	{
+		typeof(qp->local_gid) *content_p;
+		content_p = malloc(sizeof(*content_p));
+		memcpy(content_p, &qp->local_gid, sizeof(union ibv_gid));
+		if(register_update_mem(&qp_ptr->local_gid,
+					sizeof(*content_p), content_p)) {
+			ibv_destroy_qp(qp);
+			return NULL;
+		}
+	}
+
+	{
+		typeof(qp_ptr->pause_flag) *content_p;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = 0;
+		if(register_update_mem(&qp_ptr->pause_flag,
+					sizeof(*content_p), content_p)) {
+			ibv_destroy_qp(qp);
+			return NULL;
+		}
+	}
+
+	if(add_switch_list_node(qp->real_qpn, qp_ptr, qp)) {
+		perror("add_switch_list_node");
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	if(qp->qp_type != IBV_QPT_UD && qp_param->qp_state >= IBV_QPS_RTR)
+		pthread_create(&thread_id, NULL, pid_service, (void*)qp);
+
+	pthread_rwlock_init(&qp->rwlock, NULL);
+
+	if(ibv_cmd_install_qpndict(qp->context, qp->real_qpn,
+				qp->qp_num)) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	return qp;
+}
+
+LATEST_SYMVER_FUNC(ibv_resume_free_qp, 1_1, "IBVERBS_1.1",
+			void, struct ibv_qp *qp) {
+	get_ops(qp->context)->free_qp(qp);
+}
+
+static int replay_recv_wr_cb(struct ibv_qp *orig_qp, struct ibv_qp *new_qp) {
+	orig_qp->clear_qpndict_flag = 1;
+	return get_ops(new_qp->context)->prepare_qp_recv_replay(orig_qp, new_qp);
+}
+
+static int replay_srq_recv_wr_cb(struct ibv_srq *orig_srq, struct ibv_qp *new_srq,
+								int *head, int *tail) {
+	return get_ops(new_srq->context)->prepare_srq_replay(orig_srq, new_srq, head, tail);
+}
+
+static int iter_cq_insert_fake_comp_event(struct ibv_cq *cq,
+					void *entry, void *in_param) {
+	struct ib_uverbs_comp_event_desc desc;
+
+	if(!cq->arm_flag)
+		return 0;
+
+	if(!cq->channel)
+		return 0;
+
+	desc.cq_handle = cq;
+	if(write(cq->channel->fd, &desc, sizeof(desc)) < 0) {
+		return -1;
+	}
+
+	return 0;
+}
+
+LATEST_SYMVER_FUNC(ibv_prepare_for_replay, 1_1, "IBVERBS_1.1",
+			int, int (*qp_load_cb)(struct ibv_qp *orig_qp, void *replay_fn),
+			int (*srq_load_cb)(struct ibv_srq *orig_srq, void *replay_fn, int head, int tail)) {
+	return rbtree_traverse_cq(iter_cq_insert_fake_comp_event, NULL) ||
+			switch_all_qps(replay_recv_wr_cb, qp_load_cb) ||
+			switch_all_srqs(replay_srq_recv_wr_cb, srq_load_cb);
+}
+
+LATEST_SYMVER_FUNC(ibv_update_mem, 1_1, "IBVERBS_1.1",
+			int,
+			int (*update_mem_fn)(void *ptr, size_t size,
+								void *content_p),
+			int (*keep_mmap_fn)(unsigned long long start,
+								unsigned long long end)) {
+	int err;
+
+	err = update_all_mem(update_mem_fn);
+	if(err) {
+		return err;
+	}
+
+	err = keep_all_mmap(keep_mmap_fn);
+	return err;
+}
+
 struct ibv_qp_ex *ibv_qp_to_qp_ex(struct ibv_qp *qp)
 {
 	struct verbs_qp *vqp = (struct verbs_qp *)qp;
@@ -697,20 +1589,293 @@ int ibv_query_qp_data_in_order(struct ibv_qp *qp, enum ibv_wr_opcode op,
 #endif
 }
 
+static int write_modify_qp_attr(struct ibv_context *context, struct ibv_qp *qp,
+			struct ibv_qp_attr *attr, int attr_mask) {
+	char qp_dir_fname[128];
+	char info_fname[32];
+	int qp_dir_fd;
+	int attr_fd = -1;
+	int mask_fd = -1;
+
+	sprintf(qp_dir_fname, "/proc/rdma_uwrite/%d/%d/qp_%d/",
+					rdma_getpid(context), context->cmd_fd, qp->handle);
+	
+	qp_dir_fd = open(qp_dir_fname, O_DIRECTORY);
+	if(qp_dir_fd < 0) {
+		return -1;
+	}
+
+	if(attr->qp_state > IBV_QPS_RTS || attr->qp_state < IBV_QPS_INIT) {
+		close(qp_dir_fd);
+		return 0;
+	}
+
+	sprintf(info_fname, "attr_%d", attr->qp_state - 1);
+	attr_fd = openat(qp_dir_fd, info_fname, O_WRONLY);
+	sprintf(info_fname, "mask_%d", attr->qp_state - 1);
+	mask_fd = openat(qp_dir_fd, info_fname, O_WRONLY);
+
+	if(attr_fd < 0 || mask_fd < 0) {
+		if(attr_fd >= 0)
+			close(attr_fd);
+		if(mask_fd >= 0)
+			close(mask_fd);
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	if(write(attr_fd, attr, sizeof(*attr)) < 0) {
+		close(attr_fd);
+		close(mask_fd);
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	if(write(mask_fd, &attr_mask, sizeof(attr_mask)) < 0) {
+		close(attr_fd);
+		close(mask_fd);
+		close(qp_dir_fd);
+		return -1;
+	}
+
+	close(attr_fd);
+	close(mask_fd);
+	close(qp_dir_fd);
+	return 0;
+}
+
+#include "rbtree.h"
+
+static declare_and_init_rbtree(rendpoint_tree);
+
+struct rendpoint_entry {
+	union ibv_gid				rgid;
+	pid_t						rpid;
+	void						*rkey_arr;
+	struct rb_node				node;
+};
+
+static inline struct rendpoint_entry *to_rendpoint_entry(struct rb_node *n) {
+	return n? container_of(n, struct rendpoint_entry, node): NULL;
+}
+
+static inline int rendpoint_entry_compare(const struct rb_node *n1, const struct rb_node *n2) {
+	struct rendpoint_entry *ent1 = to_rendpoint_entry(n1);
+	struct rendpoint_entry *ent2 = to_rendpoint_entry(n2);
+	int cmp;
+
+	cmp = memcmp(&ent1->rgid, &ent2->rgid, sizeof(union ibv_gid));
+	if(cmp < 0) {
+		return -1;
+	}
+	else if(cmp > 0)
+		return 1;
+	else {
+		if(ent1->rpid < ent2->rpid)
+			return -1;
+		else if(ent1->rpid > ent2->rpid)
+			return 1;
+		else
+			return 0;
+	}
+}
+
+static struct rendpoint_entry *search_rendpoint_entry(union ibv_gid *gid, pid_t pid,
+					struct rb_node **p_parent, struct rb_node ***p_insert) {
+	struct rendpoint_entry target;
+	struct rb_node *match;
+
+	memcpy(&target.rgid, gid, sizeof(union ibv_gid));
+	target.rpid = pid;
+	match = ___search(&target.node, &rendpoint_tree, p_parent, p_insert,
+							SEARCH_EXACTLY, rendpoint_entry_compare);
+	return to_rendpoint_entry(match);
+}
+
+static int add_rendpoint_entry(union ibv_gid *gid, pid_t pid, void *addr) {
+	struct rendpoint_entry *ent;
+	struct rb_node *parent, **insert;
+
+	pthread_rwlock_wrlock(&rendpoint_tree.rwlock);
+	ent = search_rendpoint_entry(gid, pid, &parent, &insert);
+	if(ent) {
+		pthread_rwlock_unlock(&rendpoint_tree.rwlock);
+		return -EEXIST;
+	}
+
+	ent = malloc(sizeof(*ent));
+	if(!ent) {
+		pthread_rwlock_unlock(&rendpoint_tree.rwlock);
+		return -ENOMEM;
+	}
+
+	memcpy(&ent->rgid, gid, sizeof(union ibv_gid));
+	ent->rpid = pid;
+	ent->rkey_arr = addr;
+	rbtree_add_node(&ent->node, parent, insert, &rendpoint_tree);
+	pthread_rwlock_unlock(&rendpoint_tree.rwlock);
+
+	return 0;
+}
+
+static void *get_rkey_arr_from_rgid_and_rpid(union ibv_gid *gid, pid_t pid) {
+	struct rendpoint_entry *ent;
+	void *addr;
+
+	pthread_rwlock_rdlock(&rendpoint_tree.rwlock);
+	ent = search_rendpoint_entry(gid, pid, NULL, NULL);
+	if(!ent) {
+		pthread_rwlock_unlock(&rendpoint_tree.rwlock);
+		return NULL;
+	}
+
+	addr = ent->rkey_arr;
+	pthread_rwlock_unlock(&rendpoint_tree.rwlock);
+	return addr;
+}
+
 LATEST_SYMVER_FUNC(ibv_modify_qp, 1_1, "IBVERBS_1.1",
 		   int,
 		   struct ibv_qp *qp, struct ibv_qp_attr *attr,
 		   int attr_mask)
 {
 	int ret;
+	struct ibv_qp_attr tmp_attr;
+
+	memcpy(&tmp_attr, attr, sizeof(tmp_attr));
+
+	if(attr->qp_state == IBV_QPS_RTR &&
+					(attr_mask & (IBV_QP_STATE | IBV_QP_AV))) {
+		struct sockaddr_in remote_addr;
+		union ibv_gid local_gid;
+		int sock;
+		int err;
+		char fname[1024];
+		ssize_t size;
+		int info_fd;
+		int flags;
+
+		sock = socket(AF_INET, SOCK_DGRAM, 0);
+		if(sock < 0) {
+			return -1;
+		}
+
+		flags = fcntl(sock, F_GETFL, 0);
+		fcntl(sock, F_SETFL, flags | O_NONBLOCK);
+
+		ibv_query_gid(qp->context, 1, attr->ah_attr.grh.sgid_index, &local_gid);
+		memcpy(&qp->local_gid, &local_gid, sizeof(union ibv_gid));
+
+		remote_addr.sin_family = AF_INET;
+		remote_addr.sin_port = htons(attr->dest_qp_num % 65536 + 8000);
+		memcpy(&remote_addr.sin_addr.s_addr, &tmp_attr.ah_attr.grh.dgid.raw[12],
+						sizeof(uint32_t));
+
+		while((size = recvfrom(sock, &qp->dest_pid, sizeof(pid_t), 0, NULL, NULL)) < 0 && errno == EAGAIN) {
+			ssize_t this_size = sendto(sock, &qp->dest_pid, sizeof(pid_t), 0, &remote_addr, sizeof(remote_addr));
+			if(this_size != sizeof(pid_t)) {
+				close(sock);
+				return -1;
+			}
+		}
+		if(size != sizeof(pid_t)) {
+			close(sock);
+			return -1;
+		}
+
+//		printf("In %s(%d): this_pid: %d, dest_pid: %d\n", __FILE__, __LINE__, rdma_getpid(qp->context), qp->dest_pid);
+
+		qp->dest_vqpn = attr->dest_qp_num;
+		memcpy(&qp->rc_dest_gid, &tmp_attr.ah_attr.grh.dgid, sizeof(union ibv_gid));
+
+		while((size = recvfrom(sock, &qp->dest_qpn, sizeof(uint32_t), 0, NULL, NULL)) < 0 && errno == EAGAIN) {
+			ssize_t this_size = sendto(sock, &qp->dest_qpn, sizeof(uint32_t), 0, &remote_addr, sizeof(remote_addr));
+			if(this_size != sizeof(uint32_t)) {
+				close(sock);
+				return -1;
+			}
+		}
+		if(size != sizeof(uint32_t)) {
+			close(sock);
+			return -1;
+		}
+
+		close(sock);
+//		printf("In %s(%d): this_real_qpn: %d, dest_real_qpn: %d\n", __FILE__, __LINE__, qp->real_qpn, qp->dest_qpn);
+
+		sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/rc_dest_pgid", rdma_getpid(qp->context),
+									qp->context->cmd_fd, qp->handle);
+		info_fd = open(fname, O_WRONLY);
+		if(info_fd < 0) {
+			return -1;
+		}
 
-	ret = get_ops(qp->context)->modify_qp(qp, attr, attr_mask);
+		if(write(info_fd, &tmp_attr.ah_attr.grh.dgid, sizeof(union ibv_gid)) < 0) {
+			close(info_fd);
+			return -1;
+		}
+
+		close(info_fd);
+
+		sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/dest_pqpn", rdma_getpid(qp->context),
+									qp->context->cmd_fd, qp->handle);
+		info_fd = open(fname, O_WRONLY);
+		if(info_fd < 0) {
+			return -1;
+		}
+
+		if(write(info_fd, &qp->dest_qpn, sizeof(uint32_t)) < 0) {
+			close(info_fd);
+			return -1;
+		}
+
+		close(info_fd);
+		tmp_attr.dest_qp_num = qp->dest_qpn;
+
+		err = ibv_cmd_register_remote_gid_pid(qp->context,
+								&tmp_attr.ah_attr.grh.dgid, qp->dest_pid);
+		if(err) {
+			return -1;
+		}
+
+		sprintf(fname, "/proc/rdma_uwrite/%d/%d/<%02x%02x:%02x%02x:%02x%02x:%02x%02x:%02x%02x:%02x%02x:%02x%02x:%02x%02x>_%d",
+								rdma_getpid(qp->context), qp->context->cmd_fd,
+								tmp_attr.ah_attr.grh.dgid.raw[0], tmp_attr.ah_attr.grh.dgid.raw[1], tmp_attr.ah_attr.grh.dgid.raw[2], tmp_attr.ah_attr.grh.dgid.raw[3],
+								tmp_attr.ah_attr.grh.dgid.raw[4], tmp_attr.ah_attr.grh.dgid.raw[5], tmp_attr.ah_attr.grh.dgid.raw[6], tmp_attr.ah_attr.grh.dgid.raw[7],
+								tmp_attr.ah_attr.grh.dgid.raw[8], tmp_attr.ah_attr.grh.dgid.raw[9], tmp_attr.ah_attr.grh.dgid.raw[10], tmp_attr.ah_attr.grh.dgid.raw[11],
+								tmp_attr.ah_attr.grh.dgid.raw[12], tmp_attr.ah_attr.grh.dgid.raw[13], tmp_attr.ah_attr.grh.dgid.raw[14], tmp_attr.ah_attr.grh.dgid.raw[15],
+								qp->dest_pid);
+		qp->rkey_arr = get_rkey_arr_from_rgid_and_rpid(&tmp_attr.ah_attr.grh.dgid, qp->dest_pid);
+		if(!qp->rkey_arr) {
+			int fd = open(fname, O_RDWR);
+			if(fd < 0) {
+				return -1;
+			}
+			qp->rkey_arr = mmap(NULL, getpagesize(), PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
+			if(qp->rkey_arr == MAP_FAILED) {
+				close(fd);
+				return -1;
+			}
+
+			if(add_rendpoint_entry(&tmp_attr.ah_attr.grh.dgid, qp->dest_pid, qp->rkey_arr)) {
+				close(fd);
+				return -1;
+			}
+
+			close(fd);
+		}
+	}
+
+	ret = get_ops(qp->context)->modify_qp(qp, &tmp_attr, attr_mask);
 	if (ret)
 		return ret;
 
 	if (attr_mask & IBV_QP_STATE)
 		qp->state = attr->qp_state;
 
+	if(write_modify_qp_attr(qp->context, qp, attr, attr_mask))
+		return -1;
+
 	return 0;
 }
 
@@ -718,6 +1883,8 @@ LATEST_SYMVER_FUNC(ibv_destroy_qp, 1_1, "IBVERBS_1.1",
 		   int,
 		   struct ibv_qp *qp)
 {
+	del_qpn_dict_node(qp);
+	rbtree_del_qp(qp);
 	return get_ops(qp->context)->destroy_qp(qp);
 }
 
diff --git a/libibverbs/verbs.h b/libibverbs/verbs.h
index 3dd9a79..f2d3dec 100644
--- a/libibverbs/verbs.h
+++ b/libibverbs/verbs.h
@@ -1213,6 +1213,8 @@ struct ibv_srq {
 	pthread_mutex_t		mutex;
 	pthread_cond_t		cond;
 	uint32_t		events_completed;
+	uint64_t			cnt;
+	void			*wait_srq_node;
 };
 
 /*
@@ -1259,6 +1261,31 @@ struct ibv_qp {
 	pthread_mutex_t		mutex;
 	pthread_cond_t		cond;
 	uint32_t		events_completed;
+	uint32_t				real_qpn;
+	uint32_t				orig_real_qpn;
+	int					clear_qpndict_flag;
+	uint32_t				dest_vqpn;
+	uint32_t				dest_qpn;
+	pid_t					dest_pid;
+	uint32_t				*rkey_arr;
+	union ibv_gid			local_gid;
+	union ibv_gid			rc_dest_gid;
+
+	int						pause_flag;
+	pthread_rwlock_t		rwlock;
+
+	struct {
+		uint32_t			real_qpn;
+		uint32_t			virt_qpn;
+		unsigned long		color;
+		void				*left;
+		void				*right;
+		long				padding;
+
+	} old_dict_node;
+
+	int					touched;
+	void*					wait_qp_node;
 };
 
 struct ibv_qp_ex {
@@ -1461,6 +1488,11 @@ struct ibv_cq {
 	pthread_cond_t		cond;
 	uint32_t		comp_events_completed;
 	uint32_t		async_events_completed;
+	int				arm_flag;
+	struct ibv_wc	*wc;
+	struct ibv_qp	**qps;
+	struct ibv_srq	**srqs;
+	int				stop_flag;
 };
 
 struct ibv_poll_cq_attr {
@@ -1932,6 +1964,7 @@ struct ibv_device {
 	char			dev_path[IBV_SYSFS_PATH_MAX];
 	/* Path to infiniband class device in sysfs */
 	char			ibdev_path[IBV_SYSFS_PATH_MAX];
+	uint32_t		*qpn_dict;
 };
 
 struct _compat_ibv_port_attr;
@@ -1976,7 +2009,41 @@ struct ibv_context_ops {
 	void *(*_compat_attach_mcast)(void);
 	void *(*_compat_detach_mcast)(void);
 	void *(*_compat_async_event)(void);
-};
+
+	int (*_compat_uwrite_cq)(void);
+	void *(*_compat_resume_cq)(void);
+	int (*_compat_get_cons_index)(void);
+	void (*_compat_set_cons_index)(void);
+	void (*_compat_copy_cqe_to_shaded)(void);
+
+	int (*_compat_uwrite_qp)(void);
+	void *(*_compat_resume_qp)(void);
+	void (*_compat_free_qp)(void);
+	int (*_compat_is_q_empty)(void);
+	int (*_compat_copy_qp)(void);
+	void *(*_compat_calloc_qp)(void);
+	int (*_compat_replay_recv_wr)(void);
+	int (*_compat_prepare_qp_recv_replay)(void);
+	void (*_compat_record_qp_index)(void);
+
+	void *(*_compat_resume_srq)(void);
+
+	void (*_compat_migrrdma_start_poll)(void);
+	void (*_compat_migrrdma_end_poll)(void);
+	int (*_compat_migrrdma_poll_cq)(void);
+	void (*_compat_migrrdma_start_inspect_qp)(void);
+	void (*_compat_migrrdma_start_inspect_qp_v2)(void);
+	int (*_compat_migrrdma_is_q_empty)(void);
+	uint64_t (*_compat_qp_get_n_posted)(void);
+	uint64_t (*_compat_qp_get_n_acked)(void);
+	uint64_t (*_compat_srq_get_n_acked)(void);
+
+	int (*_compat_uwrite_srq)(void);
+	int (*_compat_replay_srq_recv_wr)(void);
+	int (*_compat_prepare_srq_replay)(void);
+};
+
+#include <sys/mman.h>
 
 struct ibv_context {
 	struct ibv_device      *device;
@@ -1986,6 +2053,11 @@ struct ibv_context {
 	int			num_comp_vectors;
 	pthread_mutex_t		mutex;
 	void		       *abi_compat;
+
+	int						lkey_fd;
+	void					*lkey_mapping;
+	int						rkey_fd;
+	void					*rkey_mapping;
 };
 
 enum ibv_cq_init_attr_mask {
@@ -2260,11 +2332,50 @@ int ibv_get_device_index(struct ibv_device *device);
  */
 __be64 ibv_get_device_guid(struct ibv_device *device);
 
+struct verbs_device *get_verbs_device(struct ibv_device **p_ib_dev,
+				struct ibv_device **dev_list, int cmd_fd);
+
+pid_t rdma_getpid(struct ibv_context *context);
+
 /**
  * ibv_open_device - Initialize device for use
  */
 struct ibv_context *ibv_open_device(struct ibv_device *device);
 
+struct footprint_gid_entry {
+	union ibv_gid					gid;
+	uint32_t						gid_index;
+	uint32_t						gid_type;
+};
+
+struct ibv_resume_context_param {
+	char								cdev[32];
+	int									cmd_fd;
+	int									async_fd;
+	int									lkey_mmap_fd;
+//	void								*lkey_mmap_addr;
+	char								lkey_map[4096];
+//	int									local_rkey_mmap_fd;
+//	void								*local_rkey_mmap_addr;
+//	char								local_rkey_map[4096 * 256];
+//	int									lqpn_mmap_fd;
+//	void								*lqpn_mmap_addr;
+//	char								lqpn_map[4096 * 256];
+	int									rkey_mmap_fd;
+//	void								*rkey_mmap_addr;
+	char								rkey_map[4096];
+//	int									rqpn_mmap_fd;
+//	void								*rqpn_mmap_addr;
+//	char								rqpn_map[4096 * 256];
+	struct footprint_gid_entry			gid_table[256];
+	void								*ctx_uaddr;
+};
+
+struct ibv_context *ibv_resume_context(struct ibv_device **dev_list,
+		const struct ibv_resume_context_param *context_param);
+
+void ibv_free_tmp_context(struct ibv_context *context);
+
 /**
  * ibv_close_device - Release device
  */
@@ -2416,6 +2527,21 @@ int ibv_get_pkey_index(struct ibv_context *context, uint8_t port_num,
  */
 struct ibv_pd *ibv_alloc_pd(struct ibv_context *context);
 
+struct ibv_pd *ibv_resume_pd(struct ibv_context *context, int vhandle);
+
+struct ibv_resume_mr_param {
+	int							pd_vhandle;
+	int							mr_vhandle;
+	void						*iova;
+	size_t						length;
+	int							access_flags;
+	uint32_t					vlkey;
+	uint32_t					vrkey;
+};
+
+int ibv_resume_mr(struct ibv_context *context, struct ibv_pd *pd,
+			const struct ibv_resume_mr_param *mr_param);
+
 /**
  * ibv_dealloc_pd - Free a protection domain
  */
@@ -2642,6 +2768,7 @@ static inline int ibv_bind_mw(struct ibv_qp *qp, struct ibv_mw *mw,
  * ibv_create_comp_channel - Create a completion event channel
  */
 struct ibv_comp_channel *ibv_create_comp_channel(struct ibv_context *context);
+int ibv_resume_comp_channel(struct ibv_context *ctx, int comp_fd);
 
 /**
  * ibv_destroy_comp_channel - Destroy a completion event channel
@@ -2767,6 +2894,8 @@ struct ibv_mr *ibv_reg_dm_mr(struct ibv_pd *pd, struct ibv_dm *dm,
 	return vctx->reg_dm_mr(pd, dm, dm_offset, length, access);
 }
 
+void ibv_wait_local_wqes(void);
+
 /**
  * ibv_create_cq - Create a completion queue
  * @context - Context CQ will be attached to
@@ -2782,6 +2911,64 @@ struct ibv_cq *ibv_create_cq(struct ibv_context *context, int cqe,
 			     struct ibv_comp_channel *channel,
 			     int comp_vector);
 
+struct ibv_resume_cq_param {
+	int							cq_size;
+	int							comp_fd;
+	void						*meta_uaddr;
+	void						*buf_addr;
+	void						*db_addr;
+	int							cq_vhandle;
+	uint32_t					cons_index;
+	uint32_t					prod_index;
+};
+
+struct ibv_resume_srq_param {
+	struct ibv_srq_init_attr	init_attr;
+	void						*meta_uaddr;
+	void						*buf_addr;
+	void						*db_addr;
+	int							pd_vhandle;
+	int							vhandle;
+};
+
+struct ibv_cq *ibv_resume_cq(struct ibv_context *context, 
+				const struct ibv_resume_cq_param *cq_param);
+
+struct ibv_resume_qp_param {
+	int									pd_vhandle;
+	int									qp_vhandle;
+	int									send_cq_handle;
+	int									recv_cq_handle;
+	enum ibv_qp_state					qp_state;
+	struct ibv_qp_init_attr				init_attr;
+	struct ibv_qp_attr					modify_qp_attr[3];
+	int									modify_qp_mask[3];
+	void								*meta_uaddr;
+	uint32_t							vqpn;
+	void								*buf_addr;
+	void								*db_addr;
+	uint32_t							send_cur_post;
+	uint32_t							recv_head;
+	uint32_t							recv_tail;
+	int32_t								usr_idx;
+};
+
+struct ibv_qp *ibv_resume_create_qp(struct ibv_context *context,
+		struct ibv_pd *pd, struct ibv_cq *send_cq, struct ibv_cq *recv_cq, struct ibv_srq *srq,
+		const struct ibv_resume_qp_param *qp_param, unsigned long long *bf_reg);
+
+void ibv_resume_free_qp(struct ibv_qp *qp);
+
+int ibv_prepare_for_replay(int (*qp_load_cb)(struct ibv_qp *orig_qp, void *replay_fn),
+						int (*srq_load_cb)(struct ibv_srq *orig_srq, void *replay_fn, int head, int tail));
+int ibv_update_mem(int (*update_mem_fn)(void *ptr, size_t size,
+								void *content_p),
+					int (*keep_mmap_fn)(unsigned long long start,
+								unsigned long long end));
+
+int ibv_update_rkey_mapping(struct ibv_context *context, const union ibv_gid *vgid,
+					const union ibv_gid *real_gid, const uint32_t vrkey, const uint32_t rkey);
+
 /**
  * ibv_create_cq_ex - Create a completion queue
  * @context - Context CQ will be attached to
@@ -2897,6 +3084,9 @@ static inline int ibv_modify_cq(struct ibv_cq *cq, struct ibv_modify_cq_attr *at
 struct ibv_srq *ibv_create_srq(struct ibv_pd *pd,
 			       struct ibv_srq_init_attr *srq_init_attr);
 
+struct ibv_srq *ibv_resume_srq(struct ibv_pd *pd,
+				   struct ibv_resume_srq_param *param);
+
 static inline struct ibv_srq *
 ibv_create_srq_ex(struct ibv_context *context,
 		  struct ibv_srq_init_attr_ex *srq_init_attr_ex)
diff --git a/providers/mlx5/cq.c b/providers/mlx5/cq.c
index a68556e..9e8aa54 100644
--- a/providers/mlx5/cq.c
+++ b/providers/mlx5/cq.c
@@ -50,6 +50,7 @@ enum {
 	CQ_OK					=  0,
 	CQ_EMPTY				= -1,
 	CQ_POLL_ERR				= -2,
+	CQ_SHADED				= -3,
 	CQ_POLL_NODATA				= ENOENT
 };
 
@@ -150,9 +151,37 @@ static void *get_sw_cqe(struct mlx5_cq *cq, int n)
 	}
 }
 
-static void *next_cqe_sw(struct mlx5_cq *cq)
+static void *migrrdma_next_cqe_sw(struct mlx5_cq *cq, int *err) {
+	void *cqe = get_sw_cqe(cq, cq->fake_index);
+	*err = CQ_OK;
+	return cqe;
+}
+
+static void *next_cqe_sw(struct mlx5_cq *cq, int *err)
 {
-	return get_sw_cqe(cq, cq->cons_index);
+	void *tmp_cqe = cq->tmp_buf.buf +
+						(cq->tmp_cons_index & cq->verbs_cq.cq.cqe) *
+							cq->cqe_sz;
+	struct mlx5_cqe64 *tmp_cqe64;
+
+	tmp_cqe64 = (cq->cqe_sz == 64)? tmp_cqe: tmp_cqe + 64;
+
+	if(cq->shaded_flag &&
+			(likely(mlx5dv_get_cqe_opcode(tmp_cqe64) != MLX5_CQE_INVALID) &&
+				!((tmp_cqe64->op_own & MLX5_CQE_OWNER_MASK) ^
+				!!(cq->tmp_cons_index & (cq->verbs_cq.cq.cqe + 1))))) {
+		*err = CQ_SHADED;
+		cq->tmp_cons_index++;
+		return tmp_cqe;
+	}
+	else {
+		cq->shaded_flag = 0;
+		*err = CQ_OK;
+		tmp_cqe = get_sw_cqe(cq, cq->cons_index);
+		if(tmp_cqe)
+			cq->cons_index++;
+		return tmp_cqe;
+	}
 }
 
 static void update_cons_index(struct mlx5_cq *cq)
@@ -160,7 +189,8 @@ static void update_cons_index(struct mlx5_cq *cq)
 	cq->dbrec[MLX5_CQ_SET_CI] = htobe32(cq->cons_index & 0xffffff);
 }
 
-static inline void handle_good_req(struct ibv_wc *wc, struct mlx5_cqe64 *cqe, struct mlx5_wq *wq, int idx)
+static inline void handle_good_req(struct ibv_wc *wc, struct mlx5_cqe64 *cqe, struct mlx5_wq *wq, int idx,
+						struct mlx5_cq *cq, int is_shaded)
 {
 	switch (be32toh(cqe->sop_drop_qpn) >> 24) {
 	case MLX5_OPCODE_RDMA_WRITE_IMM:
@@ -191,7 +221,10 @@ static inline void handle_good_req(struct ibv_wc *wc, struct mlx5_cqe64 *cqe, st
 	case MLX5_OPCODE_UMR:
 	case MLX5_OPCODE_SET_PSV:
 	case MLX5_OPCODE_NOP:
-		wc->opcode = wq->wr_data[idx];
+		if(likely(!is_shaded && !cq->migr_flag))
+			wc->opcode = wq->wr_data[idx];
+		else
+			wc->opcode = wq->tmp_wr_data[idx];
 		break;
 	case MLX5_OPCODE_TSO:
 		wc->opcode    = IBV_WC_TSO;
@@ -199,8 +232,12 @@ static inline void handle_good_req(struct ibv_wc *wc, struct mlx5_cqe64 *cqe, st
 	}
 }
 
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <unistd.h>
+
 static inline int handle_responder_lazy(struct mlx5_cq *cq, struct mlx5_cqe64 *cqe,
-					struct mlx5_resource *cur_rsc, struct mlx5_srq *srq)
+					struct mlx5_resource *cur_rsc, struct mlx5_srq *srq, int is_shaded)
 {
 	uint16_t	wqe_ctr;
 	struct mlx5_wq *wq;
@@ -209,8 +246,25 @@ static inline int handle_responder_lazy(struct mlx5_cq *cq, struct mlx5_cqe64 *c
 
 	if (srq) {
 		wqe_ctr = be16toh(cqe->wqe_counter);
-		cq->verbs_cq.cq_ex.wr_id = srq->wrid[wqe_ctr];
-		mlx5_free_srq_wqe(srq, wqe_ctr);
+		if(likely(!is_shaded && !cq->migr_flag))
+			cq->verbs_cq.cq_ex.wr_id = srq->wrid[wqe_ctr];
+		else
+			cq->verbs_cq.cq_ex.wr_id = srq->tmp_wrid[wqe_ctr];
+		if(!is_shaded) {
+			if(!srq->migr_flag)
+				mlx5_free_srq_wqe(srq, wqe_ctr);
+			else {
+				mlx5_spin_lock(&srq->lock);
+				srq->staged_index[(srq->staged_head++) % srq->onflight_cap] = wqe_ctr;
+				mlx5_spin_unlock(&srq->lock);
+			}
+			{
+				struct srq_recv_wr_item *cur_recv_wr =
+							srq->onflight_recv_wr + wqe_ctr;
+				list_del(&cur_recv_wr->list);
+				srq->n_acked++;
+		}
+		}
 		if (cqe->op_own & MLX5_INLINE_SCATTER_32)
 			err = mlx5_copy_to_recv_srq(srq, wqe_ctr, cqe,
 						    be32toh(cqe->byte_cnt));
@@ -226,9 +280,26 @@ static inline int handle_responder_lazy(struct mlx5_cq *cq, struct mlx5_cqe64 *c
 			wq = &(rsc_to_mrwq(cur_rsc)->rq);
 		}
 
-		wqe_ctr = wq->tail & (wq->wqe_cnt - 1);
-		cq->verbs_cq.cq_ex.wr_id = wq->wrid[wqe_ctr];
-		++wq->tail;
+		wqe_ctr = be16toh(cqe->wqe_counter) & (wq->wqe_cnt - 1);
+		if(likely(!is_shaded && !cq->migr_flag))
+			cq->verbs_cq.cq_ex.wr_id = wq->wrid[wqe_ctr];
+		else
+			cq->verbs_cq.cq_ex.wr_id = wq->tmp_wrid[wqe_ctr];
+		if(!is_shaded) {
+			++wq->tail;
+			wq->n_acked++;
+			if(ibv_get_signal()) {
+				wq->commit_head = wq->head;
+				wq->commit_posted = wq->n_posted;
+			}
+			qp->onflight_tail++;
+#if 0
+			if(qp->switch_qp) {
+				mlx5_destroy_qp_tmp(&qp->switch_qp->verbs_qp.qp);
+				qp->switch_qp = NULL;
+			}
+#endif
+		}
 		if (cqe->op_own & MLX5_INLINE_SCATTER_32)
 			err = mlx5_copy_to_recv_wqe(qp, wqe_ctr, cqe,
 						    be32toh(cqe->byte_cnt));
@@ -249,8 +320,89 @@ static inline int get_csum_ok(struct mlx5_cqe64 *cqe)
 	       << IBV_WC_IP_CSUM_OK_SHIFT;
 }
 
-static inline int handle_responder(struct ibv_wc *wc, struct mlx5_cqe64 *cqe,
-				   struct mlx5_resource *cur_rsc, struct mlx5_srq *srq)
+static inline int migrrdma_handle_responder(struct mlx5_cq *cq, struct ibv_wc *wc, struct mlx5_cqe64 *cqe,
+				   struct mlx5_resource *cur_rsc, struct mlx5_srq *srq, int is_shaded) {
+	uint16_t	wqe_ctr;
+	struct mlx5_wq *wq;
+	struct mlx5_qp *qp = rsc_to_mqp(cur_rsc);
+	uint8_t g;
+	int err = 0;
+
+	wc->byte_len = be32toh(cqe->byte_cnt);
+	if (srq) {
+		if(srq->inspect_flag) {
+			struct srq_recv_wr_item *cur_item;
+		wqe_ctr = be16toh(cqe->wqe_counter);
+			cur_item = srq->migrrdma_onflight + wqe_ctr;
+			list_del(&cur_item->list);
+			srq->migrrdma_acked++;
+		}
+	} else {
+		if (likely(cur_rsc->type == MLX5_RSC_TYPE_QP)) {
+			wq = &qp->rq;
+			if (qp->qp_cap_cache & MLX5_RX_CSUM_VALID)
+				wc->wc_flags |= get_csum_ok(cqe);
+		} else {
+			wq = &(rsc_to_mrwq(cur_rsc)->rq);
+		}
+
+		wqe_ctr = qp->migrrdma_rq.tail & (wq->wqe_cnt - 1);
+		if(likely(!is_shaded && !cq->migr_flag))
+			wc->wr_id = wq->wrid[wqe_ctr];
+		else
+			wc->wr_id = wq->tmp_wrid[wqe_ctr];
+
+		if(!is_shaded) {
+			++qp->migrrdma_rq.tail;
+			qp->migrrdma_rq.n_acked++;
+//			qp->onflight_tail++;
+		}
+
+#if 0
+		if (cqe->op_own & MLX5_INLINE_SCATTER_32)
+			err = mlx5_copy_to_recv_wqe(qp, wqe_ctr, cqe,
+						    wc->byte_len);
+		else if (cqe->op_own & MLX5_INLINE_SCATTER_64)
+			err = mlx5_copy_to_recv_wqe(qp, wqe_ctr, cqe - 1,
+						    wc->byte_len);
+#endif
+	}
+	if (err)
+		return err;
+
+	switch (cqe->op_own >> 4) {
+	case MLX5_CQE_RESP_WR_IMM:
+		wc->opcode	= IBV_WC_RECV_RDMA_WITH_IMM;
+		wc->wc_flags	|= IBV_WC_WITH_IMM;
+		wc->imm_data = cqe->imm_inval_pkey;
+		break;
+	case MLX5_CQE_RESP_SEND:
+		wc->opcode   = IBV_WC_RECV;
+		break;
+	case MLX5_CQE_RESP_SEND_IMM:
+		wc->opcode	= IBV_WC_RECV;
+		wc->wc_flags	|= IBV_WC_WITH_IMM;
+		wc->imm_data = cqe->imm_inval_pkey;
+		break;
+	case MLX5_CQE_RESP_SEND_INV:
+		wc->opcode = IBV_WC_RECV;
+		wc->wc_flags |= IBV_WC_WITH_INV;
+		wc->invalidated_rkey = be32toh(cqe->imm_inval_pkey);
+		break;
+	}
+	wc->slid	   = be16toh(cqe->slid);
+	wc->sl		   = (be32toh(cqe->flags_rqpn) >> 24) & 0xf;
+	wc->src_qp	   = be32toh(cqe->flags_rqpn) & 0xffffff;
+	wc->dlid_path_bits = cqe->ml_path & 0x7f;
+	g = (be32toh(cqe->flags_rqpn) >> 28) & 3;
+	wc->wc_flags |= g ? IBV_WC_GRH : 0;
+	wc->pkey_index     = be32toh(cqe->imm_inval_pkey) & 0xffff;
+
+	return IBV_WC_SUCCESS;
+}
+
+static inline int handle_responder(struct mlx5_cq *cq, struct ibv_wc *wc, struct mlx5_cqe64 *cqe,
+				   struct mlx5_resource *cur_rsc, struct mlx5_srq *srq, int is_shaded)
 {
 	uint16_t	wqe_ctr;
 	struct mlx5_wq *wq;
@@ -261,8 +413,25 @@ static inline int handle_responder(struct ibv_wc *wc, struct mlx5_cqe64 *cqe,
 	wc->byte_len = be32toh(cqe->byte_cnt);
 	if (srq) {
 		wqe_ctr = be16toh(cqe->wqe_counter);
-		wc->wr_id = srq->wrid[wqe_ctr];
-		mlx5_free_srq_wqe(srq, wqe_ctr);
+		if(likely(!is_shaded && !cq->migr_flag))
+			wc->wr_id = srq->wrid[wqe_ctr];
+		else
+			wc->wr_id = srq->tmp_wrid[wqe_ctr];
+		if(!is_shaded) {
+			if(!srq->migr_flag)
+				mlx5_free_srq_wqe(srq, wqe_ctr);
+			else {
+				mlx5_spin_lock(&srq->lock);
+				srq->staged_index[(srq->staged_head++) % srq->onflight_cap] = wqe_ctr;
+				mlx5_spin_unlock(&srq->lock);
+			}
+			{
+				struct srq_recv_wr_item *cur_recv_wr =
+							srq->onflight_recv_wr + wqe_ctr;
+				list_del(&cur_recv_wr->list);
+			srq->n_acked++;
+		}
+		}
 		if (cqe->op_own & MLX5_INLINE_SCATTER_32)
 			err = mlx5_copy_to_recv_srq(srq, wqe_ctr, cqe,
 						    wc->byte_len);
@@ -278,9 +447,26 @@ static inline int handle_responder(struct ibv_wc *wc, struct mlx5_cqe64 *cqe,
 			wq = &(rsc_to_mrwq(cur_rsc)->rq);
 		}
 
-		wqe_ctr = wq->tail & (wq->wqe_cnt - 1);
-		wc->wr_id = wq->wrid[wqe_ctr];
-		++wq->tail;
+		wqe_ctr = be16toh(cqe->wqe_counter) & (wq->wqe_cnt - 1);
+		if(likely(!is_shaded && !cq->migr_flag))
+			wc->wr_id = wq->wrid[wqe_ctr];
+		else
+			wc->wr_id = wq->tmp_wrid[wqe_ctr];
+		if(!is_shaded) {
+			++wq->tail;
+			wq->n_acked++;
+			if(ibv_get_signal()) {
+				wq->commit_head = wq->head;
+				wq->commit_posted = wq->n_posted;
+			}
+			qp->onflight_tail++;
+#if 0
+			if(qp->switch_qp) {
+				mlx5_destroy_qp_tmp(&qp->switch_qp->verbs_qp.qp);
+				qp->switch_qp = NULL;
+			}
+#endif
+		}
 		if (cqe->op_own & MLX5_INLINE_SCATTER_32)
 			err = mlx5_copy_to_recv_wqe(qp, wqe_ctr, cqe,
 						    wc->byte_len);
@@ -524,6 +710,38 @@ static inline int get_cur_rsc(struct mlx5_context *mctx,
 
 }
 
+static inline int migrrdma_get_next_cqe(struct mlx5_cq *cq,
+				    struct mlx5_cqe64 **pcqe64,
+				    void **pcqe) {
+	void *cqe;
+	struct mlx5_cqe64 *cqe64;
+	int err = CQ_OK;
+
+	cqe = migrrdma_next_cqe_sw(cq, &err);
+	if(!cqe) {
+		return CQ_EMPTY;
+	}
+
+	cqe64 = (cq->cqe_sz == 64) ? cqe : cqe + 64;
+
+	pthread_rwlock_wrlock(&cq->fake_index_rwlock);
+	++cq->fake_index;
+	pthread_rwlock_unlock(&cq->fake_index_rwlock);
+
+	VALGRIND_MAKE_MEM_DEFINED(cqe64, sizeof *cqe64);
+
+	/*
+	 * Make sure we read CQ entry contents after we've checked the
+	 * ownership bit.
+	 */
+	udma_from_device_barrier();
+
+	*pcqe64 = cqe64;
+	*pcqe = cqe;
+
+	return err;
+}
+
 static inline int mlx5_get_next_cqe(struct mlx5_cq *cq,
 				    struct mlx5_cqe64 **pcqe64,
 				    void **pcqe)
@@ -534,14 +752,15 @@ static inline int mlx5_get_next_cqe(struct mlx5_cq *cq,
 {
 	void *cqe;
 	struct mlx5_cqe64 *cqe64;
+	int err = CQ_OK;
 
-	cqe = next_cqe_sw(cq);
+	cqe = next_cqe_sw(cq, &err);
 	if (!cqe)
 		return CQ_EMPTY;
 
 	cqe64 = (cq->cqe_sz == 64) ? cqe : cqe + 64;
 
-	++cq->cons_index;
+//	++cq->cons_index;
 
 	VALGRIND_MAKE_MEM_DEFINED(cqe64, sizeof *cqe64);
 
@@ -565,7 +784,7 @@ static inline int mlx5_get_next_cqe(struct mlx5_cq *cq,
 	*pcqe64 = cqe64;
 	*pcqe = cqe;
 
-	return CQ_OK;
+	return err;
 }
 
 static int handle_tag_matching(struct mlx5_cq *cq,
@@ -696,13 +915,368 @@ static inline int is_odp_pfault_err(struct mlx5_err_cqe *ecqe)
 	       ecqe->vendor_err_synd == MLX5_CQE_VENDOR_SYNDROME_ODP_PFAULT;
 }
 
+static inline int migrrdma_parse_cqe(struct mlx5_cq *cq,
+				 struct mlx5_cqe64 *cqe64,
+				 void *cqe,
+				 struct mlx5_resource **cur_rsc,
+				 struct mlx5_srq **cur_srq,
+				 struct ibv_wc *wc, struct ibv_qp **qps, struct ibv_srq **srqs,
+				 int cqe_ver, int lazy, int is_shaded) {
+	struct mlx5_wq *wq;
+	uint16_t wqe_ctr;
+	uint32_t qpn;
+	uint32_t srqn_uidx;
+	int idx;
+	uint8_t opcode;
+	struct mlx5_err_cqe *ecqe;
+	struct mlx5_sigerr_cqe *sigerr_cqe;
+	struct mlx5_mkey *mkey;
+	int err;
+	struct mlx5_qp *mqp;
+	struct mlx5_context *mctx;
+	uint8_t is_srq;
+
+again:
+	is_srq = 0;
+	err = 0;
+
+	mctx = to_mctx(cq->verbs_cq.cq.context);
+	qpn = be32toh(cqe64->sop_drop_qpn) & 0xffffff;
+	if (lazy) {
+		cq->cqe64 = cqe64;
+		cq->flags &= (~MLX5_CQ_LAZY_FLAGS);
+	} else {
+		wc->wc_flags = 0;
+		if(!is_shaded && !cq->migr_flag) {
+			uint32_t *qpn_dict = cq->verbs_cq.cq.context->device->qpn_dict;
+			wc->qp_num = qpn_dict[qpn];
+		}
+		else {
+			int err;
+			uint32_t vqpn;
+			err = get_vqpn_from_old(qpn, &vqpn);
+			if(err) {
+				return CQ_POLL_ERR;
+			}
+			wc->qp_num = vqpn;
+		}
+	}
+
+	opcode = mlx5dv_get_cqe_opcode(cqe64);
+	switch (opcode) {
+	case MLX5_CQE_REQ:
+	{
+		mqp = get_req_context(mctx, cur_rsc,
+				      (cqe_ver ? (be32toh(cqe64->srqn_uidx) & 0xffffff) : qpn),
+				      cqe_ver);
+		if (unlikely(!mqp))
+			return CQ_POLL_ERR;
+		if(qps)
+			*qps = &mqp->verbs_qp.qp;
+		if(srqs)
+			*srqs = NULL;
+		wq = &mqp->sq;
+		wqe_ctr = be16toh(cqe64->wqe_counter);
+		idx = wqe_ctr & (wq->wqe_cnt - 1);
+		if (lazy) {
+			uint32_t wc_byte_len;
+
+			switch (be32toh(cqe64->sop_drop_qpn) >> 24) {
+			case MLX5_OPCODE_UMR:
+			case MLX5_OPCODE_SET_PSV:
+			case MLX5_OPCODE_NOP:
+				if(likely(!is_shaded && !cq->migr_flag))
+					cq->cached_opcode = wq->wr_data[idx];
+				else
+					cq->cached_opcode = wq->tmp_wr_data[idx];
+				break;
+
+			case MLX5_OPCODE_RDMA_READ:
+				wc_byte_len = be32toh(cqe64->byte_cnt);
+				goto scatter_out;
+			case MLX5_OPCODE_ATOMIC_CS:
+			case MLX5_OPCODE_ATOMIC_FA:
+				wc_byte_len = 8;
+
+			scatter_out:
+#if 0
+				if (cqe64->op_own & MLX5_INLINE_SCATTER_32)
+					err = mlx5_copy_to_send_wqe(
+					    mqp, wqe_ctr, cqe, wc_byte_len);
+				else if (cqe64->op_own & MLX5_INLINE_SCATTER_64)
+					err = mlx5_copy_to_send_wqe(
+					    mqp, wqe_ctr, cqe - 1, wc_byte_len);
+#endif
+				break;
+			}
+
+			if(likely(!is_shaded && !cq->migr_flag))
+				cq->verbs_cq.cq_ex.wr_id = wq->wrid[idx];
+			else
+				cq->verbs_cq.cq_ex.wr_id = wq->tmp_wrid[idx];
+			cq->verbs_cq.cq_ex.status = err;
+		} else {
+			handle_good_req(wc, cqe64, wq, idx, cq, is_shaded);
+
+#if 0
+			if (cqe64->op_own & MLX5_INLINE_SCATTER_32)
+				err = mlx5_copy_to_send_wqe(mqp, wqe_ctr, cqe,
+							    wc->byte_len);
+			else if (cqe64->op_own & MLX5_INLINE_SCATTER_64)
+				err = mlx5_copy_to_send_wqe(
+				    mqp, wqe_ctr, cqe - 1, wc->byte_len);
+#endif
+
+			if(likely(!is_shaded && !cq->migr_flag))
+				wc->wr_id = wq->wrid[idx];
+			else
+				wc->wr_id = wq->tmp_wrid[idx];
+			wc->status = err;
+		}
+
+		if(!is_shaded &&
+					mqp->rsc.rsn == (be32toh(cqe64->srqn_uidx) & 0xffffff)) {
+#if 0
+			if(mqp->switch_qp) {
+				mlx5_destroy_qp_tmp(&mqp->switch_qp->verbs_qp.qp);
+				mqp->switch_qp = NULL;
+			}
+#endif
+			mqp->migrrdma_sq.n_acked += (wq->wqe_head[idx] + 1 - mqp->migrrdma_sq.tail);
+			mqp->migrrdma_sq.tail = wq->wqe_head[idx] + 1;
+		}
+		break;
+	}
+	case MLX5_CQE_RESP_WR_IMM:
+	case MLX5_CQE_RESP_SEND:
+	case MLX5_CQE_RESP_SEND_IMM:
+	case MLX5_CQE_RESP_SEND_INV:
+		srqn_uidx = be32toh(cqe64->srqn_uidx) & 0xffffff;
+		err = get_cur_rsc(mctx, cqe_ver, qpn, srqn_uidx, cur_rsc,
+				  cur_srq, &is_srq);
+		if (unlikely(err))
+			return CQ_POLL_ERR;
+
+		if(is_srq) {
+			struct mlx5_srq *srq = (struct mlx5_srq *)(*cur_srq);
+			if(srqs)
+				*srqs = &srq->vsrq.srq;
+			if(qps)
+				*qps = NULL;
+		}
+		else {
+			struct mlx5_qp *qp = rsc_to_mqp(*cur_rsc);
+			if(qps)
+				*qps = &qp->verbs_qp.qp;
+			if(srqs)
+				*srqs = NULL;
+		}
+
+		if (lazy) {
+			if (likely(cqe64->app != MLX5_CQE_APP_TAG_MATCHING)) {
+				cq->verbs_cq.cq_ex.status = handle_responder_lazy
+					(cq, cqe64, *cur_rsc,
+					 is_srq ? *cur_srq : NULL, is_shaded);
+			} else {
+				if (unlikely(!is_srq))
+					return CQ_POLL_ERR;
+
+				err = handle_tag_matching(cq, cqe64, *cur_srq);
+				if (unlikely(err))
+					return CQ_POLL_ERR;
+			}
+		} else {
+			wc->status = migrrdma_handle_responder(cq, wc, cqe64, *cur_rsc,
+					      is_srq ? *cur_srq : NULL, is_shaded);
+		}
+		break;
+
+	case MLX5_CQE_NO_PACKET:
+		if (unlikely(cqe64->app != MLX5_CQE_APP_TAG_MATCHING))
+			return CQ_POLL_ERR;
+		srqn_uidx = be32toh(cqe64->srqn_uidx) & 0xffffff;
+		err = get_cur_rsc(mctx, cqe_ver, qpn, srqn_uidx, cur_rsc,
+				  cur_srq, &is_srq);
+		if (unlikely(err || !is_srq))
+			return CQ_POLL_ERR;
+
+		if(is_srq) {
+			struct mlx5_srq *srq = (struct mlx5_srq *)(*cur_srq);
+			if(srqs)
+				*srqs = &srq->vsrq.srq;
+			if(qps)
+				*qps = NULL;
+		}
+		else {
+			struct mlx5_qp *qp = rsc_to_mqp(*cur_rsc);
+			if(qps)
+				*qps = &qp->verbs_qp.qp;
+			if(srqs)
+				*srqs = NULL;
+		}
+
+		err = handle_tag_matching(cq, cqe64, *cur_srq);
+		if (unlikely(err))
+			return CQ_POLL_ERR;
+		break;
+
+	case MLX5_CQE_SIG_ERR:
+		sigerr_cqe = (struct mlx5_sigerr_cqe *)cqe64;
+
+		pthread_mutex_lock(&mctx->mkey_table_mutex);
+		mkey = mlx5_find_mkey(mctx, be32toh(sigerr_cqe->mkey) >> 8);
+		if (!mkey) {
+			pthread_mutex_unlock(&mctx->mkey_table_mutex);
+			return CQ_POLL_ERR;
+		}
+
+		mkey->sig->err_exists = true;
+		mkey->sig->err_count++;
+		get_sig_err_info(sigerr_cqe, &mkey->sig->err_info);
+		pthread_mutex_unlock(&mctx->mkey_table_mutex);
+
+		err = migrrdma_get_next_cqe(cq, &cqe64, &cqe);
+		/*
+		 * CQ_POLL_NODATA indicates that CQ was not empty but the polled
+		 * CQE was handled internally and should not processed by the
+		 * caller.
+		 */
+		if (err == CQ_EMPTY)
+			return CQ_POLL_NODATA;
+		goto again;
+
+	case MLX5_CQE_RESIZE_CQ:
+		break;
+	case MLX5_CQE_REQ_ERR:
+	case MLX5_CQE_RESP_ERR:
+		srqn_uidx = be32toh(cqe64->srqn_uidx) & 0xffffff;
+		ecqe = (struct mlx5_err_cqe *)cqe64;
+		{
+			enum ibv_wc_status *pstatus = lazy ? &cq->verbs_cq.cq_ex.status : &wc->status;
+
+			*pstatus = mlx5_handle_error_cqe(ecqe);
+		}
+
+		if (!lazy)
+			wc->vendor_err = ecqe->vendor_err_synd;
+
+		if (unlikely(ecqe->syndrome != MLX5_CQE_SYNDROME_WR_FLUSH_ERR &&
+			     ecqe->syndrome != MLX5_CQE_SYNDROME_TRANSPORT_RETRY_EXC_ERR &&
+			     !is_odp_pfault_err(ecqe))) {
+			mlx5_err(mctx->dbg_fp, PFX "%s: got completion with error:\n",
+				mctx->hostname);
+			dump_cqe(mctx, ecqe);
+			if (mlx5_freeze_on_error_cqe) {
+				mlx5_err(mctx->dbg_fp, PFX "freezing at poll cq...");
+				while (1)
+					sleep(10);
+			}
+		}
+
+		if (opcode == MLX5_CQE_REQ_ERR) {
+			mqp = get_req_context(mctx, cur_rsc,
+					      (cqe_ver ? srqn_uidx : qpn), cqe_ver);
+			if (unlikely(!mqp))
+				return CQ_POLL_ERR;
+			if(qps)
+				*qps = &mqp->verbs_qp.qp;
+			if(srqs)
+				*srqs = NULL;
+			wq = &mqp->sq;
+			wqe_ctr = be16toh(cqe64->wqe_counter);
+			idx = wqe_ctr & (wq->wqe_cnt - 1);
+			if (lazy) {
+				if(likely(!is_shaded && !cq->migr_flag))
+					cq->verbs_cq.cq_ex.wr_id = wq->wrid[idx];
+				else
+					cq->verbs_cq.cq_ex.wr_id = wq->tmp_wrid[idx];
+			}
+			else {
+				if(likely(!is_shaded && !cq->migr_flag))
+					wc->wr_id = wq->wrid[idx];
+				else
+					wc->wr_id = wq->tmp_wrid[idx];
+			}
+
+			if(!is_shaded && mqp->rsc.rsn == srqn_uidx) {
+				mqp->migrrdma_sq.n_acked += (wq->wqe_head[idx] + 1 - mqp->migrrdma_sq.tail);
+				mqp->migrrdma_sq.tail = wq->wqe_head[idx] + 1;
+			}
+		} else {
+			err = get_cur_rsc(mctx, cqe_ver, qpn, srqn_uidx,
+					  cur_rsc, cur_srq, &is_srq);
+			if (unlikely(err))
+				return CQ_POLL_ERR;
+
+			if(is_srq) {
+				struct mlx5_srq *srq = (struct mlx5_srq *)(*cur_srq);
+				if(srqs)
+					*srqs = &srq->vsrq.srq;
+				if(qps)
+					*qps = NULL;
+			}
+			else {
+				struct mlx5_qp *qp = rsc_to_mqp(*cur_rsc);
+				if(qps)
+					*qps = &qp->verbs_qp.qp;
+				if(srqs)
+					*srqs = NULL;
+			}
+
+			if (is_srq) {
+				wqe_ctr = be16toh(cqe64->wqe_counter);
+				if (is_odp_pfault_err(ecqe)) {
+					mlx5_complete_odp_fault(*cur_srq, wqe_ctr);
+					err = migrrdma_get_next_cqe(cq, &cqe64, &cqe);
+					/* CQ_POLL_NODATA indicates that CQ was not empty but the polled CQE
+					 * was handled internally and should not processed by the caller.
+					 */
+					if (err == CQ_EMPTY)
+						return CQ_POLL_NODATA;
+					goto again;
+				}
+
+#if 0
+				if (lazy)
+					cq->verbs_cq.cq_ex.wr_id = (*cur_srq)->wrid[wqe_ctr];
+				else
+					wc->wr_id = (*cur_srq)->wrid[wqe_ctr];
+				mlx5_free_srq_wqe(*cur_srq, wqe_ctr);
+#endif
+			} else {
+				switch ((*cur_rsc)->type) {
+				case MLX5_RSC_TYPE_RWQ:
+					wq = &(rsc_to_mrwq(*cur_rsc)->rq);
+					break;
+				default:
+					wq = &(rsc_to_mqp(*cur_rsc)->rq);
+					break;
+				}
+
+				if (lazy)
+					cq->verbs_cq.cq_ex.wr_id = wq->wrid[rsc_to_mqp(*cur_rsc)->migrrdma_rq.tail & (wq->wqe_cnt - 1)];
+				else
+					wc->wr_id = wq->wrid[rsc_to_mqp(*cur_rsc)->migrrdma_rq.tail & (wq->wqe_cnt - 1)];
+
+				if(!is_shaded) {
+					++rsc_to_mqp(*cur_rsc)->migrrdma_rq.tail;
+					rsc_to_mqp(*cur_rsc)->migrrdma_rq.n_acked++;
+				}
+			}
+		}
+		break;
+	}
+
+	return CQ_OK;
+}
+
 static inline int mlx5_parse_cqe(struct mlx5_cq *cq,
 				 struct mlx5_cqe64 *cqe64,
 				 void *cqe,
 				 struct mlx5_resource **cur_rsc,
 				 struct mlx5_srq **cur_srq,
 				 struct ibv_wc *wc,
-				 int cqe_ver, int lazy)
+				 int cqe_ver, int lazy, int is_shaded)
 				 ALWAYS_INLINE;
 static inline int mlx5_parse_cqe(struct mlx5_cq *cq,
 				 struct mlx5_cqe64 *cqe64,
@@ -710,7 +1284,7 @@ static inline int mlx5_parse_cqe(struct mlx5_cq *cq,
 				 struct mlx5_resource **cur_rsc,
 				 struct mlx5_srq **cur_srq,
 				 struct ibv_wc *wc,
-				 int cqe_ver, int lazy)
+				 int cqe_ver, int lazy, int is_shaded)
 {
 	struct mlx5_wq *wq;
 	uint16_t wqe_ctr;
@@ -737,7 +1311,19 @@ again:
 		cq->flags &= (~MLX5_CQ_LAZY_FLAGS);
 	} else {
 		wc->wc_flags = 0;
-		wc->qp_num = qpn;
+		if(!is_shaded && !cq->migr_flag) {
+			uint32_t *qpn_dict = cq->verbs_cq.cq.context->device->qpn_dict;
+			wc->qp_num = qpn_dict[qpn];
+		}
+		else {
+			int err;
+			uint32_t vqpn;
+			err = get_vqpn_from_old(qpn, &vqpn);
+			if(err) {
+				return CQ_POLL_ERR;
+			}
+			wc->qp_num = vqpn;
+		}
 	}
 
 	opcode = mlx5dv_get_cqe_opcode(cqe64);
@@ -759,7 +1345,10 @@ again:
 			case MLX5_OPCODE_UMR:
 			case MLX5_OPCODE_SET_PSV:
 			case MLX5_OPCODE_NOP:
-				cq->cached_opcode = wq->wr_data[idx];
+				if(likely(!is_shaded && !cq->migr_flag))
+					cq->cached_opcode = wq->wr_data[idx];
+				else
+					cq->cached_opcode = wq->tmp_wr_data[idx];
 				break;
 
 			case MLX5_OPCODE_RDMA_READ:
@@ -779,10 +1368,13 @@ again:
 				break;
 			}
 
-			cq->verbs_cq.cq_ex.wr_id = wq->wrid[idx];
+			if(likely(!is_shaded && !cq->migr_flag))
+				cq->verbs_cq.cq_ex.wr_id = wq->wrid[idx];
+			else
+				cq->verbs_cq.cq_ex.wr_id = wq->tmp_wrid[idx];
 			cq->verbs_cq.cq_ex.status = err;
 		} else {
-			handle_good_req(wc, cqe64, wq, idx);
+			handle_good_req(wc, cqe64, wq, idx, cq, is_shaded);
 
 			if (cqe64->op_own & MLX5_INLINE_SCATTER_32)
 				err = mlx5_copy_to_send_wqe(mqp, wqe_ctr, cqe,
@@ -791,11 +1383,38 @@ again:
 				err = mlx5_copy_to_send_wqe(
 				    mqp, wqe_ctr, cqe - 1, wc->byte_len);
 
-			wc->wr_id = wq->wrid[idx];
+			if(likely(!is_shaded && !cq->migr_flag))
+				wc->wr_id = wq->wrid[idx];
+			else
+				wc->wr_id = wq->tmp_wrid[idx];
 			wc->status = err;
 		}
 
-		wq->tail = wq->wqe_head[idx] + 1;
+		if(!is_shaded &&
+					mqp->rsc.rsn == (be32toh(cqe64->srqn_uidx) & 0xffffff)) {
+#if 0
+			if(mqp->switch_qp) {
+				mlx5_destroy_qp_tmp(&mqp->switch_qp->verbs_qp.qp);
+				mqp->switch_qp = NULL;
+			}
+#endif
+			wq->n_acked += (wq->wqe_head[idx] + 1 - wq->tail);
+			wq->tail = wq->wqe_head[idx] + 1;
+			if(ibv_get_signal()) {
+				wq->commit_head = wq->head;
+				wq->commit_posted = wq->n_posted;
+			}
+		}
+		else if(!is_shaded) {
+			wq->tail += (mqp->tmp_sq.wqe_head[idx] + 1 - mqp->tmp_sq.tail);
+			if(ibv_get_signal()) {
+				wq->commit_head = wq->head;
+				wq->commit_posted = wq->n_posted;
+			}
+			mqp->tmp_sq.tail = mqp->tmp_sq.wqe_head[idx] + 1;
+			wc->wr_id = mqp->tmp_sq.wrid[idx];
+		}
+
 		break;
 	}
 	case MLX5_CQE_RESP_WR_IMM:
@@ -812,7 +1431,7 @@ again:
 			if (likely(cqe64->app != MLX5_CQE_APP_TAG_MATCHING)) {
 				cq->verbs_cq.cq_ex.status = handle_responder_lazy
 					(cq, cqe64, *cur_rsc,
-					 is_srq ? *cur_srq : NULL);
+					 is_srq ? *cur_srq : NULL, is_shaded);
 			} else {
 				if (unlikely(!is_srq))
 					return CQ_POLL_ERR;
@@ -822,8 +1441,8 @@ again:
 					return CQ_POLL_ERR;
 			}
 		} else {
-			wc->status = handle_responder(wc, cqe64, *cur_rsc,
-					      is_srq ? *cur_srq : NULL);
+			wc->status = handle_responder(cq, wc, cqe64, *cur_rsc,
+					      is_srq ? *cur_srq : NULL, is_shaded);
 		}
 		break;
 
@@ -901,11 +1520,43 @@ again:
 			wq = &mqp->sq;
 			wqe_ctr = be16toh(cqe64->wqe_counter);
 			idx = wqe_ctr & (wq->wqe_cnt - 1);
-			if (lazy)
-				cq->verbs_cq.cq_ex.wr_id = wq->wrid[idx];
-			else
-				wc->wr_id = wq->wrid[idx];
-			wq->tail = wq->wqe_head[idx] + 1;
+			if (lazy) {
+				if(likely(!is_shaded && !cq->migr_flag))
+					cq->verbs_cq.cq_ex.wr_id = wq->wrid[idx];
+				else
+					cq->verbs_cq.cq_ex.wr_id = wq->tmp_wrid[idx];
+			}
+			else {
+				if(likely(!is_shaded && !cq->migr_flag))
+					wc->wr_id = wq->wrid[idx];
+				else
+					wc->wr_id = wq->tmp_wrid[idx];
+			}
+
+			if(!is_shaded &&
+						mqp->rsc.rsn == (be32toh(cqe64->srqn_uidx) & 0xffffff)) {
+#if 0
+				if(mqp->switch_qp) {
+					mlx5_destroy_qp_tmp(&mqp->switch_qp->verbs_qp.qp);
+					mqp->switch_qp = NULL;
+				}
+#endif
+				wq->n_acked += (wq->wqe_head[idx] + 1 - wq->tail);
+				wq->tail = wq->wqe_head[idx] + 1;
+				if(ibv_get_signal()) {
+					wq->commit_head = wq->head;
+					wq->commit_posted = wq->n_posted;
+				}
+			}
+			else if(!is_shaded) {
+				wq->tail += (mqp->tmp_sq.wqe_head[idx] + 1 - mqp->tmp_sq.tail);
+				if(ibv_get_signal()) {
+					wq->commit_head = wq->head;
+					wq->commit_posted = wq->n_posted;
+				}
+				mqp->tmp_sq.tail = mqp->tmp_sq.wqe_head[idx] + 1;
+				wc->wr_id = mqp->tmp_sq.wrid[idx];
+			}
 		} else {
 			err = get_cur_rsc(mctx, cqe_ver, qpn, srqn_uidx,
 					  cur_rsc, cur_srq, &is_srq);
@@ -926,10 +1577,30 @@ again:
 				}
 
 				if (lazy)
-					cq->verbs_cq.cq_ex.wr_id = (*cur_srq)->wrid[wqe_ctr];
+					if(likely(!is_shaded && !cq->migr_flag))
+						cq->verbs_cq.cq_ex.wr_id = (*cur_srq)->wrid[wqe_ctr];
+					else
+						cq->verbs_cq.cq_ex.wr_id = (*cur_srq)->tmp_wrid[wqe_ctr];
 				else
-					wc->wr_id = (*cur_srq)->wrid[wqe_ctr];
-				mlx5_free_srq_wqe(*cur_srq, wqe_ctr);
+					if(likely(!is_shaded && !cq->migr_flag))
+						wc->wr_id = (*cur_srq)->wrid[wqe_ctr];
+					else
+						wc->wr_id = (*cur_srq)->tmp_wrid[wqe_ctr];
+				if(!is_shaded) {
+					if(!(*cur_srq)->migr_flag)
+						mlx5_free_srq_wqe(*cur_srq, wqe_ctr);
+					else {
+						mlx5_spin_lock(&(*cur_srq)->lock);
+						(*cur_srq)->staged_index[((*cur_srq)->staged_head++) % (*cur_srq)->onflight_cap] = wqe_ctr;
+						mlx5_spin_unlock(&(*cur_srq)->lock);
+					}
+					{
+						struct srq_recv_wr_item *cur_recv_wr =
+									(*cur_srq)->onflight_recv_wr + wqe_ctr;
+						list_del(&cur_recv_wr->list);
+						(*cur_srq)->n_acked++;
+					}
+				}
 			} else {
 				switch ((*cur_rsc)->type) {
 				case MLX5_RSC_TYPE_RWQ:
@@ -941,10 +1612,26 @@ again:
 				}
 
 				if (lazy)
-					cq->verbs_cq.cq_ex.wr_id = wq->wrid[wq->tail & (wq->wqe_cnt - 1)];
+					cq->verbs_cq.cq_ex.wr_id = wq->wrid[be16toh(cqe64->wqe_counter) & (wq->wqe_cnt - 1)];
 				else
-					wc->wr_id = wq->wrid[wq->tail & (wq->wqe_cnt - 1)];
-				++wq->tail;
+					wc->wr_id = wq->wrid[be16toh(cqe64->wqe_counter) & (wq->wqe_cnt - 1)];
+				if(!is_shaded) {
+					++wq->tail;
+					wq->n_acked++;
+					if(ibv_get_signal()) {
+						wq->commit_head = wq->head;
+						wq->commit_posted = wq->n_posted;
+					}
+					if((*cur_rsc)->type != MLX5_RSC_TYPE_RWQ) {
+						(rsc_to_mqp(*cur_rsc))->onflight_tail++;
+#if 0
+						if((rsc_to_mqp(*cur_rsc))->switch_qp) {
+							mlx5_destroy_qp_tmp(&(rsc_to_mqp(*cur_rsc))->switch_qp->verbs_qp.qp);
+							(rsc_to_mqp(*cur_rsc))->switch_qp = NULL;
+						}
+#endif
+					}
+				}
 			}
 		}
 		break;
@@ -961,7 +1648,22 @@ static inline int mlx5_parse_lazy_cqe(struct mlx5_cq *cq,
 				      struct mlx5_cqe64 *cqe64,
 				      void *cqe, int cqe_ver)
 {
-	return mlx5_parse_cqe(cq, cqe64, cqe, &cq->cur_rsc, &cq->cur_srq, NULL, cqe_ver, 1);
+	return mlx5_parse_cqe(cq, cqe64, cqe, &cq->cur_rsc, &cq->cur_srq, NULL, cqe_ver, 1, 0);
+}
+
+static inline int migrrdma_poll_one(struct mlx5_cq *cq,
+				struct mlx5_resource **cur_rsc,
+				struct mlx5_srq **cur_srq,
+				struct ibv_wc *wc, struct ibv_qp **qps, struct ibv_srq **srqs, int cqe_ver) {
+	struct mlx5_cqe64 *cqe64;
+	void *cqe;
+	int err;
+
+	err = migrrdma_get_next_cqe(cq, &cqe64, &cqe);
+	if(err == CQ_EMPTY)
+		return err;
+
+	return migrrdma_parse_cqe(cq, cqe64, cqe, cur_rsc, cur_srq, wc, qps, srqs, cqe_ver, 0, 0);
 }
 
 static inline int mlx5_poll_one(struct mlx5_cq *cq,
@@ -977,17 +1679,54 @@ static inline int mlx5_poll_one(struct mlx5_cq *cq,
 	struct mlx5_cqe64 *cqe64;
 	void *cqe;
 	int err;
+	int is_shaded = 0;
 
 	err = mlx5_get_next_cqe(cq, &cqe64, &cqe);
+	if(err == CQ_SHADED) {
+		err = CQ_OK;
+		is_shaded = 1;
+	}
 	if (err == CQ_EMPTY)
 		return err;
 
-	return mlx5_parse_cqe(cq, cqe64, cqe, cur_rsc, cur_srq, wc, cqe_ver, 0);
+	return mlx5_parse_cqe(cq, cqe64, cqe, cur_rsc, cur_srq, wc, cqe_ver, 0, is_shaded);
+}
+
+void migrrdma_start_poll(struct ibv_cq *ibcq) {
+	struct mlx5_cq *cq = to_mcq(ibcq);
+
+	cq->use_fake_index = 1;
+	cq->fake_index = cq->cons_index;
+}
+
+void migrrdma_end_poll(struct ibv_cq *ibcq) {
+	struct mlx5_cq *cq = to_mcq(ibcq);
+
+	cq->use_fake_index = 0;
+}
+
+int migrrdma_poll_cq(struct ibv_cq *ibcq, int ne,
+		struct ibv_wc *wc, struct ibv_qp **qps, struct ibv_srq **srqs) {
+	struct mlx5_cq *cq = to_mcq(ibcq);
+	struct mlx5_resource *rsc = NULL;
+	struct mlx5_srq *srq = NULL;
+	int npolled;
+	int err = CQ_OK;
+
+	for (npolled = 0; npolled < ne; ++npolled) {
+		err = migrrdma_poll_one(cq, &rsc, &srq, wc + npolled,
+				qps? qps + npolled: NULL, srqs? srqs + npolled: NULL, 1);
+
+		if(err != CQ_OK)
+			break;
+	}
+
+	return err == CQ_POLL_ERR ? err : npolled;
 }
 
 static inline int poll_cq(struct ibv_cq *ibcq, int ne,
-		      struct ibv_wc *wc, int cqe_ver)
-		      ALWAYS_INLINE;
+		      struct ibv_wc *wc, int cqe_ver);
+
 static inline int poll_cq(struct ibv_cq *ibcq, int ne,
 		      struct ibv_wc *wc, int cqe_ver)
 {
@@ -997,7 +1736,31 @@ static inline int poll_cq(struct ibv_cq *ibcq, int ne,
 	int npolled;
 	int err = CQ_OK;
 
-	if (cq->stall_enable) {
+	if(ibv_get_signal())
+		mlx5_spin_lock(&cq->lock);
+
+	if(cq->migr_flag) {
+		struct mlx5_buf tmp_buf;
+		__be32 *tmp_dbrec;
+
+		cq->migr_flag = 0;
+
+		memcpy(&tmp_buf, cq->active_buf, sizeof(tmp_buf));
+		memcpy(cq->active_buf, &cq->migr_buf, sizeof(tmp_buf));
+		memcpy(&cq->migr_buf, &tmp_buf, sizeof(tmp_buf));
+
+		tmp_dbrec = cq->dbrec;
+		cq->dbrec = cq->migr_dbrec;
+		cq->migr_dbrec = tmp_dbrec;
+
+		cq->shaded_flag = 1;
+		cq->tmp_cons_index = cq->cons_index;
+		memcpy(cq->tmp_buf.buf, cq->migr_buf.buf,
+					(ibcq->cqe + 1) * cq->cqe_sz);
+		cq->cons_index = 0;
+	}
+
+	if (cq->stall_enable && !cq->migr_flag) {
 		if (cq->stall_adaptive_enable) {
 			if (cq->stall_last_count)
 				mlx5_stall_cycles_poll_cq(cq->stall_last_count + cq->stall_cycles);
@@ -1007,19 +1770,32 @@ static inline int poll_cq(struct ibv_cq *ibcq, int ne,
 		}
 	}
 
-	mlx5_spin_lock(&cq->lock);
-
 	for (npolled = 0; npolled < ne; ++npolled) {
+		if(cq->migr_flag)
+			break;
+
+		if(ibcq->stop_flag)
+			break;
+
 		err = mlx5_poll_one(cq, &rsc, &srq, wc + npolled, cqe_ver);
+
 		if (err != CQ_OK)
 			break;
 	}
 
-	update_cons_index(cq);
+	while(cq->use_fake_index) {
+		uint32_t fake_index;
+		pthread_rwlock_rdlock(&cq->fake_index_rwlock);
+		fake_index = cq->fake_index;
+		pthread_rwlock_unlock(&cq->fake_index_rwlock);
+		if(cq->cons_index < fake_index) {
+			break;
+		}
+	}
 
-	mlx5_spin_unlock(&cq->lock);
+	update_cons_index(cq);
 
-	if (cq->stall_enable) {
+	if (cq->stall_enable && !cq->migr_flag) {
 		if (cq->stall_adaptive_enable) {
 			if (npolled == 0) {
 				cq->stall_cycles = max(cq->stall_cycles-mlx5_stall_cq_dec_step,
@@ -1039,6 +1815,9 @@ static inline int poll_cq(struct ibv_cq *ibcq, int ne,
 		}
 	}
 
+	if(ibv_get_signal())
+		mlx5_spin_unlock(&cq->lock);
+
 	return err == CQ_POLL_ERR ? err : npolled;
 }
 
@@ -1110,7 +1889,7 @@ static inline int mlx5_start_poll(struct ibv_cq_ex *ibcq, struct ibv_poll_cq_att
 		}
 	}
 
-	if (lock)
+	if (lock && ibv_get_signal())
 		mlx5_spin_lock(&cq->lock);
 
 	cq->cur_rsc = NULL;
@@ -1118,7 +1897,7 @@ static inline int mlx5_start_poll(struct ibv_cq_ex *ibcq, struct ibv_poll_cq_att
 
 	err = mlx5_get_next_cqe(cq, &cqe64, &cqe);
 	if (err == CQ_EMPTY) {
-		if (lock)
+		if (lock && ibv_get_signal())
 			mlx5_spin_unlock(&cq->lock);
 
 		if (stall) {
@@ -1138,7 +1917,7 @@ static inline int mlx5_start_poll(struct ibv_cq_ex *ibcq, struct ibv_poll_cq_att
 		cq->flags |= MLX5_CQ_FLAGS_FOUND_CQES;
 
 	err = mlx5_parse_lazy_cqe(cq, cqe64, cqe, cqe_version);
-	if (lock && err)
+	if (lock && ibv_get_signal() && err)
 		mlx5_spin_unlock(&cq->lock);
 
 	if (stall && err == CQ_POLL_ERR) {
@@ -1377,6 +2156,8 @@ static inline void mlx5_end_poll_lock(struct ibv_cq_ex *ibcq)
 	_mlx5_end_poll(ibcq, 1, 0);
 }
 
+#include <signal.h>
+
 int mlx5_poll_cq(struct ibv_cq *ibcq, int ne, struct ibv_wc *wc)
 {
 	return poll_cq(ibcq, ne, wc, 0);
@@ -1384,6 +2165,9 @@ int mlx5_poll_cq(struct ibv_cq *ibcq, int ne, struct ibv_wc *wc)
 
 int mlx5_poll_cq_v1(struct ibv_cq *ibcq, int ne, struct ibv_wc *wc)
 {
+	if(ibcq->stop_flag)
+		return 0;
+
 	return poll_cq(ibcq, ne, wc, 1);
 }
 
@@ -1459,8 +2243,21 @@ static inline enum ibv_wc_opcode mlx5_cq_read_wc_opcode(struct ibv_cq_ex *ibcq)
 static inline uint32_t mlx5_cq_read_wc_qp_num(struct ibv_cq_ex *ibcq)
 {
 	struct mlx5_cq *cq = to_mcq(ibv_cq_ex_to_cq(ibcq));
+	uint32_t qpn = be32toh(cq->cqe64->sop_drop_qpn) & 0xffffff;
 
-	return be32toh(cq->cqe64->sop_drop_qpn) & 0xffffff;
+	if(!cq->shaded_flag && !cq->migr_flag) {
+		uint32_t *qpn_dict = cq->verbs_cq.cq.context->device->qpn_dict;
+		return qpn_dict[qpn];
+	}
+	else {
+		int err;
+		uint32_t vqpn;
+		err = get_vqpn_from_old(qpn, &vqpn);
+		if(err) {
+			return CQ_POLL_ERR;
+		}
+		return vqpn;
+	}
 }
 
 static inline unsigned int mlx5_cq_read_wc_flags(struct ibv_cq_ex *ibcq)
@@ -1714,6 +2511,17 @@ int mlx5_arm_cq(struct ibv_cq *ibvcq, int solicited)
 	uint32_t ci;
 	uint32_t cmd;
 
+	if((cq->migr_flag || cq->shaded_flag) && ibvcq->channel) {
+		struct ib_uverbs_comp_event_desc desc;
+		desc.cq_handle = ibvcq;
+		if(write(ibvcq->channel->fd, &desc, sizeof(desc)) < 0) {
+			return -1;
+		}
+
+		ibvcq->arm_flag = 1;
+		return 0;
+	}
+
 	sn  = cq->arm_sn & 3;
 	ci  = cq->cons_index & 0xffffff;
 	cmd = solicited ? MLX5_CQ_DB_REQ_NOT_SOL : MLX5_CQ_DB_REQ_NOT;
@@ -1724,6 +2532,11 @@ int mlx5_arm_cq(struct ibv_cq *ibvcq, int solicited)
 
 	cq->dbrec[MLX5_CQ_ARM_DB] = htobe32(sn << 28 | cmd | ci);
 
+	if(cq->migr_flag) {
+		ibvcq->arm_flag = 1;
+		return 0;
+	}
+
 	/*
 	 * Make sure that the doorbell record in host memory is
 	 * written before ringing the doorbell via PCI WC MMIO.
@@ -1731,6 +2544,7 @@ int mlx5_arm_cq(struct ibv_cq *ibvcq, int solicited)
 	mmio_wc_start();
 
 	mmio_write64_be(ctx->cq_uar_reg + MLX5_CQ_DOORBELL, htobe64(doorbell));
+	ibvcq->arm_flag = 1;
 
 	mmio_flush_writes();
 
@@ -1844,9 +2658,11 @@ void __mlx5_cq_clean(struct mlx5_cq *cq, uint32_t rsn, struct mlx5_srq *srq)
 
 void mlx5_cq_clean(struct mlx5_cq *cq, uint32_t qpn, struct mlx5_srq *srq)
 {
-	mlx5_spin_lock(&cq->lock);
+	if(ibv_get_signal())
+		mlx5_spin_lock(&cq->lock);
 	__mlx5_cq_clean(cq, qpn, srq);
-	mlx5_spin_unlock(&cq->lock);
+	if(ibv_get_signal())
+		mlx5_spin_unlock(&cq->lock);
 }
 
 static uint8_t sw_ownership_bit(int n, int nent)
diff --git a/providers/mlx5/mlx5.c b/providers/mlx5/mlx5.c
index 14f1581..021a58d 100644
--- a/providers/mlx5/mlx5.c
+++ b/providers/mlx5/mlx5.c
@@ -101,6 +101,19 @@ static const struct verbs_context_ops mlx5_ctx_common_ops = {
 	.dealloc_mw    = mlx5_dealloc_mw,
 	.bind_mw       = mlx5_bind_mw,
 	.create_cq     = mlx5_create_cq,
+	.resume_cq	   = mlx5_resume_cq,
+	.uwrite_cq	   = mlx5_uwrite_cq,
+	.uwrite_qp	   = mlx5_uwrite_qp,
+	.uwrite_srq	   = mlx5_uwrite_srq,
+	.get_cons_index		= mlx5_get_cons_index,
+	.set_cons_index		= mlx5_set_cons_index,
+	.copy_cqe_to_shaded		= mlx5_copy_cqe_to_shaded,
+	.resume_srq				= mlx5_resume_srq,
+	.migrrdma_start_poll	= migrrdma_start_poll,
+	.migrrdma_end_poll		= migrrdma_end_poll,
+	.migrrdma_poll_cq		= migrrdma_poll_cq,
+	.migrrdma_start_inspect_qp = migrrdma_start_inspect_qp,
+	.migrrdma_start_inspect_qp_v2 = migrrdma_start_inspect_qp_v2,
 	.poll_cq       = mlx5_poll_cq,
 	.req_notify_cq = mlx5_arm_cq,
 	.cq_event      = mlx5_cq_event,
@@ -112,6 +125,20 @@ static const struct verbs_context_ops mlx5_ctx_common_ops = {
 	.destroy_srq   = mlx5_destroy_srq,
 	.post_srq_recv = mlx5_post_srq_recv,
 	.create_qp     = mlx5_create_qp,
+	.resume_qp	   = mlx5_resume_qp,
+	.free_qp	   = mlx5_free_qp,
+	.is_q_empty    = mlx5_is_q_empty,
+	.migrrdma_is_q_empty    = mlx5_migrrdma_is_q_empty,
+	.qp_get_n_posted		= mlx5_qp_get_n_posted,
+	.qp_get_n_acked			= mlx5_qp_get_n_acked,
+	.srq_get_n_acked		= mlx5_srq_get_n_acked,
+	.copy_qp	   = mlx5_copy_qp,
+	.calloc_qp	   = mlx5_calloc_qp,
+	.replay_recv_wr = mlx5_replay_recv_wr,
+	.prepare_qp_recv_replay = mlx5_prepare_qp_recv_replay,
+	.replay_srq_recv_wr = mlx5_replay_srq_recv_wr,
+	.prepare_srq_replay = mlx5_prepare_srq_replay,
+	.record_qp_index = mlx5_record_qp_index,
 	.query_qp      = mlx5_query_qp,
 	.modify_qp     = mlx5_modify_qp,
 	.destroy_qp    = mlx5_destroy_qp,
@@ -218,6 +245,17 @@ static int32_t get_free_uidx(struct mlx5_context *ctx)
 	return (tind << MLX5_UIDX_TABLE_SHIFT) | i;
 }
 
+void change_uidx(struct mlx5_context *ctx,
+				struct mlx5_qp *tgt, struct mlx5_qp *src) {
+	int32_t tind;
+	int32_t uidx = src->rsc.rsn;
+
+	pthread_mutex_lock(&ctx->uidx_table_mutex);
+	tind = uidx >> MLX5_UIDX_TABLE_SHIFT;
+	ctx->uidx_table[tind].table[uidx & MLX5_UIDX_TABLE_MASK] = tgt;
+	pthread_mutex_unlock(&ctx->uidx_table_mutex);
+}
+
 int32_t mlx5_store_uidx(struct mlx5_context *ctx, void *rsc)
 {
 	int32_t tind;
@@ -2257,6 +2295,10 @@ err_free:
 	return -1;
 }
 
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+
 static struct verbs_context *mlx5_alloc_context(struct ibv_device *ibdev,
 						int cmd_fd,
 						void *private_data)
@@ -2267,6 +2309,8 @@ static struct verbs_context *mlx5_alloc_context(struct ibv_device *ibdev,
 	struct mlx5dv_context_attr      *ctx_attr = private_data;
 	bool				always_devx = false;
 	int ret;
+	char fname[128];
+	int info_fd;
 
 	context = mlx5_init_context(ibdev, cmd_fd);
 	if (!context)
@@ -2282,7 +2326,6 @@ static struct verbs_context *mlx5_alloc_context(struct ibv_device *ibdev,
 	req.max_cqe_version = MLX5_CQE_VERSION_V1;
 	req.lib_caps |= (MLX5_LIB_CAP_4K_UAR | MLX5_LIB_CAP_DYN_UAR);
 	if (ctx_attr && ctx_attr->flags) {
-
 		if (!check_comp_mask(ctx_attr->flags,
 				     MLX5DV_CONTEXT_FLAGS_DEVX)) {
 			errno = EINVAL;
@@ -2312,6 +2355,34 @@ retry_open:
 	if (ret)
 		goto err;
 
+	context->ibv_ctx.context.lkey_fd = -1;
+	context->ibv_ctx.context.lkey_mapping = MAP_FAILED;
+	context->ibv_ctx.context.rkey_fd = -1;
+	context->ibv_ctx.context.rkey_mapping = MAP_FAILED;
+	
+	ret = ibv_cmd_install_footprint(&context->ibv_ctx.context);
+	if(ret) {
+		ibv_close_device(&context->ibv_ctx.context);
+		return NULL;
+	}
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/nc_uar",
+					rdma_getpid(&context->ibv_ctx.context),
+					context->ibv_ctx.context.cmd_fd);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_close_device(&context->ibv_ctx.context);
+		return NULL;
+	}
+
+	if(write(info_fd, &context->nc_uar, sizeof(context->nc_uar)) < 0) {
+		close(info_fd);
+		ibv_close_device(&context->ibv_ctx.context);
+		return NULL;
+	}
+
+	close(info_fd);
+
 	return &context->ibv_ctx;
 
 err:
@@ -2319,6 +2390,337 @@ err:
 	return NULL;
 }
 
+static struct verbs_context *mlx5_pre_resume_context(struct ibv_device *ibdev,
+								int cmd_fd) {
+	struct mlx5_context *mlx5_ctx;
+	struct mlx5_alloc_ucontext	req = {};
+	struct mlx5_alloc_ucontext_resp resp = {};
+	int err;
+
+	mlx5_ctx = mlx5_init_context(ibdev, cmd_fd);
+	if(!mlx5_ctx)
+		return NULL;
+	
+	req.total_num_bfregs = mlx5_ctx->tot_uuars;
+	req.num_low_latency_bfregs = mlx5_ctx->low_lat_uuars;
+	req.max_cqe_version = MLX5_CQE_VERSION_V1;
+	req.lib_caps |= (MLX5_LIB_CAP_4K_UAR | MLX5_LIB_CAP_DYN_UAR);
+	req.flags = MLX5_IB_ALLOC_UCTX_DEVX;
+
+	err = mlx5_cmd_get_context(mlx5_ctx, &req, sizeof(req),
+							&resp, sizeof(resp));
+	if(err) {
+		ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+		free(mlx5_ctx);
+		return NULL;
+	}
+
+	if(ibv_cmd_install_footprint(&mlx5_ctx->ibv_ctx.context)) {
+		ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+		free(mlx5_ctx);
+		return NULL;
+	}
+
+	if(ibv_cmd_install_ctx_resp(&mlx5_ctx->ibv_ctx.context, &resp, sizeof(resp))) {
+		ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+		free(mlx5_ctx);
+		return NULL;
+	}
+
+	return &mlx5_ctx->ibv_ctx;
+}
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+
+static struct verbs_context *mlx5_resume_context(struct ibv_device *ibdev,
+					int cmd_fd, int *async_fd, struct verbs_context *orig_ctx) {
+	struct mlx5_context *mlx5_ctx;
+	struct mlx5_alloc_ucontext	req = {};
+	struct mlx5_alloc_ucontext_resp resp = {};
+	char fname[128];
+	int ctx_resp_fd;
+	int err;
+	int info_fd;
+
+	mlx5_ctx = mlx5_init_context(ibdev, cmd_fd);
+	if(!mlx5_ctx)
+		return NULL;
+	
+	req.total_num_bfregs = mlx5_ctx->tot_uuars;
+	req.num_low_latency_bfregs = mlx5_ctx->low_lat_uuars;
+	req.max_cqe_version = MLX5_CQE_VERSION_V1;
+	req.lib_caps |= (MLX5_LIB_CAP_4K_UAR | MLX5_LIB_CAP_DYN_UAR);
+	req.flags = MLX5_IB_ALLOC_UCTX_DEVX;
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/ctx_resp", rdma_getpid(&mlx5_ctx->ibv_ctx.context), cmd_fd);
+	ctx_resp_fd = open(fname, O_RDONLY);
+	if(ctx_resp_fd < 0) {
+		ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+		free(mlx5_ctx);
+		return NULL;
+	}
+
+	if(read(ctx_resp_fd, &resp, sizeof(resp)) < 0) {
+		close(ctx_resp_fd);
+		ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+		free(mlx5_ctx);
+		return NULL;
+	}
+
+	close(ctx_resp_fd);
+
+	err = mlx5_set_context(mlx5_ctx, &resp.drv_payload, false);
+	if(err) {
+		ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+		free(mlx5_ctx);
+		return NULL;
+	}
+
+	for(int i = 0; i < mlx5_ctx->tot_uuars / (mlx5_ctx->num_uars_per_page * MLX5_NUM_NON_FP_BFREGS_PER_UAR); i++) {
+		typeof(mlx5_ctx->uar[i].reg) *content_p;
+
+		to_mctx(&orig_ctx->context)->uar[i].reg = mlx5_ctx->uar[i].reg;
+		content_p = malloc(sizeof(*content_p));
+		if(!content_p) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+
+		*content_p = mlx5_ctx->uar[i].reg;
+		if(register_update_mem(&to_mctx(&orig_ctx->context)->uar[i].reg,
+						sizeof(*content_p), content_p)) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+
+		if(mlx5_ctx->uar[i].reg &&
+					register_keep_mmap_region(mlx5_ctx->uar[i].reg,
+						to_mdev(mlx5_ctx->ibv_ctx.context.device)->page_size)) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+	}
+
+	for(int i = 0; i < mlx5_ctx->tot_uuars / (mlx5_ctx->num_uars_per_page * MLX5_NUM_NON_FP_BFREGS_PER_UAR) &&
+							to_mctx(&orig_ctx->context)->bfs; i++) {
+		for(int j = 0; j < mlx5_ctx->num_uars_per_page; j++) {
+			for(int k = 0; k < NUM_BFREGS_PER_UAR; k++) {
+				int bfi = (i * mlx5_ctx->num_uars_per_page + j) * NUM_BFREGS_PER_UAR + k;
+				typeof(mlx5_ctx->bfs[bfi].reg) *content_p;
+
+				to_mctx(&orig_ctx->context)->bfs[bfi].reg = mlx5_ctx->bfs[bfi].reg;
+				content_p = malloc(sizeof(*content_p));
+				*content_p = mlx5_ctx->bfs[bfi].reg;
+				if(register_update_mem(&to_mctx(&orig_ctx->context)->bfs[bfi].reg,
+								sizeof(*content_p), content_p)) {
+					ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+					free(mlx5_ctx);
+					return NULL;
+				}
+			}
+		}
+	}
+
+	{
+		typeof(mlx5_ctx->hca_core_clock) *content_p;
+
+		to_mctx(&orig_ctx->context)->hca_core_clock = mlx5_ctx->hca_core_clock;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = mlx5_ctx->hca_core_clock;
+		if(register_update_mem(&to_mctx(&orig_ctx->context)->hca_core_clock,
+								sizeof(*content_p), content_p)) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+
+		if(register_keep_mmap_region(mlx5_ctx->hca_core_clock -
+						(mlx5_ctx->core_clock.offset &
+						(to_mdev(mlx5_ctx->ibv_ctx.context.device)->page_size - 1)),
+						to_mdev(mlx5_ctx->ibv_ctx.context.device)->page_size)) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+	}
+
+	{
+		typeof(mlx5_ctx->clock_info_page) *content_p;
+
+		to_mctx(&orig_ctx->context)->clock_info_page = mlx5_ctx->clock_info_page;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = mlx5_ctx->clock_info_page;
+		if(register_update_mem(&to_mctx(&orig_ctx->context)->clock_info_page,
+								sizeof(*content_p), content_p)) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+
+		if(register_keep_mmap_region(mlx5_ctx->clock_info_page,
+						to_mdev(mlx5_ctx->ibv_ctx.context.device)->page_size)) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+	}
+
+	{
+		typeof(mlx5_ctx->nc_uar->uar) *content_p;
+
+		to_mctx(&orig_ctx->context)->nc_uar->uar = mlx5_ctx->nc_uar->uar;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = mlx5_ctx->nc_uar->uar;
+		if(register_update_mem(&to_mctx(&orig_ctx->context)->nc_uar->uar,
+								sizeof(*content_p), content_p)) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+
+		if(register_keep_mmap_region(mlx5_ctx->nc_uar->uar,
+							mlx5_ctx->nc_uar->length)) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+	}
+
+	{
+		typeof(mlx5_ctx->nc_uar->reg) *content_p;
+
+		to_mctx(&orig_ctx->context)->nc_uar->reg = mlx5_ctx->nc_uar->reg;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = mlx5_ctx->nc_uar->reg;
+		if(register_update_mem(&to_mctx(&orig_ctx->context)->nc_uar->reg,
+								sizeof(*content_p), content_p)) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+	}
+
+	{
+		typeof(mlx5_ctx->nc_uar->page_id) *content_p;
+
+		to_mctx(&orig_ctx->context)->nc_uar->page_id = mlx5_ctx->nc_uar->page_id;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = mlx5_ctx->nc_uar->page_id;
+		if(register_update_mem(&to_mctx(&orig_ctx->context)->nc_uar->page_id,
+								sizeof(*content_p), content_p)) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+	}
+
+	err = ibv_cmd_alloc_async_fd(&mlx5_ctx->ibv_ctx.context);
+	if(err) {
+		ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+		free(mlx5_ctx);
+		return NULL;
+	}
+
+	if(async_fd)
+		*async_fd = mlx5_ctx->ibv_ctx.context.async_fd;
+
+	err = ibv_cmd_register_async_fd(&mlx5_ctx->ibv_ctx.context, *async_fd);
+	if(err) {
+		ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+		free(mlx5_ctx);
+		return NULL;
+	}
+
+	to_mctx(&orig_ctx->context)->cq_uar_reg = mlx5_ctx->cq_uar_reg;
+
+	{
+		typeof(mlx5_ctx->cq_uar_reg) *content_p;
+
+		content_p = malloc(sizeof(*content_p));
+		if(!content_p) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+
+		*content_p = mlx5_ctx->cq_uar_reg;
+		if(register_update_mem(&to_mctx(&orig_ctx->context)->cq_uar_reg,
+						sizeof(*content_p), content_p)) {
+			ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+			free(mlx5_ctx);
+			return NULL;
+		}
+	}
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/nc_uar",
+					rdma_getpid(&mlx5_ctx->ibv_ctx.context),
+					mlx5_ctx->ibv_ctx.context.cmd_fd);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+		free(mlx5_ctx);
+		return NULL;
+	}
+
+	if(write(info_fd, &to_mctx(&orig_ctx->context)->nc_uar,
+					sizeof(to_mctx(&orig_ctx->context)->nc_uar)) < 0) {
+		close(info_fd);
+		ibv_close_device(&mlx5_ctx->ibv_ctx.context);
+		free(mlx5_ctx);
+		return NULL;
+	}
+
+	close(info_fd);
+
+	return &mlx5_ctx->ibv_ctx;
+}
+
+static void mlx5_free_tmp_context(struct ibv_context *context) {
+	struct verbs_context *vctx = container_of(context, struct verbs_context, context);
+	struct mlx5_context *mlx5_ctx = container_of(vctx, struct mlx5_context, ibv_ctx);
+	free(vctx->priv);
+	free(mlx5_ctx);
+}
+
+static struct ibv_context *mlx5_dup_context(struct ibv_context *context, struct ibv_qp *qp) {
+	struct mlx5_context *mlx5_ctx = to_mctx(context);
+	struct mlx5_context *dup_mlx5_ctx;
+	int usr_idx = 0;
+
+	dup_mlx5_ctx = calloc(1, sizeof(*dup_mlx5_ctx));
+	if(!dup_mlx5_ctx)
+		return NULL;
+	
+	memcpy(dup_mlx5_ctx, mlx5_ctx, sizeof(*mlx5_ctx));
+
+	dup_mlx5_ctx->uidx_table[usr_idx >> MLX5_UIDX_TABLE_SHIFT]
+				.table[usr_idx & MLX5_UIDX_TABLE_MASK] = to_mqp(qp);
+
+	list_head_init(&dup_mlx5_ctx->dyn_uar_bf_list);
+	list_head_init(&dup_mlx5_ctx->dyn_uar_qp_shared_list);
+	list_head_init(&dup_mlx5_ctx->dyn_uar_qp_dedicated_list);
+//	list_head_init(&dup_mlx5_ctx->hugetlb_list);
+//	list_head_init(&dup_mlx5_ctx->reserved_qpns.blk_list);
+
+	return &dup_mlx5_ctx->ibv_ctx.context;
+}
+
+static struct verbs_context *mlx5_get_ops_context(struct ibv_device *ibdev, int cmd_fd) {
+	struct mlx5_context *mlx5_ctx;
+
+	mlx5_ctx = mlx5_init_context(ibdev, cmd_fd);
+	if(!mlx5_ctx)
+		return NULL;
+	
+	verbs_set_ops(&mlx5_ctx->ibv_ctx, &mlx5_ctx_common_ops);
+	return &mlx5_ctx->ibv_ctx;
+}
+
 static struct verbs_context *mlx5_import_context(struct ibv_device *ibdev,
 						int cmd_fd)
 
@@ -2369,6 +2771,7 @@ static void mlx5_free_context(struct ibv_context *ibctx)
 		       page_size);
 	if (context->clock_info_page)
 		munmap((void *)context->clock_info_page, page_size);
+
 	close_debug_file(context);
 	clean_dyn_uars(ibctx);
 	reserved_qpn_blks_free(context);
@@ -2387,14 +2790,35 @@ static void mlx5_uninit_device(struct verbs_device *verbs_device)
 static struct verbs_device *mlx5_device_alloc(struct verbs_sysfs_dev *sysfs_dev)
 {
 	struct mlx5_device *dev;
+	struct ibv_device *ibdev;
+	char fname[128];
+	int fd;
 
 	dev = calloc(1, sizeof *dev);
 	if (!dev)
 		return NULL;
+	ibdev = &dev->verbs_dev.device;
 
 	dev->page_size   = sysconf(_SC_PAGESIZE);
 	dev->driver_abi_ver = sysfs_dev->abi_ver;
 
+	sprintf(fname, "/proc/rdma_uwrite/%s/qpn_dict",
+						sysfs_dev->ibdev_name);
+	fd = open(fname, O_RDONLY);
+	if(fd < 0) {
+		free(dev);
+		return NULL;
+	}
+
+	ibdev->qpn_dict = mmap(NULL, 4096 * 4096 * sizeof(uint32_t),
+					PROT_READ, MAP_SHARED, fd, 0);
+	if(ibdev->qpn_dict == MAP_FAILED) {
+		close(fd);
+		free(dev);
+		return NULL;
+	}
+
+	close(fd);
 	return &dev->verbs_dev;
 }
 
@@ -2406,6 +2830,11 @@ static const struct verbs_device_ops mlx5_dev_ops = {
 	.alloc_device = mlx5_device_alloc,
 	.uninit_device = mlx5_uninit_device,
 	.alloc_context = mlx5_alloc_context,
+	.resume_context = mlx5_resume_context,
+	.pre_resume_context = mlx5_pre_resume_context,
+	.free_tmp_context = mlx5_free_tmp_context,
+	.dup_context = mlx5_dup_context,
+	.get_ops_context = mlx5_get_ops_context,
 	.import_context = mlx5_import_context,
 };
 
diff --git a/providers/mlx5/mlx5.h b/providers/mlx5/mlx5.h
index 9509f83..90d4918 100644
--- a/providers/mlx5/mlx5.h
+++ b/providers/mlx5/mlx5.h
@@ -470,6 +470,8 @@ struct mlx5_cq {
 	struct verbs_cq			verbs_cq;
 	struct mlx5_buf			buf_a;
 	struct mlx5_buf			buf_b;
+	struct mlx5_buf			tmp_buf;
+	struct mlx5_buf			migr_buf;
 	struct mlx5_buf		       *active_buf;
 	struct mlx5_buf		       *resize_buf;
 	int				resize_cqes;
@@ -477,7 +479,13 @@ struct mlx5_cq {
 	struct mlx5_spinlock		lock;
 	uint32_t			cqn;
 	uint32_t			cons_index;
+	uint32_t			tmp_cons_index;
+	uint32_t			fake_index;
+	uint32_t			use_fake_index;
+	pthread_rwlock_t	fake_index_rwlock;
+	int					shaded_flag;
 	__be32			       *dbrec;
+	__be32					*migr_dbrec;
 	bool				custom_db;
 	int				arm_sn;
 	int				cqe_sz;
@@ -494,6 +502,8 @@ struct mlx5_cq {
 	int				cached_opcode;
 	struct mlx5dv_clock_info	last_clock_info;
 	struct ibv_pd			*parent_domain;
+
+	int				migr_flag;
 };
 
 struct mlx5_tag_entry {
@@ -512,12 +522,19 @@ struct mlx5_srq_op {
 	uint32_t	       wqe_head;
 };
 
+struct srq_recv_wr_item {
+	struct ibv_recv_wr			recv_wr;
+	struct list_head			list;
+};
+
 struct mlx5_srq {
 	struct mlx5_resource            rsc;  /* This struct must be first */
 	struct verbs_srq		vsrq;
 	struct mlx5_buf			buf;
+	struct mlx5_buf			migr_buf;
 	struct mlx5_spinlock		lock;
 	uint64_t		       *wrid;
+	uint64_t				*tmp_wrid;
 	uint32_t			srqn;
 	int				max;
 	int				max_gs;
@@ -527,6 +544,7 @@ struct mlx5_srq {
 	int				waitq_head;
 	int				waitq_tail;
 	__be32			       *db;
+	__be32					*migr_db;
 	bool				custom_db;
 	uint16_t			counter;
 	int				wq_sig;
@@ -539,6 +557,28 @@ struct mlx5_srq {
 	int				op_tail;
 	int				unexp_in;
 	int				unexp_out;
+
+	struct srq_recv_wr_item		*onflight_recv_wr;
+	struct srq_recv_wr_item		*migrrdma_onflight;
+	struct list_head			onflight_list;
+	struct list_head			migrrdma_onflight_list;
+	struct list_head			staged_onflight_list;
+	unsigned			onflight_cap;
+	unsigned			onflight_max_sge;
+	int				inspect_flag;
+
+	int				migr_flag;
+	int				rollback_head;
+	int				rollback_tail;
+	uint16_t		rollback_counter;
+
+	int				*staged_index;
+	unsigned		staged_tail;
+	unsigned		staged_head;
+	uint64_t		n_posted;
+	uint64_t		n_acked;
+	uint64_t		migrrdma_acked;
+	uint64_t		commit_posted;
 };
 
 
@@ -559,18 +599,26 @@ struct wr_list {
 
 struct mlx5_wq {
 	uint64_t		       *wrid;
+	uint64_t				*tmp_wrid;
 	unsigned		       *wqe_head;
+	unsigned				*tmp_wqe_head;
 	struct mlx5_spinlock		lock;
 	unsigned			wqe_cnt;
 	unsigned			max_post;
 	unsigned			head;
 	unsigned			tail;
+	unsigned			commit_head;
+	unsigned			commit_tail;
 	unsigned			cur_post;
 	int				max_gs;
 	int				wqe_shift;
 	int				offset;
 	void			       *qend;
 	uint32_t			*wr_data;
+	uint32_t			*tmp_wr_data;
+	uint64_t			n_posted;
+	uint64_t			n_acked;
+	uint64_t			commit_posted;
 };
 
 struct mlx5_devx_uar {
@@ -627,12 +675,14 @@ struct mlx5_qp {
 	struct mlx5dv_qp_ex		dv_qp;
 	struct ibv_qp		       *ibv_qp;
 	struct mlx5_buf                 buf;
+	struct mlx5_buf					migr_buf;
 	int                             max_inline_data;
 	int                             buf_size;
 	/* For Raw Packet QP, use different buffers for the SQ and RQ */
 	struct mlx5_buf                 sq_buf;
 	int				sq_buf_size;
 	struct mlx5_bf		       *bf;
+	struct mlx5_bf				*new_bf;
 
 	/* Start of new post send API specific fields */
 	bool				inl_wqe;
@@ -655,6 +705,7 @@ struct mlx5_qp {
 	struct mlx5_wq                  sq;
 
 	__be32                         *db;
+	__be32							*migr_db;
 	bool				custom_db;
 	struct mlx5_wq                  rq;
 	int                             wq_sig;
@@ -683,6 +734,31 @@ struct mlx5_qp {
 	 * write to the set_ece will clear this field.
 	 */
 	uint32_t			get_ece;
+
+	struct ibv_recv_wr	*onflight_recv_wr;
+	unsigned			onflight_tail;
+	unsigned			onflight_head;
+	unsigned			onflight_cap;
+	unsigned			onflight_max_sge;
+
+	struct ibv_send_wr	*cached_send_wr_list;
+	unsigned			n_cached;
+	unsigned			cached_cap;
+
+	struct mlx5_wq		rollback_rq;
+	struct mlx5_wq		rollback_sq;
+	struct mlx5_bf		migr_bf;
+	void				*migr_reg;
+
+	int					migr_flag;
+
+	struct mlx5_qp			*switch_qp;
+
+	struct mlx5_wq		migrrdma_rq;
+	struct mlx5_wq		migrrdma_sq;
+
+	struct mlx5_wq		tmp_sq;
+	uint64_t			two_sided_posted;
 };
 
 struct mlx5_ah {
@@ -1052,6 +1128,57 @@ int mlx5_bind_mw(struct ibv_qp *qp, struct ibv_mw *mw,
 struct ibv_cq *mlx5_create_cq(struct ibv_context *context, int cqe,
 			       struct ibv_comp_channel *channel,
 			       int comp_vector);
+
+struct ibv_cq *mlx5_resume_cq(struct ibv_context *context, struct ibv_cq *cq_meta,
+			int cqe, struct ibv_comp_channel *channel, int comp_vector,
+			void *buf_addr, void *db_addr, int vhandle);
+
+struct ibv_qp *mlx5_resume_qp(struct ibv_context *context, int pd_handle, int qp_handle,
+				struct ibv_qp_init_attr *attr, void *buf_addr, void *db_addr,
+				int32_t usr_idx, struct ibv_qp *orig_qp, unsigned long long *bf_reg);
+
+struct ibv_srq *mlx5_resume_srq(struct ibv_pd *pd, struct ibv_resume_srq_param *param);
+
+void mlx5_free_qp(struct ibv_qp *qp);
+
+int mlx5_is_q_empty(struct ibv_qp *qp);
+
+int mlx5_migrrdma_is_q_empty(struct ibv_qp *qp);
+
+void mlx5_copy_qp(struct ibv_qp *qp1, struct ibv_qp *qp2, void *param);
+
+struct ibv_qp *mlx5_calloc_qp(void);
+
+int mlx5_prepare_qp_recv_replay(struct ibv_qp *qp, struct ibv_qp *new_qp);
+
+int mlx5_replay_recv_wr(struct ibv_qp *qp);
+
+int mlx5_replay_srq_recv_wr(struct ibv_srq *srq, int head, int tail);
+
+int mlx5_prepare_srq_replay(struct ibv_srq *srq, struct ibv_srq *new_srq,
+						int *head, int *tail);
+
+uint32_t get_prod_index(struct mlx5_cq *cq);
+
+int mlx5_uwrite_cq(struct ibv_cq *cq, int cq_dir_fd);
+int mlx5_uwrite_qp(struct ibv_qp *qp, struct ibv_qp *new_qp);
+
+int mlx5_uwrite_srq(struct ibv_srq *srq, struct ibv_srq *new_srq);
+
+void migrrdma_start_poll(struct ibv_cq *ibcq);
+void migrrdma_end_poll(struct ibv_cq *ibcq);
+int migrrdma_poll_cq(struct ibv_cq *ibcq, int ne,
+		struct ibv_wc *wc, struct ibv_qp **qps, struct ibv_srq **srqs);
+
+void migrrdma_start_inspect_qp(struct ibv_qp *qp);
+void migrrdma_start_inspect_qp_v2(struct ibv_qp *qp);
+
+int mlx5_get_cons_index(struct ibv_cq *cq);
+void mlx5_set_cons_index(struct ibv_cq *cq, int cons_index);
+void mlx5_copy_cqe_to_shaded(struct ibv_cq *ibcq);
+
+void mlx5_record_qp_index(struct ibv_qp *qp);
+
 struct ibv_cq_ex *mlx5_create_cq_ex(struct ibv_context *context,
 				    struct ibv_cq_init_attr_ex *cq_attr);
 int mlx5_cq_fill_pfns(struct mlx5_cq *cq,
@@ -1085,6 +1212,13 @@ void mlx5_free_srq_wqe(struct mlx5_srq *srq, int ind);
 int mlx5_post_srq_recv(struct ibv_srq *ibsrq,
 		       struct ibv_recv_wr *wr,
 		       struct ibv_recv_wr **bad_wr);
+int __mlx5_post_srq_recv__(struct ibv_srq *ibsrq,
+		       struct ibv_recv_wr *wr,
+		       struct ibv_recv_wr **bad_wr);
+
+uint64_t mlx5_qp_get_n_posted(struct ibv_qp *qp);
+uint64_t mlx5_qp_get_n_acked(struct ibv_qp *qp);
+uint64_t mlx5_srq_get_n_acked(struct ibv_srq *srq);
 
 struct ibv_qp *mlx5_create_qp(struct ibv_pd *pd, struct ibv_qp_init_attr *attr);
 int mlx5_query_qp(struct ibv_qp *qp, struct ibv_qp_attr *attr,
@@ -1098,10 +1232,15 @@ int mlx5_modify_qp_rate_limit(struct ibv_qp *qp,
 			      struct ibv_qp_rate_limit_attr *attr);
 int mlx5_modify_qp_drain_sigerr(struct ibv_qp *qp);
 int mlx5_destroy_qp(struct ibv_qp *qp);
+int mlx5_destroy_qp_tmp(struct ibv_qp *qp);
 void mlx5_init_qp_indices(struct mlx5_qp *qp);
 void mlx5_init_rwq_indices(struct mlx5_rwq *rwq);
 int mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
 			  struct ibv_send_wr **bad_wr);
+int __mlx5_post_recv__(struct ibv_qp *ibqp, struct ibv_recv_wr *wr,
+			struct ibv_recv_wr **bad_wr, const int is_app);
+int __mlx5_post_send__(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
+		   struct ibv_send_wr **bad_wr, const int is_app);
 int mlx5_post_recv(struct ibv_qp *ibqp, struct ibv_recv_wr *wr,
 			  struct ibv_recv_wr **bad_wr);
 int mlx5_post_wq_recv(struct ibv_wq *ibwq, struct ibv_recv_wr *wr,
@@ -1113,6 +1252,8 @@ void mlx5_set_sq_sizes(struct mlx5_qp *qp, struct ibv_qp_cap *cap,
 struct mlx5_qp *mlx5_find_qp(struct mlx5_context *ctx, uint32_t qpn);
 int mlx5_store_qp(struct mlx5_context *ctx, uint32_t qpn, struct mlx5_qp *qp);
 void mlx5_clear_qp(struct mlx5_context *ctx, uint32_t qpn);
+void change_uidx(struct mlx5_context *ctx,
+			struct mlx5_qp *tgt, struct mlx5_qp *src);
 int32_t mlx5_store_uidx(struct mlx5_context *ctx, void *rsc);
 void mlx5_clear_uidx(struct mlx5_context *ctx, uint32_t uidx);
 struct mlx5_srq *mlx5_find_srq(struct mlx5_context *ctx, uint32_t srqn);
diff --git a/providers/mlx5/qp.c b/providers/mlx5/qp.c
index 8984eab..b0bb0eb 100644
--- a/providers/mlx5/qp.c
+++ b/providers/mlx5/qp.c
@@ -191,18 +191,66 @@ static int mlx5_wq_overflow(struct mlx5_wq *wq, int nreq, struct mlx5_cq *cq)
 	if (cur + nreq < wq->max_post)
 		return 0;
 
-	mlx5_spin_lock(&cq->lock);
+	if(ibv_get_signal())
+		mlx5_spin_lock(&cq->lock);
 	cur = wq->head - wq->tail;
-	mlx5_spin_unlock(&cq->lock);
+	if(ibv_get_signal())
+		mlx5_spin_unlock(&cq->lock);
 
 	return cur + nreq >= wq->max_post;
 }
 
-static inline void set_raddr_seg(struct mlx5_wqe_raddr_seg *rseg,
+#include <linux/in.h>
+#include <sys/types.h>
+#include <sys/socket.h>
+#include <unistd.h>
+
+struct msg_fmt {
+	pid_t					pid;
+	uint32_t				vrkey;
+};
+
+static inline void set_raddr_seg(struct ibv_qp *qp, struct mlx5_wqe_raddr_seg *rseg,
 				 uint64_t remote_addr, uint32_t rkey)
 {
+	uint32_t *rkey_arr = qp->rkey_arr;
+
+	if(!rkey_arr[rkey]) {
+		uint32_t *local_rkey_arr = qp->context->rkey_mapping;
+		uint32_t remote_real_rkey;
+		struct sockaddr_in remote_netaddr;
+		struct msg_fmt msg_fmt;
+		int sk;
+		int err;
+
+		sk = socket(AF_INET, SOCK_DGRAM, 0);
+		if(sk < 0) {
+			return;
+		}
+		
+		remote_netaddr.sin_family = AF_INET;
+		remote_netaddr.sin_port = htons(45645);
+		memcpy(&remote_netaddr.sin_addr.s_addr, &qp->rc_dest_gid.raw[12],
+								sizeof(uint32_t));
+		msg_fmt.pid = qp->dest_pid;
+		msg_fmt.vrkey = rkey;
+		err = sendto(sk, &msg_fmt, sizeof(msg_fmt), 0, (struct sockaddr*)&remote_netaddr,
+									sizeof(struct sockaddr));
+		if(err < 0) {
+			close(sk);
+			return;
+		}
+
+		err = recvfrom(sk, &remote_real_rkey, sizeof(remote_real_rkey), 0, NULL, NULL);
+		if(err < 0) {
+			return;
+		}
+
+		rkey_arr[rkey] = remote_real_rkey;
+	}
+
 	rseg->raddr    = htobe64(remote_addr);
-	rseg->rkey     = htobe32(rkey);
+	rseg->rkey     = htobe32(rkey_arr[rkey]);
 	rseg->reserved = 0;
 }
 
@@ -269,19 +317,21 @@ static void set_datagram_seg(struct mlx5_wqe_datagram_seg *dseg,
 			  wr->wr.ud.remote_qkey);
 }
 
-static void set_data_ptr_seg(struct mlx5_wqe_data_seg *dseg, struct ibv_sge *sg,
+static void set_data_ptr_seg(struct ibv_context *ctx, struct mlx5_wqe_data_seg *dseg, struct ibv_sge *sg,
 			     int offset)
 {
+	uint32_t *lkey_arr = ctx->lkey_mapping;
 	dseg->byte_count = htobe32(sg->length - offset);
-	dseg->lkey       = htobe32(sg->lkey);
+	dseg->lkey       = htobe32(lkey_arr[sg->lkey]);
 	dseg->addr       = htobe64(sg->addr + offset);
 }
 
-static void set_data_ptr_seg_atomic(struct mlx5_wqe_data_seg *dseg,
+static void set_data_ptr_seg_atomic(struct ibv_context *ctx, struct mlx5_wqe_data_seg *dseg,
 				    struct ibv_sge *sg)
 {
+	uint32_t *lkey_arr = ctx->lkey_mapping;
 	dseg->byte_count = htobe32(MLX5_ATOMIC_SIZE);
-	dseg->lkey       = htobe32(sg->lkey);
+	dseg->lkey       = htobe32(lkey_arr[sg->lkey]);
 	dseg->addr       = htobe64(sg->addr);
 }
 
@@ -760,6 +810,11 @@ static inline void post_send_db(struct mlx5_qp *qp, struct mlx5_bf *bf,
 
 	qp->sq.head += nreq;
 
+	if(qp->migr_flag) {
+		qp->sq.head -= nreq;
+		return;
+	}
+
 	/*
 	 * Make sure that descriptors are written before
 	 * updating doorbell record and ringing the doorbell
@@ -767,22 +822,38 @@ static inline void post_send_db(struct mlx5_qp *qp, struct mlx5_bf *bf,
 	udma_to_device_barrier();
 	qp->db[MLX5_SND_DBR] = htobe32(qp->sq.cur_post & 0xffff);
 
+	if(qp->migr_flag) {
+		qp->sq.head -= nreq;
+		return;
+	}
+
 	/* Make sure that the doorbell write happens before the memcpy
 	 * to WC memory below
 	 */
 	ctx = to_mctx(qp->ibv_qp->context);
 	if (bf->need_lock)
-		mmio_wc_spinlock(&bf->lock.lock);
-	else
-		mmio_wc_start();
+			mmio_wc_spinlock(&bf->lock.lock);
+		else
+			mmio_wc_start();
+
+	if(qp->migr_flag) {
+		qp->sq.head -= nreq;
+		return;
+	}
 
 	if (!ctx->shut_up_bf && nreq == 1 && bf->uuarn &&
-	    (inl || ctx->prefer_bf) && size > 1 &&
+				(inl || ctx->prefer_bf) && size > 1 &&
 	    size <= bf->buf_size / 16)
-		mlx5_bf_copy(bf->reg + bf->offset, ctrl,
-			     align(size * 16, 64), qp);
+			mlx5_bf_copy(bf->reg + bf->offset, ctrl,
+					align(size * 16, 64), qp);
 	else
-		mmio_write64_be(bf->reg + bf->offset, *(__be64 *)ctrl);
+			mmio_write64_be(bf->reg + bf->offset, *(__be64 *)ctrl);
+
+	qp->sq.n_posted += nreq;
+	if(qp->migr_flag) {
+		qp->sq.head -= nreq;
+		return;
+	}
 
 	/*
 	 * use mmio_flush_writes() to ensure write combining buffers are
@@ -795,7 +866,7 @@ static inline void post_send_db(struct mlx5_qp *qp, struct mlx5_bf *bf,
 	 * Flush before toggling bf_offset to be latency oriented.
 	 */
 	mmio_flush_writes();
-	bf->offset ^= bf->buf_size;
+		bf->offset ^= bf->buf_size;
 	if (bf->need_lock)
 		mlx5_spin_unlock(&bf->lock);
 }
@@ -824,12 +895,14 @@ static inline int _mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
 	uint8_t next_fence;
 	uint32_t max_tso = 0;
 	FILE *fp = to_mctx(ibqp->context)->dbg_fp; /* The compiler ignores in non-debug mode */
-
-	mlx5_spin_lock(&qp->sq.lock);
+	uint64_t n_two_sided = 0;
 
 	next_fence = qp->fm_cache;
 
 	for (nreq = 0; wr; ++nreq, wr = wr->next) {
+		if(qp->migr_flag)
+			break;
+
 		if (unlikely(wr->opcode < 0 ||
 		    wr->opcode >= sizeof mlx5_ib_opcode / sizeof mlx5_ib_opcode[0])) {
 			mlx5_dbg(fp, MLX5_DBG_QP_SEND, "bad opcode %d\n", wr->opcode);
@@ -846,6 +919,11 @@ static inline int _mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
 			goto out;
 		}
 
+		if(wr->opcode == IBV_WR_SEND || wr->opcode == IBV_WR_SEND_WITH_IMM
+						|| wr->opcode == IBV_WR_SEND_WITH_INV) {
+			n_two_sided++;
+		}
+
 		if (unlikely(wr->num_sge > qp->sq.max_gs)) {
 			mlx5_dbg(fp, MLX5_DBG_QP_SEND, "max gs exceeded %d (max = %d)\n",
 				 wr->num_sge, qp->sq.max_gs);
@@ -864,7 +942,7 @@ static inline int _mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
 		*(uint32_t *)(seg + 8) = 0;
 		ctrl->imm = send_ieth(wr);
 		ctrl->fm_ce_se = qp->sq_signal_bits | fence |
-			(wr->send_flags & IBV_SEND_SIGNALED ?
+			((wr->send_flags & IBV_SEND_SIGNALED) ?
 			 MLX5_WQE_CTRL_CQ_UPDATE : 0) |
 			(wr->send_flags & IBV_SEND_SOLICITED ?
 			 MLX5_WQE_CTRL_SOLICITED : 0);
@@ -887,7 +965,7 @@ static inline int _mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
 			case IBV_WR_RDMA_READ:
 			case IBV_WR_RDMA_WRITE:
 			case IBV_WR_RDMA_WRITE_WITH_IMM:
-				set_raddr_seg(seg, wr->wr.rdma.remote_addr,
+				set_raddr_seg(ibqp, seg, wr->wr.rdma.remote_addr,
 					      wr->wr.rdma.rkey);
 				seg  += sizeof(struct mlx5_wqe_raddr_seg);
 				size += sizeof(struct mlx5_wqe_raddr_seg) / 16;
@@ -901,7 +979,7 @@ static inline int _mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
 					*bad_wr = wr;
 					goto out;
 				}
-				set_raddr_seg(seg, wr->wr.atomic.remote_addr,
+				set_raddr_seg(ibqp, seg, wr->wr.atomic.remote_addr,
 					      wr->wr.atomic.rkey);
 				seg  += sizeof(struct mlx5_wqe_raddr_seg);
 
@@ -954,7 +1032,7 @@ static inline int _mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
 			switch (wr->opcode) {
 			case IBV_WR_RDMA_WRITE:
 			case IBV_WR_RDMA_WRITE_WITH_IMM:
-				set_raddr_seg(seg, wr->wr.rdma.remote_addr,
+				set_raddr_seg(ibqp, seg, wr->wr.rdma.remote_addr,
 					      wr->wr.rdma.rkey);
 				seg  += sizeof(struct mlx5_wqe_raddr_seg);
 				size += sizeof(struct mlx5_wqe_raddr_seg) / 16;
@@ -1099,7 +1177,7 @@ static inline int _mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
 						   IBV_WR_ATOMIC_CMP_AND_SWP ||
 						   wr->opcode ==
 						   IBV_WR_ATOMIC_FETCH_AND_ADD))
-						set_data_ptr_seg_atomic(dpseg, wr->sg_list + i);
+						set_data_ptr_seg_atomic(ibqp->context, dpseg, wr->sg_list + i);
 					else {
 						if (unlikely(wr->opcode == IBV_WR_TSO)) {
 							if (max_tso < wr->sg_list[i].length) {
@@ -1109,7 +1187,7 @@ static inline int _mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
 							}
 							max_tso -= wr->sg_list[i].length;
 						}
-						set_data_ptr_seg(dpseg, wr->sg_list + i,
+						set_data_ptr_seg(ibqp->context, dpseg, wr->sg_list + i,
 								 sg_copy_ptr.offset);
 					}
 					sg_copy_ptr.offset = 0;
@@ -1123,7 +1201,10 @@ static inline int _mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
 		ctrl->opmod_idx_opcode = htobe32(((qp->sq.cur_post & 0xffff) << 8) |
 					       mlx5_opcode			 |
 					       (opmod << 24));
-		ctrl->qpn_ds = htobe32(size | (ibqp->qp_num << 8));
+		uint32_t real_qpn;
+		real_qpn = ibqp->real_qpn;
+//		ctrl->qpn_ds = htobe32(size | (ibqp->qp_num << 8));
+		ctrl->qpn_ds = htobe32(size | (real_qpn << 8));
 
 		if (unlikely(qp->wq_sig))
 			ctrl->signature = wq_sig(ctrl);
@@ -1140,32 +1221,198 @@ static inline int _mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
 
 out:
 	qp->fm_cache = next_fence;
-	post_send_db(qp, bf, nreq, inl, size, ctrl);
-
-	mlx5_spin_unlock(&qp->sq.lock);
+	if(!qp->migr_flag) {
+		qp->two_sided_posted += n_two_sided;
+		post_send_db(qp, bf, nreq, inl, size, ctrl);
+		if(qp->migr_flag) {
+			qp->two_sided_posted -= n_two_sided;
+		}
+	}
 
 	return err;
 }
 
-int mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
-		   struct ibv_send_wr **bad_wr)
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <unistd.h>
+#include <signal.h>
+
+int __mlx5_post_send__(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
+		   struct ibv_send_wr **bad_wr, const int is_app)
 {
+	struct ibv_send_wr *cur_wr;
+	struct mlx5_qp *mqp = to_mqp(ibqp);
+	int err;
+
 #ifdef MW_DEBUG
 	if (wr->opcode == IBV_WR_BIND_MW) {
-		if (wr->bind_mw.mw->type == IBV_MW_TYPE_1)
-			return EINVAL;
+		if (wr->bind_mw.mw->type == IBV_MW_TYPE_1) {
+			err = EINVAL;
+			goto out;
+		}
 
 		if (!wr->bind_mw.bind_info.mr ||
 		    !wr->bind_mw.bind_info.addr ||
-		    !wr->bind_mw.bind_info.length)
-			return EINVAL;
+		    !wr->bind_mw.bind_info.length) {
+			err = EINVAL;
+			goto out;
+		}
 
-		if (wr->bind_mw.bind_info.mr->pd != wr->bind_mw.mw->pd)
-			return EINVAL;
+		if (wr->bind_mw.bind_info.mr->pd != wr->bind_mw.mw->pd) {
+			err = EINVAL;
+			goto out;
+		}
 	}
 #endif
 
-	return _mlx5_post_send(ibqp, wr, bad_wr);
+	if(ibv_get_signal() && is_app)
+		mlx5_spin_lock(&mqp->sq.lock);
+	if(is_app && mqp->migr_flag) {
+		struct mlx5_buf tmp_buf;
+		__be32 *tmp_db;
+
+		mqp->migr_flag = 0;
+		if(mqp->switch_qp) {
+			off_t off = offsetof(struct mlx5_qp, verbs_qp.qp.rwlock);
+			off_t off2 = offsetof(struct mlx5_qp, verbs_qp.qp.rwlock)
+							+ sizeof(mqp->verbs_qp.qp.rwlock);
+			struct mlx5_qp *mqp2 = mqp->switch_qp;
+			struct mlx5_qp tmp_mqp;
+
+			mqp2->sq.tail -= (mqp->sq.head - mqp->sq.tail);
+			memcpy(&mqp2->tmp_sq, &mqp->sq, sizeof(struct mlx5_wq));
+			change_uidx(to_mctx(ibqp->context), mqp, mqp2);
+
+			memcpy(&tmp_mqp, mqp, off);
+			memcpy((void*)&tmp_mqp + off2, (void*)mqp + off2,
+									sizeof(struct mlx5_qp) - off2);
+			memcpy(mqp, mqp2, off);
+			memcpy((void *)mqp + off2, (void *)mqp2 + off2,
+									sizeof(struct mlx5_qp) - off2);
+			memcpy(mqp2, &tmp_mqp, off);
+			memcpy((void*)mqp2 + off2, (void*)&tmp_mqp + off2,
+									sizeof(struct mlx5_qp) - off2);
+
+			mqp->switch_qp = mqp2;
+			goto rollback;
+		}
+
+		memcpy(&tmp_buf, &mqp->buf, sizeof(tmp_buf));
+		memcpy(&mqp->buf, &mqp->migr_buf, sizeof(tmp_buf));
+		memcpy(&mqp->migr_buf, &tmp_buf, sizeof(tmp_buf));
+
+		tmp_db = mqp->db;
+		mqp->db = mqp->migr_db;
+		mqp->migr_db = tmp_db;
+
+//		memcpy(mqp->bf, &mqp->migr_bf, sizeof(struct mlx5_bf));
+		mqp->bf = get_alloc_bf(mqp->new_bf);
+		if(!mqp->bf) {
+			mqp->bf = calloc(1, sizeof(struct mlx5_bf));
+			memcpy(mqp->bf, &mqp->migr_bf, sizeof(struct mlx5_bf));
+			add_bf_addr_map_entry(mqp->new_bf, mqp->bf);
+		}
+
+		mqp->rollback_rq.tail += mqp->rq.tail - mqp->rq.commit_tail;
+		if(mqp->rollback_rq.tail > mqp->rollback_rq.head) {
+			mqp->rollback_rq.tail = mqp->rollback_rq.head;
+		}
+
+		memcpy(&mqp->rq, &mqp->rollback_rq, offsetof(struct mlx5_wq, lock));
+		memcpy(&mqp->rq.wqe_cnt, &mqp->rollback_rq.wqe_cnt, 
+					sizeof(struct mlx5_wq) - offsetof(struct mlx5_wq, wqe_cnt));
+
+		mqp->rollback_sq.tail += mqp->sq.tail - mqp->sq.commit_tail;
+		if(mqp->rollback_sq.tail > mqp->rollback_sq.head) {
+			mqp->rollback_sq.tail = mqp->rollback_sq.head;
+		}
+
+		memcpy(&mqp->sq, &mqp->rollback_sq, offsetof(struct mlx5_wq, lock));
+		memcpy(&mqp->sq.wqe_cnt, &mqp->rollback_sq.wqe_cnt, 
+					sizeof(struct mlx5_wq) - offsetof(struct mlx5_wq, wqe_cnt));
+
+		mqp->sq_start = mqp->buf.buf + mqp->sq.offset;
+		mqp->sq.qend = mqp->buf.buf + mqp->sq.offset +
+						(mqp->sq.wqe_cnt << mqp->sq.wqe_shift);
+
+		mqp->onflight_tail = 0;
+		mqp->onflight_head = 0;
+
+		del_qpn_dict_node_2(ibqp);
+		add_qpn_dict_node(ibqp);
+	}
+
+rollback:
+	mqp->sq.commit_tail = mqp->sq.tail;
+	err = _mlx5_post_send(ibqp, wr, bad_wr);
+	if(is_app && mqp->migr_flag) {
+		if(ibv_get_signal() && is_app)
+			mlx5_spin_unlock(&mqp->sq.lock);
+		return __mlx5_post_send__(ibqp, wr, bad_wr, is_app);
+	}
+
+	if(ibv_get_signal() && is_app)
+		mlx5_spin_unlock(&mqp->sq.lock);
+	return err;
+}
+
+int mlx5_post_send(struct ibv_qp *ibqp, struct ibv_send_wr *wr,
+		   struct ibv_send_wr **bad_wr) {
+	int suspend_flag;
+	struct mlx5_qp *mqp = to_mqp(ibqp);
+
+	pthread_rwlock_rdlock(&ibqp->rwlock);
+	suspend_flag = ibqp->pause_flag;
+	pthread_rwlock_unlock(&ibqp->rwlock);
+
+	if(suspend_flag) {
+		int staged_ncached;
+		int first = 1;
+		struct ibv_send_wr *last_wr;
+		struct ibv_send_wr *this_first_wr;
+
+		mlx5_spin_lock(&mqp->sq.lock);
+		staged_ncached = mqp->n_cached;
+		for(; wr != NULL; wr = wr->next) {
+			struct ibv_send_wr *this_wr = mqp->cached_send_wr_list +
+								staged_ncached;
+			struct ibv_sge *sg_list = this_wr->sg_list;
+
+			memcpy(this_wr, wr, sizeof(struct ibv_send_wr));
+			this_wr->sg_list = sg_list;
+			this_wr->next = NULL;
+			if(staged_ncached > 0 && !first) {
+				(this_wr-1)->next = this_wr;
+			}
+			first = 0;
+			this_wr->num_sge = wr->num_sge;
+			memcpy(this_wr->sg_list, wr->sg_list, sizeof(struct ibv_sge) * wr->num_sge);
+
+			staged_ncached++;
+		}
+
+		last_wr = mqp->cached_send_wr_list + mqp->n_cached - 1;
+		this_first_wr = mqp->cached_send_wr_list + mqp->n_cached;
+
+		pthread_rwlock_rdlock(&ibqp->rwlock);
+		suspend_flag = ibqp->pause_flag;
+		pthread_rwlock_unlock(&ibqp->rwlock);
+
+		if(!suspend_flag) {
+			mqp->n_cached = 0;
+			mlx5_spin_unlock(&mqp->sq.lock);
+			return __mlx5_post_send__(ibqp, wr, bad_wr, 1);
+		}
+
+		if(mqp->n_cached > 0)
+			last_wr->next = this_first_wr;
+		mqp->n_cached = staged_ncached;
+		mlx5_spin_unlock(&mqp->sq.lock);
+		return 0;
+	}
+
+	mqp->n_cached = 0;
+	return __mlx5_post_send__(ibqp, wr, bad_wr, 1);
 }
 
 enum {
@@ -1281,7 +1528,9 @@ static inline void _common_wqe_init(struct ibv_qp_ex *ibqp,
 
 static inline void _common_wqe_finilize(struct mlx5_qp *mqp)
 {
-	mqp->cur_ctrl->qpn_ds = htobe32(mqp->cur_size | (mqp->ibv_qp->qp_num << 8));
+	uint32_t real_qpn;
+	real_qpn = mqp->ibv_qp->real_qpn;
+	mqp->cur_ctrl->qpn_ds = htobe32(mqp->cur_size | (real_qpn << 8));
 
 	if (unlikely(mqp->wq_sig))
 		mqp->cur_ctrl->signature = wq_sig(mqp->cur_ctrl);
@@ -1459,7 +1708,7 @@ static inline void _mlx5_send_wr_rdma(struct ibv_qp_ex *ibqp,
 	if (unlikely(raddr_seg == mqp->sq.qend))
 		raddr_seg = mlx5_get_send_wqe(mqp, 0);
 
-	set_raddr_seg(raddr_seg, remote_addr, rkey);
+	set_raddr_seg((struct ibv_qp *)ibqp, raddr_seg, remote_addr, rkey);
 
 	mqp->cur_data = raddr_seg + sizeof(struct mlx5_wqe_raddr_seg);
 	mqp->cur_size = (sizeof(struct mlx5_wqe_ctrl_seg) + transport_seg_sz +
@@ -1519,7 +1768,7 @@ static inline void _mlx5_send_wr_atomic(struct ibv_qp_ex *ibqp, uint32_t rkey,
 	if (unlikely(raddr_seg == mqp->sq.qend))
 		raddr_seg = mlx5_get_send_wqe(mqp, 0);
 
-	set_raddr_seg(raddr_seg, remote_addr, rkey);
+	set_raddr_seg((struct ibv_qp *)ibqp, raddr_seg, remote_addr, rkey);
 
 	_set_atomic_seg((struct mlx5_wqe_atomic_seg *)(raddr_seg + sizeof(struct mlx5_wqe_raddr_seg)),
 			ib_op, swap, compare_add);
@@ -3192,7 +3441,11 @@ int mlx5_bind_mw(struct ibv_qp *qp, struct ibv_mw *mw,
 	wr.bind_mw.mw = mw;
 	wr.bind_mw.rkey = ibv_inc_rkey(mw->rkey);
 
+	if(ibv_get_signal())
+		mlx5_spin_lock(&to_mqp(qp)->sq.lock);
 	ret = _mlx5_post_send(qp, &wr, &bad_wr);
+	if(ibv_get_signal())
+		mlx5_spin_unlock(&to_mqp(qp)->sq.lock);
 	if (ret)
 		return ret;
 
@@ -3236,7 +3489,8 @@ int mlx5_post_wq_recv(struct ibv_wq *ibwq, struct ibv_recv_wr *wr,
 	int i, j;
 	struct mlx5_rwqe_sig *sig;
 
-	mlx5_spin_lock(&rwq->rq.lock);
+	if(ibv_get_signal())
+		mlx5_spin_lock(&rwq->rq.lock);
 
 	ind = rwq->rq.head & (rwq->rq.wqe_cnt - 1);
 
@@ -3264,7 +3518,7 @@ int mlx5_post_wq_recv(struct ibv_wq *ibwq, struct ibv_recv_wr *wr,
 		for (i = 0, j = 0; i < wr->num_sge; ++i) {
 			if (unlikely(!wr->sg_list[i].length))
 				continue;
-			set_data_ptr_seg(scat + j++, wr->sg_list + i, 0);
+			set_data_ptr_seg(ibwq->context, scat + j++, wr->sg_list + i, 0);
 		}
 
 		if (j < rwq->rq.max_gs) {
@@ -3293,13 +3547,14 @@ out:
 		*(rwq->recv_db) = htobe32(rwq->rq.head & 0xffff);
 	}
 
-	mlx5_spin_unlock(&rwq->rq.lock);
+	if(ibv_get_signal())
+		mlx5_spin_unlock(&rwq->rq.lock);
 
 	return err;
 }
 
-int mlx5_post_recv(struct ibv_qp *ibqp, struct ibv_recv_wr *wr,
-		   struct ibv_recv_wr **bad_wr)
+int __mlx5_post_recv__(struct ibv_qp *ibqp, struct ibv_recv_wr *wr,
+		   struct ibv_recv_wr **bad_wr, const int is_app)
 {
 	struct mlx5_qp *qp = to_mqp(ibqp);
 	struct mlx5_wqe_data_seg *scat;
@@ -3308,8 +3563,11 @@ int mlx5_post_recv(struct ibv_qp *ibqp, struct ibv_recv_wr *wr,
 	int ind;
 	int i, j;
 	struct mlx5_rwqe_sig *sig;
+	struct ibv_recv_wr *cur_wr;
+	struct mlx5_qp *mqp = to_mqp(ibqp);
 
-	mlx5_spin_lock(&qp->rq.lock);
+	if(ibv_get_signal() && is_app)
+		mlx5_spin_lock(&qp->rq.lock);
 
 	ind = qp->rq.head & (qp->rq.wqe_cnt - 1);
 
@@ -3337,7 +3595,7 @@ int mlx5_post_recv(struct ibv_qp *ibqp, struct ibv_recv_wr *wr,
 		for (i = 0, j = 0; i < wr->num_sge; ++i) {
 			if (unlikely(!wr->sg_list[i].length))
 				continue;
-			set_data_ptr_seg(scat + j++, wr->sg_list + i, 0);
+			set_data_ptr_seg(ibqp->context, scat + j++, wr->sg_list + i, 0);
 		}
 
 		if (j < qp->rq.max_gs) {
@@ -3356,9 +3614,14 @@ int mlx5_post_recv(struct ibv_qp *ibqp, struct ibv_recv_wr *wr,
 	}
 
 out:
-	if (likely(nreq)) {
+	if (likely(nreq && !qp->migr_flag)) {
 		qp->rq.head += nreq;
 
+		if(qp->migr_flag) {
+			qp->rq.head -= nreq;
+			goto unlock;
+		}
+
 		/*
 		 * Make sure that descriptors are written before
 		 * doorbell record.
@@ -3373,13 +3636,125 @@ out:
 		 * differently in the hardware.
 		 */
 		if (likely(!((ibqp->qp_type == IBV_QPT_RAW_PACKET ||
-			      qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY) &&
+			    		qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY) &&
 			     ibqp->state < IBV_QPS_RTR)))
-			qp->db[MLX5_RCV_DBR] = htobe32(qp->rq.head & 0xffff);
+				qp->db[MLX5_RCV_DBR] = htobe32(qp->rq.head & 0xffff);
+
+		qp->rq.n_posted += nreq;
+		if(qp->migr_flag) {
+			qp->rq.head -= nreq;
+		}
 	}
 
-	mlx5_spin_unlock(&qp->rq.lock);
+unlock:
+	if(ibv_get_signal() && is_app)
+		mlx5_spin_unlock(&qp->rq.lock);
+
+	return err;
+}
+
+int mlx5_post_recv(struct ibv_qp *ibqp, struct ibv_recv_wr *wr,
+		   struct ibv_recv_wr **bad_wr) {
+	struct mlx5_qp *mqp = to_mqp(ibqp);
+	int err;
+
+	pthread_rwlock_rdlock(&ibqp->rwlock);
+	if(mqp->migr_flag) {
+		struct mlx5_buf tmp_buf;
+		__be32 *tmp_db;
+
+		mqp->migr_flag = 0;
+		if(mqp->switch_qp) {
+			off_t off = offsetof(struct mlx5_qp, verbs_qp.qp.rwlock);
+			off_t off2 = offsetof(struct mlx5_qp, verbs_qp.qp.rwlock)
+							+ sizeof(mqp->verbs_qp.qp.rwlock);
+			struct mlx5_qp *mqp2 = mqp->switch_qp;
+			struct mlx5_qp tmp_mqp;
+
+			change_uidx(to_mctx(ibqp->context), mqp, mqp2);
+
+			memcpy(&tmp_mqp, mqp, off);
+			memcpy((void*)&tmp_mqp + off2, (void*)mqp + off2,
+									sizeof(struct mlx5_qp) - off2);
+			memcpy(mqp, mqp2, off);
+			memcpy((void *)mqp + off2, (void *)mqp2 + off2,
+									sizeof(struct mlx5_qp) - off2);
+			memcpy(mqp2, &tmp_mqp, off);
+			memcpy((void*)mqp2 + off2, (void*)&tmp_mqp + off2,
+									sizeof(struct mlx5_qp) - off2);
+
+			mqp->switch_qp = mqp2;
+			goto rollback;
+		}
+
+		memcpy(&tmp_buf, &mqp->buf, sizeof(tmp_buf));
+		memcpy(&mqp->buf, &mqp->migr_buf, sizeof(tmp_buf));
+		memcpy(&mqp->migr_buf, &tmp_buf, sizeof(tmp_buf));
+
+		tmp_db = mqp->db;
+		mqp->db = mqp->migr_db;
+		mqp->migr_db = tmp_db;
+
+//		memcpy(mqp->bf, &mqp->migr_bf, sizeof(struct mlx5_bf));
+		mqp->bf = get_alloc_bf(mqp->new_bf);
+		if(!mqp->bf) {
+			mqp->bf = calloc(1, sizeof(struct mlx5_bf));
+			memcpy(mqp->bf, &mqp->migr_bf, sizeof(struct mlx5_bf));
+			add_bf_addr_map_entry(mqp->new_bf, mqp->bf);
+		}
+
+		mqp->rollback_rq.tail += mqp->rq.tail - mqp->rq.commit_tail;
+		if(mqp->rollback_rq.tail > mqp->rollback_rq.head) {
+			mqp->rollback_rq.tail = mqp->rollback_rq.head;
+		}
+
+		memcpy(&mqp->rq, &mqp->rollback_rq, offsetof(struct mlx5_wq, lock));
+		memcpy(&mqp->rq.wqe_cnt, &mqp->rollback_rq.wqe_cnt, 
+					sizeof(struct mlx5_wq) - offsetof(struct mlx5_wq, wqe_cnt));
+
+		mqp->rollback_sq.tail += mqp->sq.tail - mqp->sq.commit_tail;
+		if(mqp->rollback_sq.tail > mqp->rollback_sq.head) {
+			mqp->rollback_sq.tail = mqp->rollback_sq.head;
+		}
+
+		memcpy(&mqp->sq, &mqp->rollback_sq, offsetof(struct mlx5_wq, lock));
+		memcpy(&mqp->sq.wqe_cnt, &mqp->rollback_sq.wqe_cnt, 
+					sizeof(struct mlx5_wq) - offsetof(struct mlx5_wq, wqe_cnt));
+
+		mqp->sq_start = mqp->buf.buf + mqp->sq.offset;
+		mqp->sq.qend = mqp->buf.buf + mqp->sq.offset +
+						(mqp->sq.wqe_cnt << mqp->sq.wqe_shift);
+
+		mqp->onflight_tail = 0;
+		mqp->onflight_head = 0;
+
+		del_qpn_dict_node_2(ibqp);
+		add_qpn_dict_node(ibqp);
+	}
+
+rollback:
+	mqp->rq.commit_tail = mqp->rq.tail;
+	err = __mlx5_post_recv__(ibqp, wr, bad_wr, 1);
+
+	for(; wr != NULL && wr != *bad_wr; wr = wr->next) {
+		struct ibv_recv_wr *this_wr = mqp->onflight_recv_wr +
+					((mqp->onflight_head++) % mqp->onflight_cap);
+
+		if(mqp->migr_flag)
+			break;
+
+		this_wr->wr_id = wr->wr_id;
+		this_wr->next = NULL;
+		this_wr->num_sge = wr->num_sge;
+		memcpy(this_wr->sg_list, wr->sg_list, sizeof(struct ibv_sge) * wr->num_sge);
+	}
+
+	if(mqp->migr_flag) {
+		pthread_rwlock_unlock(&ibqp->rwlock);
+		return mlx5_post_recv(ibqp, wr, bad_wr);
+	}
 
+	pthread_rwlock_unlock(&ibqp->rwlock);
 	return err;
 }
 
@@ -3490,7 +3865,7 @@ int mlx5_post_srq_ops(struct ibv_srq *ibsrq, struct ibv_ops_wr *wr,
 
 			/* message is allowed to be empty */
 			if (wr->tm.add.num_sge && wr->tm.add.sg_list->length) {
-				set_data_ptr_seg(seg, wr->tm.add.sg_list, 0);
+				set_data_ptr_seg(ibsrq->context, seg, wr->tm.add.sg_list, 0);
 				tag->ptr = (void *)(uintptr_t)wr->tm.add.sg_list->addr;
 				tag->size = wr->tm.add.sg_list->length;
 			} else {
@@ -3542,7 +3917,9 @@ int mlx5_post_srq_ops(struct ibv_srq *ibsrq, struct ibv_ops_wr *wr,
 
 		ctrl->opmod_idx_opcode = htobe32(MLX5_OPCODE_TAG_MATCHING |
 				((qp->sq.cur_post & 0xffff) << 8));
-		ctrl->qpn_ds = htobe32(size | (srq->cmd_qp->qp_num << 8));
+		uint32_t real_qpn;
+		real_qpn = srq->cmd_qp->real_qpn;
+		ctrl->qpn_ds = htobe32(size | (real_qpn << 8));
 
 		if (unlikely(qp->wq_sig))
 			ctrl->signature = wq_sig(ctrl);
diff --git a/providers/mlx5/srq.c b/providers/mlx5/srq.c
index e9568c6..26890ef 100644
--- a/providers/mlx5/srq.c
+++ b/providers/mlx5/srq.c
@@ -171,10 +171,9 @@ void mlx5_complete_odp_fault(struct mlx5_srq *srq, int ind)
 	mlx5_spin_unlock(&srq->lock);
 }
 
-int mlx5_post_srq_recv(struct ibv_srq *ibsrq,
+int __mlx5_post_srq_recv__(struct ibv_srq *ibsrq,
 		       struct ibv_recv_wr *wr,
-		       struct ibv_recv_wr **bad_wr)
-{
+		       struct ibv_recv_wr **bad_wr) {
 	struct mlx5_srq *srq = to_msrq(ibsrq);
 	struct mlx5_wqe_srq_next_seg *next;
 	struct mlx5_wqe_data_seg *scat;
@@ -182,9 +181,9 @@ int mlx5_post_srq_recv(struct ibv_srq *ibsrq,
 	int nreq;
 	int i;
 
-	mlx5_spin_lock(&srq->lock);
-
 	for (nreq = 0; wr; ++nreq, wr = wr->next) {
+		int cur_idx;
+
 		if (wr->num_sge > srq->max_gs) {
 			err = EINVAL;
 			*bad_wr = wr;
@@ -198,6 +197,7 @@ int mlx5_post_srq_recv(struct ibv_srq *ibsrq,
 			break;
 		}
 
+		cur_idx = srq->head;
 		srq->wrid[srq->head] = wr->wr_id;
 
 		next      = get_wqe(srq, srq->head);
@@ -205,8 +205,9 @@ int mlx5_post_srq_recv(struct ibv_srq *ibsrq,
 		scat      = (struct mlx5_wqe_data_seg *) (next + 1);
 
 		for (i = 0; i < wr->num_sge; ++i) {
+			uint32_t *lkey_arr = ibsrq->context->lkey_mapping;
 			scat[i].byte_count = htobe32(wr->sg_list[i].length);
-			scat[i].lkey       = htobe32(wr->sg_list[i].lkey);
+			scat[i].lkey       = htobe32(lkey_arr[wr->sg_list[i].lkey]);
 			scat[i].addr       = htobe64(wr->sg_list[i].addr);
 		}
 
@@ -215,6 +216,17 @@ int mlx5_post_srq_recv(struct ibv_srq *ibsrq,
 			scat[i].lkey       = htobe32(MLX5_INVALID_LKEY);
 			scat[i].addr       = 0;
 		}
+
+		{
+			struct srq_recv_wr_item *cur_recv_wr =
+							srq->onflight_recv_wr + cur_idx;
+			cur_recv_wr->recv_wr.wr_id = wr->wr_id;
+			cur_recv_wr->recv_wr.next = NULL;
+			cur_recv_wr->recv_wr.num_sge = wr->num_sge;
+			memcpy(cur_recv_wr->recv_wr.sg_list, wr->sg_list,
+						sizeof(struct ibv_sge) * wr->num_sge);
+			list_add_tail(&srq->staged_onflight_list, &cur_recv_wr->list);
+	}
 	}
 
 	if (nreq) {
@@ -229,6 +241,71 @@ int mlx5_post_srq_recv(struct ibv_srq *ibsrq,
 		*srq->db = htobe32(srq->counter);
 	}
 
+	srq->n_posted += nreq;
+
+	return err;
+}
+
+int mlx5_post_srq_recv(struct ibv_srq *ibsrq,
+		       struct ibv_recv_wr *wr,
+		       struct ibv_recv_wr **bad_wr)
+{
+	struct mlx5_srq *srq = to_msrq(ibsrq);
+	int err;
+
+	mlx5_spin_lock(&srq->lock);
+	if(srq->migr_flag) {
+		struct mlx5_buf tmp_buf;
+		__be32 *tmp_db;
+
+		srq->migr_flag = 0;
+
+		memcpy(&tmp_buf, &srq->buf, sizeof(tmp_buf));
+		memcpy(&srq->buf, &srq->migr_buf, sizeof(tmp_buf));
+		memcpy(&srq->migr_buf, &tmp_buf, sizeof(tmp_buf));
+
+		tmp_db = srq->db;
+		srq->db = srq->migr_db;
+		srq->migr_db = tmp_db;
+
+		srq->head = srq->rollback_head;
+		srq->tail = srq->rollback_tail;
+		srq->counter = srq->rollback_counter;
+
+		/* Commit */
+		for(; srq->staged_tail < srq->staged_head; srq->staged_tail++) {
+			unsigned cur = srq->staged_tail;
+			struct mlx5_wqe_srq_next_seg *next;
+
+			/* void mlx5_free_srq_wqe() without spinlock on srq */
+			next = get_wqe(srq, srq->tail);
+			next->next_wqe_index = htobe16(srq->staged_index[cur % srq->onflight_cap]);
+			srq->tail = srq->staged_index[cur % srq->onflight_cap];
+		}
+
+		list_head_init(&srq->staged_onflight_list);
+	}
+
+	err = __mlx5_post_srq_recv__(ibsrq, wr, bad_wr);
+	{
+		struct list_head *staged_head = (struct list_head *)srq->staged_onflight_list.n.next;
+		struct list_head *staged_tail = (struct list_head *)srq->staged_onflight_list.n.prev;
+		struct list_head *commit_head = (struct list_head *)srq->onflight_list.n.next;
+		struct list_head *commit_tail = (struct list_head *)srq->onflight_list.n.prev;
+
+		commit_tail->n.next = &staged_head->n;
+		staged_head->n.prev = &commit_tail->n;
+		staged_tail->n.next = &srq->onflight_list.n;
+		srq->onflight_list.n.prev = &staged_tail->n;
+
+		list_head_init(&srq->staged_onflight_list);
+	}
+
+	if(srq->migr_flag) {
+		mlx5_spin_unlock(&srq->lock);
+		return mlx5_post_srq_recv(ibsrq, wr, bad_wr);
+	}
+
 	mlx5_spin_unlock(&srq->lock);
 
 	return err;
@@ -246,6 +323,9 @@ static void set_srq_buf_ll(struct mlx5_srq *srq, int start, int end)
 	for (i = start; i < end; ++i) {
 		next = get_wqe(srq, i);
 		next->next_wqe_index = htobe16(i + 1);
+
+		next = srq->migr_buf.buf + (i << srq->wqe_shift);
+		next->next_wqe_index = htobe16(i + 1);
 	}
 }
 
@@ -313,6 +393,13 @@ int mlx5_alloc_srq_buf(struct ibv_context *context, struct mlx5_srq *srq,
 				    MLX5_SRQ_PREFIX))
 		return -1;
 
+	if (mlx5_alloc_prefered_buf(ctx,
+				    &srq->migr_buf, buf_size,
+				    to_mdev(context->device)->page_size,
+				    alloc_type,
+				    MLX5_SRQ_PREFIX))
+		return -1;
+
 	if (srq->buf.type != MLX5_ALLOC_TYPE_CUSTOM)
 		memset(srq->buf.buf, 0, buf_size);
 
@@ -329,6 +416,15 @@ int mlx5_alloc_srq_buf(struct ibv_context *context, struct mlx5_srq *srq,
 	srq->wrid = malloc(srq->max * sizeof(*srq->wrid));
 	if (!srq->wrid) {
 		mlx5_free_actual_buf(ctx, &srq->buf);
+		mlx5_free_actual_buf(ctx, &srq->migr_buf);
+		return -1;
+	}
+
+	srq->tmp_wrid = malloc(srq->max * sizeof(*srq->wrid));
+	if(!srq->tmp_wrid) {
+		free(srq->wrid);
+		mlx5_free_actual_buf(ctx, &srq->buf);
+		mlx5_free_actual_buf(ctx, &srq->migr_buf);
 		return -1;
 	}
 
diff --git a/providers/mlx5/verbs.c b/providers/mlx5/verbs.c
index b701d82..2f2ccd8 100644
--- a/providers/mlx5/verbs.c
+++ b/providers/mlx5/verbs.c
@@ -1039,9 +1039,24 @@ static struct ibv_cq_ex *create_cq(struct ibv_context *context,
 		goto err_spl;
 	}
 
+	if (mlx5_alloc_cq_buf(to_mctx(context), cq, &cq->migr_buf, ncqe, cqe_sz)) {
+		mlx5_dbg(fp, MLX5_DBG_CQ, "\n");
+		goto err_spl;
+	}
+
+	if(mlx5_alloc_cq_buf(to_mctx(context), cq, &cq->tmp_buf, ncqe, cqe_sz)) {
+		mlx5_dbg(fp, MLX5_DBG_CQ, "\n");
+		goto err_buf;
+	}
+
+	cq->tmp_cons_index = 0;
+	cq->shaded_flag = 0;
+
 	cq->dbrec  = mlx5_alloc_dbrec(to_mctx(context), cq->parent_domain,
 				      &cq->custom_db);
-	if (!cq->dbrec) {
+	cq->migr_dbrec = mlx5_alloc_dbrec(to_mctx(context), cq->parent_domain,
+				      &cq->custom_db);
+	if (!cq->dbrec || !cq->migr_dbrec) {
 		mlx5_dbg(fp, MLX5_DBG_CQ, "\n");
 		goto err_buf;
 	}
@@ -1126,9 +1141,15 @@ static struct ibv_cq_ex *create_cq(struct ibv_context *context,
 	return &cq->verbs_cq.cq_ex;
 
 err_db:
+	if(!cq->migr_dbrec)
+		mlx5_free_db(to_mctx(context), cq->migr_dbrec, cq->parent_domain, cq->custom_db);
 	mlx5_free_db(to_mctx(context), cq->dbrec, cq->parent_domain, cq->custom_db);
 
 err_buf:
+	if(cq->tmp_buf.buf)
+		mlx5_free_cq_buf(to_mctx(context), &cq->tmp_buf);
+	if(cq->migr_buf.buf)
+		mlx5_free_cq_buf(to_mctx(context), &cq->migr_buf);
 	mlx5_free_cq_buf(to_mctx(context), &cq->buf_a);
 
 err_spl:
@@ -1140,1036 +1161,2465 @@ err:
 	return NULL;
 }
 
-struct ibv_cq *mlx5_create_cq(struct ibv_context *context, int cqe,
-			      struct ibv_comp_channel *channel,
-			      int comp_vector)
+static struct ibv_cq_ex *resume_cq(struct ibv_context *context,
+				   const struct ibv_cq_init_attr_ex *cq_attr,
+				   int cq_alloc_flags,
+				   struct mlx5dv_cq_init_attr *mlx5cq_attr,
+				   void *buf_addr, void *db_addr)
 {
-	struct ibv_cq_ex *cq;
-	struct ibv_cq_init_attr_ex cq_attr = {.cqe = cqe, .channel = channel,
-						.comp_vector = comp_vector,
-						.wc_flags = IBV_WC_STANDARD_FLAGS};
+	struct mlx5_create_cq_ex	cmd_ex = {};
+	struct mlx5_create_cq_ex_resp	resp_ex = {};
+	struct mlx5_ib_create_cq       *cmd_drv;
+	struct mlx5_ib_create_cq_resp  *resp_drv;
+	struct mlx5_cq		       *cq;
+	int				cqe_sz;
+	int				ret;
+	int				ncqe;
+	int				rc;
+	struct mlx5_context *mctx = to_mctx(context);
+	FILE *fp = to_mctx(context)->dbg_fp;
 
-	if (cqe <= 0) {
+	if (!cq_attr->cqe) {
+		mlx5_dbg(fp, MLX5_DBG_CQ, "CQE invalid\n");
 		errno = EINVAL;
 		return NULL;
 	}
 
-	cq = create_cq(context, &cq_attr, 0, NULL);
-	return cq ? ibv_cq_ex_to_cq(cq) : NULL;
-}
-
-struct ibv_cq_ex *mlx5_create_cq_ex(struct ibv_context *context,
-				    struct ibv_cq_init_attr_ex *cq_attr)
-{
-	return create_cq(context, cq_attr, MLX5_CQ_FLAGS_EXTENDED, NULL);
-}
-
-struct ibv_cq_ex *mlx5dv_create_cq(struct ibv_context *context,
-				      struct ibv_cq_init_attr_ex *cq_attr,
-				      struct mlx5dv_cq_init_attr *mlx5_cq_attr)
-{
-	struct ibv_cq_ex *cq;
-
-	if (!is_mlx5_dev(context->device)) {
-		errno = EOPNOTSUPP;
+	if (cq_attr->comp_mask & ~CREATE_CQ_SUPPORTED_COMP_MASK) {
+		mlx5_dbg(fp, MLX5_DBG_CQ,
+			 "Unsupported comp_mask for create_cq\n");
+		errno = EINVAL;
 		return NULL;
 	}
 
-	cq = create_cq(context, cq_attr, MLX5_CQ_FLAGS_EXTENDED, mlx5_cq_attr);
-	if (!cq)
+	if (cq_attr->comp_mask & IBV_CQ_INIT_ATTR_MASK_FLAGS &&
+	    cq_attr->flags & ~CREATE_CQ_SUPPORTED_FLAGS) {
+		mlx5_dbg(fp, MLX5_DBG_CQ,
+			 "Unsupported creation flags requested for create_cq\n");
+		errno = EINVAL;
 		return NULL;
+	}
 
-	verbs_init_cq(ibv_cq_ex_to_cq(cq), context,
-		      cq_attr->channel, cq_attr->cq_context);
-	return cq;
-}
-
-int mlx5_resize_cq(struct ibv_cq *ibcq, int cqe)
-{
-	struct mlx5_cq *cq = to_mcq(ibcq);
-	struct mlx5_resize_cq_resp resp;
-	struct mlx5_resize_cq cmd;
-	struct mlx5_context *mctx = to_mctx(ibcq->context);
-	int err;
+	if (cq_attr->wc_flags & ~CREATE_CQ_SUPPORTED_WC_FLAGS) {
+		mlx5_dbg(fp, MLX5_DBG_CQ, "\n");
+		errno = ENOTSUP;
+		return NULL;
+	}
 
-	if (cqe < 0) {
+	if (mlx5cq_attr &&
+	    !check_comp_mask(mlx5cq_attr->comp_mask,
+			     MLX5_DV_CREATE_CQ_SUP_COMP_MASK)) {
+		mlx5_dbg(fp, MLX5_DBG_CQ,
+			 "unsupported vendor comp_mask for %s\n", __func__);
 		errno = EINVAL;
-		return errno;
+		return NULL;
 	}
 
-	memset(&cmd, 0, sizeof(cmd));
-	memset(&resp, 0, sizeof(resp));
-
-	if (((long long)cqe * 64) > INT_MAX)
-		return EINVAL;
-
-	mlx5_spin_lock(&cq->lock);
-	cq->active_cqes = cq->verbs_cq.cq.cqe;
-	if (cq->active_buf == &cq->buf_a)
-		cq->resize_buf = &cq->buf_b;
-	else
-		cq->resize_buf = &cq->buf_a;
-
-	cqe = align_queue_size(cqe + 1);
-	if (cqe == ibcq->cqe + 1) {
-		cq->resize_buf = NULL;
-		err = 0;
-		goto out;
+	cq =  calloc(1, sizeof *cq);
+	if (!cq) {
+		mlx5_dbg(fp, MLX5_DBG_CQ, "\n");
+		return NULL;
 	}
 
-	/* currently we don't change cqe size */
-	cq->resize_cqe_sz = cq->cqe_sz;
-	cq->resize_cqes = cqe;
-	err = mlx5_alloc_cq_buf(mctx, cq, cq->resize_buf, cq->resize_cqes, cq->resize_cqe_sz);
-	if (err) {
-		cq->resize_buf = NULL;
-		errno = ENOMEM;
-		goto out;
+	if (cq_attr->comp_mask & IBV_CQ_INIT_ATTR_MASK_FLAGS) {
+		if (cq_attr->flags & IBV_CREATE_CQ_ATTR_SINGLE_THREADED)
+			cq->flags |= MLX5_CQ_FLAGS_SINGLE_THREADED;
 	}
 
-	cmd.buf_addr = (uintptr_t)cq->resize_buf->buf;
-	cmd.cqe_size = cq->resize_cqe_sz;
-
-	err = ibv_cmd_resize_cq(ibcq, cqe - 1, &cmd.ibv_cmd, sizeof(cmd),
-				&resp.ibv_resp, sizeof(resp));
-	if (err)
-		goto out_buf;
-
-	mlx5_cq_resize_copy_cqes(mctx, cq);
-	mlx5_free_cq_buf(mctx, cq->active_buf);
-	cq->active_buf = cq->resize_buf;
-	cq->verbs_cq.cq.cqe = cqe - 1;
-	mlx5_spin_unlock(&cq->lock);
-	cq->resize_buf = NULL;
-	return 0;
-
-out_buf:
-	mlx5_free_cq_buf(mctx, cq->resize_buf);
-	cq->resize_buf = NULL;
-
-out:
-	mlx5_spin_unlock(&cq->lock);
-	return err;
-}
-
-int mlx5_destroy_cq(struct ibv_cq *cq)
-{
-	int ret;
-	struct mlx5_cq *mcq = to_mcq(cq);
-
-	ret = ibv_cmd_destroy_cq(cq);
-	if (ret)
-		return ret;
-
-	mlx5_free_db(to_mctx(cq->context), mcq->dbrec, mcq->parent_domain,
-		     mcq->custom_db);
-	mlx5_free_cq_buf(to_mctx(cq->context), mcq->active_buf);
-	if (mcq->parent_domain)
-		atomic_fetch_sub(&to_mparent_domain(mcq->parent_domain)->mpd.refcount, 1);
-	free(mcq);
-
-	return 0;
-}
-
-struct ibv_srq *mlx5_create_srq(struct ibv_pd *pd,
-				struct ibv_srq_init_attr *attr)
-{
-	struct mlx5_create_srq      cmd;
-	struct mlx5_create_srq_resp resp;
-	struct mlx5_srq		   *srq;
-	int			    ret;
-	struct mlx5_context	   *ctx;
-	int			    max_sge;
-	struct ibv_srq		   *ibsrq;
-
-	ctx = to_mctx(pd->context);
-	srq = calloc(1, sizeof *srq);
-	if (!srq) {
-		mlx5_err(ctx->dbg_fp, "%s-%d:\n", __func__, __LINE__);
-		return NULL;
+	if (cq_attr->comp_mask & IBV_CQ_INIT_ATTR_MASK_PD) {
+		if (!(to_mparent_domain(cq_attr->parent_domain))) {
+			errno = EINVAL;
+			goto err;
+		}
+		cq->parent_domain = cq_attr->parent_domain;
 	}
-	ibsrq = &srq->vsrq.srq;
 
-	memset(&cmd, 0, sizeof cmd);
-	if (mlx5_spinlock_init_pd(&srq->lock, pd)) {
-		mlx5_err(ctx->dbg_fp, "%s-%d:\n", __func__, __LINE__);
-		goto err;
+	if (cq_alloc_flags & MLX5_CQ_FLAGS_EXTENDED) {
+		rc = mlx5_cq_fill_pfns(cq, cq_attr, mctx);
+		if (rc) {
+			errno = rc;
+			goto err;
+		}
 	}
 
-	if (attr->attr.max_wr > ctx->max_srq_recv_wr) {
-		mlx5_err(ctx->dbg_fp, "%s-%d:max_wr %d, max_srq_recv_wr %d\n", __func__, __LINE__,
-			 attr->attr.max_wr, ctx->max_srq_recv_wr);
-		errno = EINVAL;
+	cmd_drv = &cmd_ex.drv_payload;
+	resp_drv = &resp_ex.drv_payload;
+	cq->cons_index = 0;
+
+	if (mlx5_spinlock_init(&cq->lock, !mlx5_single_threaded))
 		goto err;
-	}
 
-	/*
-	 * this calculation does not consider required control segments. The
-	 * final calculation is done again later. This is done so to avoid
-	 * overflows of variables
-	 */
-	max_sge = ctx->max_rq_desc_sz / sizeof(struct mlx5_wqe_data_seg);
-	if (attr->attr.max_sge > max_sge) {
-		mlx5_err(ctx->dbg_fp, "%s-%d:max_wr %d, max_srq_recv_wr %d\n", __func__, __LINE__,
-			attr->attr.max_wr, ctx->max_srq_recv_wr);
+	ncqe = align_queue_size(cq_attr->cqe + 1);
+	if ((ncqe > (1 << 24)) || (ncqe < (cq_attr->cqe + 1))) {
+		mlx5_dbg(fp, MLX5_DBG_CQ, "ncqe %d\n", ncqe);
 		errno = EINVAL;
-		goto err;
+		goto err_spl;
 	}
 
-	srq->max_gs  = attr->attr.max_sge;
-	srq->counter = 0;
+	cqe_sz = get_cqe_size(mlx5cq_attr);
+	if (cqe_sz < 0) {
+		mlx5_dbg(fp, MLX5_DBG_CQ, "\n");
+		errno = -cqe_sz;
+		goto err_spl;
+	}
 
-	if (mlx5_alloc_srq_buf(pd->context, srq, attr->attr.max_wr, pd)) {
-		mlx5_err(ctx->dbg_fp, "%s-%d:\n", __func__, __LINE__);
-		goto err;
+	if (mlx5_alloc_cq_buf(to_mctx(context), cq, &cq->buf_a, ncqe, cqe_sz)) {
+		mlx5_dbg(fp, MLX5_DBG_CQ, "\n");
+		goto err_spl;
 	}
 
-	srq->db = mlx5_alloc_dbrec(to_mctx(pd->context), pd, &srq->custom_db);
-	if (!srq->db) {
-		mlx5_err(ctx->dbg_fp, "%s-%d:\n", __func__, __LINE__);
-		goto err_free;
+	cq->dbrec  = mlx5_alloc_dbrec(to_mctx(context), cq->parent_domain,
+				      &cq->custom_db);
+	if (!cq->dbrec) {
+		mlx5_dbg(fp, MLX5_DBG_CQ, "\n");
+		goto err_buf;
 	}
 
-	if (!srq->custom_db)
-		*srq->db = 0;
+	cq->dbrec[MLX5_CQ_SET_CI]	= 0;
+	cq->dbrec[MLX5_CQ_ARM_DB]	= 0;
+	cq->arm_sn			= 0;
+	cq->cqe_sz			= cqe_sz;
+	cq->flags			= cq_alloc_flags;
 
-	cmd.buf_addr = (uintptr_t) srq->buf.buf;
-	cmd.db_addr  = (uintptr_t) srq->db;
-	srq->wq_sig = srq_sig_enabled();
-	if (srq->wq_sig)
-		cmd.flags = MLX5_SRQ_FLAG_SIGNATURE;
+	cmd_drv->buf_addr = (uintptr_t) buf_addr;
+	cmd_drv->db_addr  = (uintptr_t) db_addr;
+	cmd_drv->cqe_size = cqe_sz;
 
-	attr->attr.max_sge = srq->max_gs;
-	pthread_mutex_lock(&ctx->srq_table_mutex);
+	if (mlx5cq_attr) {
+		if (mlx5cq_attr->comp_mask & MLX5DV_CQ_INIT_ATTR_MASK_COMPRESSED_CQE) {
+			if (mctx->cqe_comp_caps.max_num &&
+			    (mlx5cq_attr->cqe_comp_res_format &
+			     mctx->cqe_comp_caps.supported_format)) {
+				cmd_drv->cqe_comp_en = 1;
+				cmd_drv->cqe_comp_res_format = mlx5cq_attr->cqe_comp_res_format;
+			} else {
+				mlx5_dbg(fp, MLX5_DBG_CQ, "CQE Compression is not supported\n");
+				errno = EINVAL;
+				goto err_db;
+			}
+		}
 
-	/* Override max_wr to let kernel know about extra WQEs for the
-	 * wait queue.
-	 */
-	attr->attr.max_wr = srq->max - 1;
+		if (mlx5cq_attr->comp_mask & MLX5DV_CQ_INIT_ATTR_MASK_FLAGS) {
+			if (mlx5cq_attr->flags & ~(MLX5DV_CQ_INIT_ATTR_FLAGS_RESERVED - 1)) {
+				mlx5_dbg(fp, MLX5_DBG_CQ,
+					 "Unsupported vendor flags for create_cq\n");
+				errno = EINVAL;
+				goto err_db;
+			}
 
-	ret = ibv_cmd_create_srq(pd, ibsrq, attr, &cmd.ibv_cmd, sizeof(cmd),
-				 &resp.ibv_resp, sizeof(resp));
-	if (ret)
-		goto err_db;
+			if (mlx5cq_attr->flags & MLX5DV_CQ_INIT_ATTR_FLAGS_CQE_PAD) {
+				if (!(mctx->vendor_cap_flags &
+				      MLX5_VENDOR_CAP_FLAGS_CQE_128B_PAD) ||
+				    (cqe_sz != 128)) {
+					mlx5_dbg(fp, MLX5_DBG_CQ,
+						 "%dB CQE paddind is not supported\n",
+						 cqe_sz);
+					errno = EINVAL;
+					goto err_db;
+				}
 
-	/* Override kernel response that includes the wait queue with the real
-	 * number of WQEs that are applicable for the application.
-	 */
-	attr->attr.max_wr = srq->tail;
+				cmd_drv->flags |= MLX5_IB_CREATE_CQ_FLAGS_CQE_128B_PAD;
+			}
+		}
+	}
 
-	ret = mlx5_store_srq(ctx, resp.srqn, srq);
-	if (ret)
-		goto err_destroy;
+	if (mctx->nc_uar) {
+		cmd_drv->flags |= MLX5_IB_CREATE_CQ_FLAGS_UAR_PAGE_INDEX;
+		cmd_drv->uar_page_index = mctx->nc_uar->page_id;
+	}
 
-	pthread_mutex_unlock(&ctx->srq_table_mutex);
+	{
+		struct ibv_cq_init_attr_ex cq_attr_ex = *cq_attr;
 
-	srq->srqn = resp.srqn;
-	srq->rsc.rsn = resp.srqn;
-	srq->rsc.type = MLX5_RSC_TYPE_SRQ;
+		cq_attr_ex.cqe = ncqe - 1;
+		ret = ibv_cmd_create_cq_ex(context, &cq_attr_ex, &cq->verbs_cq,
+					   &cmd_ex.ibv_cmd, sizeof(cmd_ex),
+					   &resp_ex.ibv_resp, sizeof(resp_ex),
+					   CREATE_CQ_CMD_FLAGS_TS_IGNORED_EX);
+	}
 
-	return ibsrq;
+	if (ret) {
+		mlx5_dbg(fp, MLX5_DBG_CQ, "ret %d\n", ret);
+		goto err_db;
+	}
 
-err_destroy:
-	ibv_cmd_destroy_srq(ibsrq);
+	if (cq->parent_domain)
+		atomic_fetch_add(&to_mparent_domain(cq->parent_domain)->mpd.refcount, 1);
+	cq->active_buf = &cq->buf_a;
+	cq->resize_buf = NULL;
+	cq->cqn = resp_drv->cqn;
+	cq->stall_enable = to_mctx(context)->stall_enable;
+	cq->stall_adaptive_enable = to_mctx(context)->stall_adaptive_enable;
+	cq->stall_cycles = to_mctx(context)->stall_cycles;
+
+	mlx5_free_db(to_mctx(ibv_cq_ex_to_cq(cq)->context), cq->dbrec,
+			 cq->parent_domain, cq->custom_db);
+	mlx5_free_cq_buf(to_mctx(ibv_cq_ex_to_cq(cq)->context), cq->active_buf);
+
+	cq->dbrec = db_addr;
+	cq->active_buf->buf = buf_addr;
+
+	return &cq->verbs_cq.cq_ex;
 
 err_db:
-	pthread_mutex_unlock(&ctx->srq_table_mutex);
-	mlx5_free_db(to_mctx(pd->context), srq->db, pd, srq->custom_db);
+	mlx5_free_db(to_mctx(context), cq->dbrec, cq->parent_domain, cq->custom_db);
 
-err_free:
-	free(srq->wrid);
-	mlx5_free_actual_buf(ctx, &srq->buf);
+err_buf:
+	mlx5_free_cq_buf(to_mctx(context), &cq->buf_a);
+
+err_spl:
+	mlx5_spinlock_destroy(&cq->lock);
 
 err:
-	free(srq);
+	free(cq);
 
 	return NULL;
 }
 
-int mlx5_modify_srq(struct ibv_srq *srq,
-		    struct ibv_srq_attr *attr,
-		    int attr_mask)
+struct ibv_cq *mlx5_create_cq(struct ibv_context *context, int cqe,
+			      struct ibv_comp_channel *channel,
+			      int comp_vector)
 {
-	struct ibv_modify_srq cmd;
+	struct ibv_cq_ex *cq;
+	struct mlx5_cq *mcq;
+	struct ibv_cq_init_attr_ex cq_attr = {.cqe = cqe, .channel = channel,
+						.comp_vector = comp_vector,
+						.wc_flags = IBV_WC_STANDARD_FLAGS};
+	char fname[128];
+	int info_fd;
 
-	return ibv_cmd_modify_srq(srq, attr, attr_mask, &cmd, sizeof cmd);
-}
+	if (cqe <= 0) {
+		errno = EINVAL;
+		return NULL;
+	}
 
-int mlx5_query_srq(struct ibv_srq *srq,
-		    struct ibv_srq_attr *attr)
-{
-	struct ibv_query_srq cmd;
+	cq = create_cq(context, &cq_attr, 0, NULL);
+	if(!cq) {
+		return NULL;
+	}
 
-	return ibv_cmd_query_srq(srq, attr, &cmd, sizeof cmd);
-}
+	mcq = to_mcq(ibv_cq_ex_to_cq(cq));
+	mcq->use_fake_index = 0;
+	pthread_rwlock_init(&mcq->fake_index_rwlock, NULL);
 
-int mlx5_destroy_srq(struct ibv_srq *srq)
-{
-	int ret;
-	struct mlx5_srq *msrq = to_msrq(srq);
-	struct mlx5_context *ctx = to_mctx(srq->context);
+	if(ibv_cmd_install_cq_handle_mapping(ibv_cq_ex_to_cq(cq)->context,
+					ibv_cq_ex_to_cq(cq)->handle, ibv_cq_ex_to_cq(cq)->handle)) {
+		ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+		return NULL;
+    }
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/cq_%d/buf_addr",
+							rdma_getpid(ibv_cq_ex_to_cq(cq)->context),
+							ibv_cq_ex_to_cq(cq)->context->cmd_fd,
+							ibv_cq_ex_to_cq(cq)->handle);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+		return NULL;
+	}
 
-	if (msrq->cmd_qp) {
-		ret = mlx5_destroy_qp(msrq->cmd_qp);
-		if (ret)
-			return ret;
-		msrq->cmd_qp = NULL;
+	if(write(info_fd, &mcq->migr_buf.buf, sizeof(void *)) < 0) {
+		close(info_fd);
+		ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+		return NULL;
 	}
 
-	ret = ibv_cmd_destroy_srq(srq);
-	if (ret)
-		return ret;
+	close(info_fd);
 
-	if (ctx->cqe_version && msrq->rsc.type == MLX5_RSC_TYPE_XSRQ)
-		mlx5_clear_uidx(ctx, msrq->rsc.rsn);
-	else
-		mlx5_clear_srq(ctx, msrq->srqn);
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/cq_%d/db_addr",
+							rdma_getpid(ibv_cq_ex_to_cq(cq)->context),
+							ibv_cq_ex_to_cq(cq)->context->cmd_fd,
+							ibv_cq_ex_to_cq(cq)->handle);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+		return NULL;
+	}
 
-	mlx5_free_db(ctx, msrq->db, srq->pd, msrq->custom_db);
-	mlx5_free_actual_buf(ctx, &msrq->buf);
-	free(msrq->tm_list);
-	free(msrq->wrid);
-	free(msrq->op);
-	free(msrq);
+	if(write(info_fd, &mcq->migr_dbrec, sizeof(void *)) < 0) {
+		close(info_fd);
+		ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+		return NULL;
+	}
 
-	return 0;
+	close(info_fd);
+
+	return ibv_cq_ex_to_cq(cq);
 }
 
-static int _sq_overhead(struct mlx5_qp *qp,
-			enum ibv_qp_type qp_type,
-			uint64_t ops,
-			uint64_t mlx5_ops)
+struct ibv_cq *mlx5_resume_cq(struct ibv_context *context, struct ibv_cq *cq_meta,
+			int cqe, struct ibv_comp_channel *channel, int comp_vector,
+			void *buf_addr, void *db_addr, int vhandle)
 {
-	size_t size = sizeof(struct mlx5_wqe_ctrl_seg);
-	size_t rdma_size = 0;
-	size_t atomic_size = 0;
-	size_t mw_size = 0;
+	struct ibv_cq_ex *cq;
+	struct mlx5_cq *mcq;
+	struct mlx5_cq *mcq_meta = to_mcq(cq_meta);
+	struct mlx5_device *dev = to_mdev(context->device);
+	struct ibv_cq_init_attr_ex cq_attr = {.cqe = cqe, .channel = channel,
+						.comp_vector = comp_vector,
+						.wc_flags = IBV_WC_STANDARD_FLAGS};
+	uint32_t i;
+	int err;
+	char fname[128];
+	int info_fd;
 
-	/* Operation overhead */
-	if (ops & (IBV_QP_EX_WITH_RDMA_WRITE |
-		   IBV_QP_EX_WITH_RDMA_WRITE_WITH_IMM |
-		   IBV_QP_EX_WITH_RDMA_READ))
-		rdma_size = sizeof(struct mlx5_wqe_ctrl_seg) +
-			    sizeof(struct mlx5_wqe_raddr_seg);
+	if (cqe <= 0) {
+		errno = EINVAL;
+		return NULL;
+	}
 
-	if (ops & (IBV_QP_EX_WITH_ATOMIC_CMP_AND_SWP |
-		   IBV_QP_EX_WITH_ATOMIC_FETCH_AND_ADD))
-		atomic_size = sizeof(struct mlx5_wqe_ctrl_seg) +
-			      sizeof(struct mlx5_wqe_raddr_seg) +
-			      sizeof(struct mlx5_wqe_atomic_seg);
+	cq = resume_cq(context, &cq_attr, 0, NULL, buf_addr, db_addr);
+	if(!cq)
+		return NULL;
 
-	if (ops & (IBV_QP_EX_WITH_BIND_MW | IBV_QP_EX_WITH_LOCAL_INV) ||
-	    (mlx5_ops & (MLX5DV_QP_EX_WITH_MR_INTERLEAVED |
-			 MLX5DV_QP_EX_WITH_MR_LIST)))
-		mw_size = sizeof(struct mlx5_wqe_ctrl_seg) +
-			  sizeof(struct mlx5_wqe_umr_ctrl_seg) +
-			  sizeof(struct mlx5_wqe_mkey_context_seg) +
-			  max_t(size_t, sizeof(struct mlx5_wqe_umr_klm_seg), 64);
+	if(ibv_cmd_install_cq_handle_mapping(context, vhandle, cq->handle)) {
+		ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+		return NULL;
+	}
 
-	size = max_t(size_t, size, rdma_size);
-	size = max_t(size_t, size, atomic_size);
-	size = max_t(size_t, size, mw_size);
+	mcq = to_mcq(ibv_cq_ex_to_cq(cq));
 
-	/* Transport overhead */
-	switch (qp_type) {
-	case IBV_QPT_DRIVER:
-		if (qp->dc_type != MLX5DV_DCTYPE_DCI)
-			return -EINVAL;
-		SWITCH_FALLTHROUGH;
+	mcq->cons_index = 0;
+	mcq->dbrec[MLX5_CQ_SET_CI] = htobe32(0 & 0xffffff);
 
-	case IBV_QPT_UD:
-		size += sizeof(struct mlx5_wqe_datagram_seg);
-		if (qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY)
-			size += sizeof(struct mlx5_wqe_eth_seg) +
-				sizeof(struct mlx5_wqe_eth_pad);
-		break;
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/cq_%d/buf_addr",
+							rdma_getpid(ibv_cq_ex_to_cq(cq)->context),
+							ibv_cq_ex_to_cq(cq)->context->cmd_fd,
+							vhandle);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+		return NULL;
+	}
 
-	case IBV_QPT_XRC_RECV:
-	case IBV_QPT_XRC_SEND:
-		size += sizeof(struct mlx5_wqe_xrc_seg);
-		break;
+	if(write(info_fd, &mcq_meta->active_buf->buf, sizeof(void *)) < 0) {
+		close(info_fd);
+		ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+		return NULL;
+	}
 
-	case IBV_QPT_RAW_PACKET:
-		size += sizeof(struct mlx5_wqe_eth_seg);
-		break;
+	close(info_fd);
 
-	case IBV_QPT_RC:
-	case IBV_QPT_UC:
-		break;
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/cq_%d/db_addr",
+							rdma_getpid(ibv_cq_ex_to_cq(cq)->context),
+							ibv_cq_ex_to_cq(cq)->context->cmd_fd,
+							vhandle);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+		return NULL;
+	}
 
-	default:
-		return -EINVAL;
+	if(write(info_fd, &mcq_meta->dbrec, sizeof(void *)) < 0) {
+		close(info_fd);
+		ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+		return NULL;
 	}
 
-	return size;
-}
+	close(info_fd);
 
-static int sq_overhead(struct mlx5_qp *qp, struct ibv_qp_init_attr_ex *attr,
-		       struct mlx5dv_qp_init_attr *mlx5_qp_attr)
-{
-	uint64_t ops;
-	uint64_t mlx5_ops = 0;
+	{
+		typeof(mcq->cqn) *content_p;
 
-	if (attr->comp_mask & IBV_QP_INIT_ATTR_SEND_OPS_FLAGS) {
-		ops = attr->send_ops_flags;
-	} else {
-		switch (attr->qp_type) {
-		case IBV_QPT_RC:
-		case IBV_QPT_UC:
-		case IBV_QPT_DRIVER:
-		case IBV_QPT_XRC_RECV:
-		case IBV_QPT_XRC_SEND:
-			ops = IBV_QP_EX_WITH_SEND |
-			      IBV_QP_EX_WITH_SEND_WITH_INV |
-			      IBV_QP_EX_WITH_SEND_WITH_IMM |
-			      IBV_QP_EX_WITH_RDMA_WRITE |
-			      IBV_QP_EX_WITH_RDMA_WRITE_WITH_IMM |
-			      IBV_QP_EX_WITH_RDMA_READ |
-			      IBV_QP_EX_WITH_ATOMIC_CMP_AND_SWP |
-			      IBV_QP_EX_WITH_ATOMIC_FETCH_AND_ADD |
-			      IBV_QP_EX_WITH_LOCAL_INV |
-			      IBV_QP_EX_WITH_BIND_MW;
-			break;
-
-		case IBV_QPT_UD:
-			ops = IBV_QP_EX_WITH_SEND |
-			      IBV_QP_EX_WITH_SEND_WITH_IMM |
-			      IBV_QP_EX_WITH_TSO;
-			break;
+		mcq_meta->cqn = mcq->cqn;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = mcq->cqn;
+		if(register_update_mem(&mcq_meta->cqn, sizeof(*content_p), content_p)) {
+			ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+			return NULL;
+		}
+	}
 
-		case IBV_QPT_RAW_PACKET:
-			ops = IBV_QP_EX_WITH_SEND |
-			      IBV_QP_EX_WITH_TSO;
-			break;
+	{
+		typeof(mcq_meta->migr_flag) *content_p;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = 1;
+		if(register_update_mem(&mcq_meta->migr_flag,
+					sizeof(*content_p), content_p)) {
+			ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+			return NULL;
+		}
+	}
 
-		default:
-			return -EINVAL;
+	{
+		typeof(mcq_meta->arm_sn) *content_p;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = 0;
+		if(register_update_mem(&mcq_meta->arm_sn,
+					sizeof(*content_p), content_p)) {
+			ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+			return NULL;
 		}
 	}
 
+	{
+		typeof(cq_meta->comp_events_completed) *content_p;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = 0;
+		if(register_update_mem(&cq_meta->comp_events_completed,
+						sizeof(*content_p), content_p)) {
+			ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+			return NULL;
+		}
+	}
 
-	if (mlx5_qp_attr &&
-	    mlx5_qp_attr->comp_mask & MLX5DV_QP_INIT_ATTR_MASK_SEND_OPS_FLAGS)
-		mlx5_ops = mlx5_qp_attr->send_ops_flags;
+	{
+		typeof(cq_meta->async_events_completed) *content_p;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = 0;
+		if(register_update_mem(&cq_meta->async_events_completed,
+						sizeof(*content_p), content_p)) {
+			ibv_destroy_cq(ibv_cq_ex_to_cq(cq));
+			return NULL;
+		}
+	}
 
-	return _sq_overhead(qp, attr->qp_type, ops, mlx5_ops);
+	return ibv_cq_ex_to_cq(cq);
 }
 
-static int mlx5_calc_send_wqe(struct mlx5_context *ctx,
-			      struct ibv_qp_init_attr_ex *attr,
-			      struct mlx5dv_qp_init_attr *mlx5_qp_attr,
-			      struct mlx5_qp *qp)
-{
-	int size;
-	int inl_size = 0;
-	int max_gather;
-	int tot_size;
+void mlx5_copy_cqe_to_shaded(struct ibv_cq *ibcq) {
+	struct mlx5_cq *cq = to_mcq(ibcq);
 
-	size = sq_overhead(qp, attr, mlx5_qp_attr);
-	if (size < 0)
-		return size;
+	if(cq->shaded_flag)
+		return;
 
-	if (attr->cap.max_inline_data) {
-		inl_size = size + align(sizeof(struct mlx5_wqe_inl_data_seg) +
-			attr->cap.max_inline_data, 16);
-	}
+	cq->shaded_flag = 1;
+	cq->tmp_cons_index = cq->cons_index;
+	memcpy(cq->tmp_buf.buf, cq->active_buf->buf,
+							(ibcq->cqe + 1) * cq->cqe_sz);
+}
 
-	if (attr->comp_mask & IBV_QP_INIT_ATTR_MAX_TSO_HEADER) {
-		size += align(attr->max_tso_header, 16);
-		qp->max_tso_header = attr->max_tso_header;
+int mlx5_uwrite_cq(struct ibv_cq *ibcq, int cq_dir_fd) {
+	struct mlx5_cq *cq = to_mcq(ibcq);
+	struct mlx5_cqe64 *cqe;
+	int i;
+
+	memset(cq->migr_buf.buf, 0, (ibcq->cqe + 1) * cq->cqe_sz);
+	for(i = 0; i < (ibcq->cqe + 1); i++) {
+		cqe = cq->migr_buf.buf + i * cq->cqe_sz;
+		cqe += cq->cqe_sz == 128? 1: 0;
+		cqe->op_own = MLX5_CQE_INVALID << 4;
 	}
 
-	max_gather = (ctx->max_sq_desc_sz - size) /
-		sizeof(struct mlx5_wqe_data_seg);
-	if (attr->cap.max_send_sge > max_gather)
-		return -EINVAL;
+	cq->migr_dbrec[MLX5_CQ_SET_CI] = 0;
+	cq->migr_dbrec[MLX5_CQ_ARM_DB] = 0;
 
-	size += attr->cap.max_send_sge * sizeof(struct mlx5_wqe_data_seg);
-	tot_size = max_int(size, inl_size);
+	return 0;
+}
 
-	if (tot_size > ctx->max_sq_desc_sz)
-		return -EINVAL;
+void mlx5_record_qp_index(struct ibv_qp *qp) {
+	struct mlx5_qp *mqp = to_mqp(qp);
 
-	return align(tot_size, MLX5_SEND_WQE_BB);
+	memcpy(&mqp->rollback_rq, &mqp->rq, sizeof(struct mlx5_wq));
+	memcpy(&mqp->rollback_sq, &mqp->sq, sizeof(struct mlx5_wq));
 }
 
-static int mlx5_calc_rcv_wqe(struct mlx5_context *ctx,
-			     struct ibv_qp_init_attr_ex *attr,
-			     struct mlx5_qp *qp)
-{
-	uint32_t size;
-	int num_scatter;
+int mlx5_uwrite_qp(struct ibv_qp *ibqp, struct ibv_qp *new_qp) {
+	struct mlx5_qp *qp = to_mqp(ibqp);
 
-	if (attr->srq)
-		return 0;
+	qp->migr_db[MLX5_RCV_DBR] = 0;
+	qp->migr_db[MLX5_SND_DBR] = 0;
 
-	num_scatter = max_t(uint32_t, attr->cap.max_recv_sge, 1);
-	size = sizeof(struct mlx5_wqe_data_seg) * num_scatter;
-	if (qp->wq_sig)
-		size += sizeof(struct mlx5_rwqe_sig);
+	return 0;
+}
 
-	if (size > ctx->max_rq_desc_sz)
-		return -EINVAL;
+int mlx5_uwrite_srq(struct ibv_srq *ibsrq, struct ibv_srq *new_ibsrq) {
+	struct mlx5_srq *srq = to_msrq(ibsrq);
 
-	size = roundup_pow_of_two(size);
+	*srq->migr_db = 0;
+	return 0;
+}
 
-	return size;
+int mlx5_get_cons_index(struct ibv_cq *cq) {
+	return to_mcq(cq)->cons_index;
 }
 
-static int mlx5_calc_sq_size(struct mlx5_context *ctx,
-			     struct ibv_qp_init_attr_ex *attr,
-			     struct mlx5dv_qp_init_attr *mlx5_qp_attr,
-			     struct mlx5_qp *qp)
+void mlx5_set_cons_index(struct ibv_cq *cq, int cons_index) {
+	to_mcq(cq)->cons_index = cons_index;
+}
+
+struct ibv_cq_ex *mlx5_create_cq_ex(struct ibv_context *context,
+				    struct ibv_cq_init_attr_ex *cq_attr)
 {
-	int wqe_size;
-	int wq_size;
-	FILE *fp = ctx->dbg_fp;
+	return create_cq(context, cq_attr, MLX5_CQ_FLAGS_EXTENDED, NULL);
+}
 
-	if (!attr->cap.max_send_wr)
-		return 0;
+struct ibv_cq_ex *mlx5dv_create_cq(struct ibv_context *context,
+				      struct ibv_cq_init_attr_ex *cq_attr,
+				      struct mlx5dv_cq_init_attr *mlx5_cq_attr)
+{
+	struct ibv_cq_ex *cq;
 
-	wqe_size = mlx5_calc_send_wqe(ctx, attr, mlx5_qp_attr, qp);
-	if (wqe_size < 0) {
-		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
-		return wqe_size;
+	if (!is_mlx5_dev(context->device)) {
+		errno = EOPNOTSUPP;
+		return NULL;
 	}
 
-	if (wqe_size > ctx->max_sq_desc_sz) {
-		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
-		return -EINVAL;
-	}
+	cq = create_cq(context, cq_attr, MLX5_CQ_FLAGS_EXTENDED, mlx5_cq_attr);
+	if (!cq)
+		return NULL;
 
-	qp->max_inline_data = wqe_size - sq_overhead(qp, attr, mlx5_qp_attr) -
-		sizeof(struct mlx5_wqe_inl_data_seg);
-	attr->cap.max_inline_data = qp->max_inline_data;
+	verbs_init_cq(ibv_cq_ex_to_cq(cq), context,
+		      cq_attr->channel, cq_attr->cq_context);
+	return cq;
+}
 
-	/*
-	 * to avoid overflow, we limit max_send_wr so
-	 * that the multiplication will fit in int
-	 */
-	if (attr->cap.max_send_wr > 0x7fffffff / ctx->max_sq_desc_sz) {
-		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
-		return -EINVAL;
-	}
+int mlx5_resize_cq(struct ibv_cq *ibcq, int cqe)
+{
+	struct mlx5_cq *cq = to_mcq(ibcq);
+	struct mlx5_resize_cq_resp resp;
+	struct mlx5_resize_cq cmd;
+	struct mlx5_context *mctx = to_mctx(ibcq->context);
+	int err;
 
-	wq_size = roundup_pow_of_two(attr->cap.max_send_wr * wqe_size);
-	qp->sq.wqe_cnt = wq_size / MLX5_SEND_WQE_BB;
-	if (qp->sq.wqe_cnt > ctx->max_send_wqebb) {
-		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
-		return -EINVAL;
+	if (cqe < 0) {
+		errno = EINVAL;
+		return errno;
 	}
 
-	qp->sq.wqe_shift = STATIC_ILOG_32(MLX5_SEND_WQE_BB) - 1;
-	qp->sq.max_gs = attr->cap.max_send_sge;
-	qp->sq.max_post = wq_size / wqe_size;
+	memset(&cmd, 0, sizeof(cmd));
+	memset(&resp, 0, sizeof(resp));
 
-	return wq_size;
-}
+	if (((long long)cqe * 64) > INT_MAX)
+		return EINVAL;
 
-enum {
-	DV_CREATE_WQ_SUPPORTED_COMP_MASK = MLX5DV_WQ_INIT_ATTR_MASK_STRIDING_RQ
-};
+	if(ibv_get_signal())
+		mlx5_spin_lock(&cq->lock);
+	cq->active_cqes = cq->verbs_cq.cq.cqe;
+	if (cq->active_buf == &cq->buf_a)
+		cq->resize_buf = &cq->buf_b;
+	else
+		cq->resize_buf = &cq->buf_a;
 
-static int mlx5_calc_rwq_size(struct mlx5_context *ctx,
-			      struct mlx5_rwq *rwq,
-			      struct ibv_wq_init_attr *attr,
-			      struct mlx5dv_wq_init_attr *mlx5wq_attr)
-{
-	size_t wqe_size;
-	int wq_size;
-	uint32_t num_scatter;
-	int is_mprq = 0;
-	int scat_spc;
+	cqe = align_queue_size(cqe + 1);
+	if (cqe == ibcq->cqe + 1) {
+		cq->resize_buf = NULL;
+		err = 0;
+		goto out;
+	}
 
-	if (!attr->max_wr)
-		return -EINVAL;
-	if (mlx5wq_attr) {
-		if (!check_comp_mask(mlx5wq_attr->comp_mask,
-				     DV_CREATE_WQ_SUPPORTED_COMP_MASK))
-			return -EINVAL;
+	/* currently we don't change cqe size */
+	cq->resize_cqe_sz = cq->cqe_sz;
+	cq->resize_cqes = cqe;
+	err = mlx5_alloc_cq_buf(mctx, cq, cq->resize_buf, cq->resize_cqes, cq->resize_cqe_sz);
+	if (err) {
+		cq->resize_buf = NULL;
+		errno = ENOMEM;
+		goto out;
+	}
 
-		is_mprq = !!(mlx5wq_attr->comp_mask &
-			     MLX5DV_WQ_INIT_ATTR_MASK_STRIDING_RQ);
+	mlx5_free_cq_buf(mctx, &cq->tmp_buf);
+	err = mlx5_alloc_cq_buf(mctx, cq, &cq->tmp_buf, cq->resize_cqes, cq->resize_cqe_sz);
+	if(err) {
+		errno = ENOMEM;
+		goto out;
 	}
 
-	/* TBD: check caps for RQ */
-	num_scatter = max_t(uint32_t, attr->max_sge, 1);
-	wqe_size = sizeof(struct mlx5_wqe_data_seg) * num_scatter +
-		sizeof(struct mlx5_wqe_srq_next_seg) * is_mprq;
+	cmd.buf_addr = (uintptr_t)cq->resize_buf->buf;
+	cmd.cqe_size = cq->resize_cqe_sz;
 
-	if (rwq->wq_sig)
-		wqe_size += sizeof(struct mlx5_rwqe_sig);
+	err = ibv_cmd_resize_cq(ibcq, cqe - 1, &cmd.ibv_cmd, sizeof(cmd),
+				&resp.ibv_resp, sizeof(resp));
+	if (err)
+		goto out_buf;
 
-	if (wqe_size <= 0 || wqe_size > ctx->max_rq_desc_sz)
-		return -EINVAL;
+	mlx5_cq_resize_copy_cqes(mctx, cq);
+	mlx5_free_cq_buf(mctx, cq->active_buf);
+	cq->active_buf = cq->resize_buf;
+	cq->verbs_cq.cq.cqe = cqe - 1;
+	if(ibv_get_signal())
+		mlx5_spin_unlock(&cq->lock);
+	cq->resize_buf = NULL;
+	return 0;
 
-	wqe_size = roundup_pow_of_two(wqe_size);
-	wq_size = roundup_pow_of_two(attr->max_wr) * wqe_size;
-	wq_size = max(wq_size, MLX5_SEND_WQE_BB);
-	rwq->rq.wqe_cnt = wq_size / wqe_size;
-	rwq->rq.wqe_shift = ilog32(wqe_size - 1);
-	rwq->rq.max_post = 1 << ilog32(wq_size / wqe_size - 1);
-	scat_spc = wqe_size -
-		((rwq->wq_sig) ? sizeof(struct mlx5_rwqe_sig) : 0) -
-		is_mprq * sizeof(struct mlx5_wqe_srq_next_seg);
-	rwq->rq.max_gs = scat_spc / sizeof(struct mlx5_wqe_data_seg);
-	return wq_size;
+out_buf:
+	mlx5_free_cq_buf(mctx, cq->resize_buf);
+	cq->resize_buf = NULL;
+
+out:
+	mlx5_spin_unlock(&cq->lock);
+	return err;
 }
 
-static int mlx5_calc_rq_size(struct mlx5_context *ctx,
-			     struct ibv_qp_init_attr_ex *attr,
-			     struct mlx5_qp *qp)
+int mlx5_destroy_cq(struct ibv_cq *cq)
 {
-	int wqe_size;
-	int wq_size;
-	int scat_spc;
-	FILE *fp = ctx->dbg_fp;
-
-	if (!attr->cap.max_recv_wr)
-		return 0;
+	int ret;
+	struct mlx5_cq *mcq = to_mcq(cq);
 
-	if (attr->cap.max_recv_wr > ctx->max_recv_wr) {
-		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
-		return -EINVAL;
+	ret = ibv_cmd_destroy_cq(cq);
+	if (ret)
+		return ret;
+
+	mlx5_free_db(to_mctx(cq->context), mcq->dbrec, mcq->parent_domain,
+		    mcq->custom_db);
+	mlx5_free_db(to_mctx(cq->context), mcq->migr_dbrec, mcq->parent_domain,
+			mcq->custom_db);
+	mlx5_free_cq_buf(to_mctx(cq->context), &mcq->tmp_buf);
+	mlx5_free_cq_buf(to_mctx(cq->context), mcq->active_buf);
+	mlx5_free_cq_buf(to_mctx(cq->context), &mcq->migr_buf);
+	if (mcq->parent_domain)
+		atomic_fetch_sub(&to_mparent_domain(mcq->parent_domain)->mpd.refcount, 1);
+	free(mcq);
+
+	return 0;
+}
+
+struct ibv_srq *mlx5_create_srq(struct ibv_pd *pd,
+				struct ibv_srq_init_attr *attr)
+{
+	struct mlx5_create_srq      cmd;
+	struct mlx5_create_srq_resp resp;
+	struct mlx5_srq		   *srq;
+	int			    ret;
+	struct mlx5_context	   *ctx;
+	int			    max_sge;
+	struct ibv_srq		   *ibsrq;
+	char fname[128];
+	int info_fd;
+
+	ctx = to_mctx(pd->context);
+	srq = calloc(1, sizeof *srq);
+	if (!srq) {
+		mlx5_err(ctx->dbg_fp, "%s-%d:\n", __func__, __LINE__);
+		return NULL;
 	}
+	ibsrq = &srq->vsrq.srq;
 
-	wqe_size = mlx5_calc_rcv_wqe(ctx, attr, qp);
-	if (wqe_size < 0 || wqe_size > ctx->max_rq_desc_sz) {
-		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
-		return -EINVAL;
+	srq->staged_index = calloc(attr->attr.max_wr, sizeof(int));
+	if(!srq->staged_index) {
+		return NULL;
 	}
 
-	wq_size = roundup_pow_of_two(attr->cap.max_recv_wr) * wqe_size;
-	if (wqe_size) {
-		wq_size = max(wq_size, MLX5_SEND_WQE_BB);
-		qp->rq.wqe_cnt = wq_size / wqe_size;
-		qp->rq.wqe_shift = ilog32(wqe_size - 1);
-		qp->rq.max_post = 1 << ilog32(wq_size / wqe_size - 1);
-		scat_spc = wqe_size -
-			(qp->wq_sig ? sizeof(struct mlx5_rwqe_sig) : 0);
-		qp->rq.max_gs = scat_spc / sizeof(struct mlx5_wqe_data_seg);
-	} else {
-		qp->rq.wqe_cnt = 0;
-		qp->rq.wqe_shift = 0;
-		qp->rq.max_post = 0;
-		qp->rq.max_gs = 0;
+	srq->staged_tail = 0;
+	srq->staged_head = 0;
+
+	srq->n_posted = 0;
+	srq->n_acked = 0;
+
+	memset(&cmd, 0, sizeof cmd);
+	if (mlx5_spinlock_init_pd(&srq->lock, pd)) {
+		mlx5_err(ctx->dbg_fp, "%s-%d:\n", __func__, __LINE__);
+		goto err;
 	}
-	return wq_size;
-}
 
-static int mlx5_calc_wq_size(struct mlx5_context *ctx,
-			     struct ibv_qp_init_attr_ex *attr,
-			     struct mlx5dv_qp_init_attr *mlx5_qp_attr,
-			     struct mlx5_qp *qp)
-{
-	int ret;
-	int result;
+	if (attr->attr.max_wr > ctx->max_srq_recv_wr) {
+		mlx5_err(ctx->dbg_fp, "%s-%d:max_wr %d, max_srq_recv_wr %d\n", __func__, __LINE__,
+			 attr->attr.max_wr, ctx->max_srq_recv_wr);
+		errno = EINVAL;
+		goto err;
+	}
 
-	ret = mlx5_calc_sq_size(ctx, attr, mlx5_qp_attr, qp);
-	if (ret < 0)
-		return ret;
+	/*
+	 * this calculation does not consider required control segments. The
+	 * final calculation is done again later. This is done so to avoid
+	 * overflows of variables
+	 */
+	max_sge = ctx->max_rq_desc_sz / sizeof(struct mlx5_wqe_data_seg);
+	if (attr->attr.max_sge > max_sge) {
+		mlx5_err(ctx->dbg_fp, "%s-%d:max_wr %d, max_srq_recv_wr %d\n", __func__, __LINE__,
+			attr->attr.max_wr, ctx->max_srq_recv_wr);
+		errno = EINVAL;
+		goto err;
+	}
 
-	result = ret;
-	ret = mlx5_calc_rq_size(ctx, attr, qp);
-	if (ret < 0)
-		return ret;
+	srq->max_gs  = attr->attr.max_sge;
+	srq->counter = 0;
 
-	result += ret;
+	if (mlx5_alloc_srq_buf(pd->context, srq, attr->attr.max_wr, pd)) {
+		mlx5_err(ctx->dbg_fp, "%s-%d:\n", __func__, __LINE__);
+		goto err;
+	}
 
-	qp->sq.offset = ret;
-	qp->rq.offset = 0;
+	srq->onflight_recv_wr = calloc(srq->max,
+					sizeof(struct srq_recv_wr_item));
+	if(!srq->onflight_recv_wr) {
+		return NULL;
+	}
 
-	return result;
-}
+	for(int i = 0; i < srq->max; i++) {
+		srq->onflight_recv_wr[i].recv_wr.sg_list =
+						calloc(attr->attr.max_sge,
+							sizeof(struct ibv_sge));
+	}
 
-static void map_uuar(struct ibv_context *context, struct mlx5_qp *qp,
-		     int uuar_index, struct mlx5_bf *dyn_bf)
-{
-	struct mlx5_context *ctx = to_mctx(context);
+	srq->migrrdma_onflight = calloc(srq->max,
+					sizeof(struct srq_recv_wr_item));
+	if(!srq->migrrdma_onflight) {
+		return NULL;
+	}
 
-	if (!dyn_bf)
-		qp->bf = &ctx->bfs[uuar_index];
-	else
-		qp->bf = dyn_bf;
-}
+	for(int i = 0; i < srq->max; i++) {
+		srq->migrrdma_onflight[i].recv_wr.sg_list =
+						calloc(attr->attr.max_sge,
+							sizeof(struct ibv_sge));
+	}
 
-static const char *qptype2key(enum ibv_qp_type type)
-{
-	switch (type) {
-	case IBV_QPT_RC: return "HUGE_RC";
-	case IBV_QPT_UC: return "HUGE_UC";
-	case IBV_QPT_UD: return "HUGE_UD";
-	case IBV_QPT_RAW_PACKET: return "HUGE_RAW_ETH";
-	default: return "HUGE_NA";
+	srq->onflight_cap = srq->max;
+	srq->onflight_max_sge = attr->attr.max_sge;
+	list_head_init(&srq->onflight_list);
+	list_head_init(&srq->migrrdma_onflight_list);
+	list_head_init(&srq->staged_onflight_list);
+	srq->inspect_flag = 0;
+
+	srq->db = mlx5_alloc_dbrec(to_mctx(pd->context), pd, &srq->custom_db);
+	srq->migr_db = mlx5_alloc_dbrec(to_mctx(pd->context), pd, &srq->custom_db);
+	if (!srq->db || !srq->migr_db) {
+		mlx5_err(ctx->dbg_fp, "%s-%d:\n", __func__, __LINE__);
+		goto err_free;
 	}
-}
 
-static size_t mlx5_set_custom_qp_alignment(struct ibv_context *context,
-					   struct mlx5_qp *qp)
-{
-	uint32_t max_stride;
-	uint32_t buf_page;
+	if (!srq->custom_db)
+		*srq->db = 0;
 
-	/* The main QP buffer alignment requirement is QP_PAGE_SIZE /
-	 * MLX5_QPC_PAGE_OFFSET_QUANTA. In case the buffer is contig, then
-	 * QP_PAGE_SIZE is the buffer size align to system page_size roundup to
-	 * the next pow of two.
+	cmd.buf_addr = (uintptr_t) srq->buf.buf;
+	cmd.db_addr  = (uintptr_t) srq->db;
+	srq->wq_sig = srq_sig_enabled();
+	if (srq->wq_sig)
+		cmd.flags = MLX5_SRQ_FLAG_SIGNATURE;
+
+	attr->attr.max_sge = srq->max_gs;
+	pthread_mutex_lock(&ctx->srq_table_mutex);
+
+	/* Override max_wr to let kernel know about extra WQEs for the
+	 * wait queue.
 	 */
-	buf_page = roundup_pow_of_two(align(qp->buf_size,
-					    to_mdev(context->device)->page_size));
-	/* Another QP buffer alignment requirement is to consider send wqe and
-	 * receive wqe strides.
+	attr->attr.max_wr = srq->max - 1;
+
+	ret = ibv_cmd_create_srq(pd, ibsrq, attr, &cmd.ibv_cmd, sizeof(cmd),
+				 &resp.ibv_resp, sizeof(resp));
+	if (ret)
+		goto err_db;
+
+	/* Override kernel response that includes the wait queue with the real
+	 * number of WQEs that are applicable for the application.
 	 */
-	max_stride = max((1 << qp->sq.wqe_shift), (1 << qp->rq.wqe_shift));
-	return max(max_stride, buf_page / MLX5_QPC_PAGE_OFFSET_QUANTA);
-}
+	attr->attr.max_wr = srq->tail;
 
-static int mlx5_alloc_qp_buf(struct ibv_context *context,
-			     struct ibv_qp_init_attr_ex *attr,
-			     struct mlx5_qp *qp,
-			     int size)
-{
-	int err;
-	enum mlx5_alloc_type alloc_type;
-	enum mlx5_alloc_type default_alloc_type = MLX5_ALLOC_TYPE_ANON;
-	const char *qp_huge_key;
-	size_t req_align = to_mdev(context->device)->page_size;
+	ret = mlx5_store_srq(ctx, resp.srqn, srq);
+	if (ret)
+		goto err_destroy;
 
-	if (qp->sq.wqe_cnt) {
-		qp->sq.wrid = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wrid));
-		if (!qp->sq.wrid) {
-			errno = ENOMEM;
-			err = -1;
-			return err;
-		}
+	pthread_mutex_unlock(&ctx->srq_table_mutex);
 
-		qp->sq.wr_data = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wr_data));
-		if (!qp->sq.wr_data) {
-			errno = ENOMEM;
-			err = -1;
-			goto ex_wrid;
-		}
+	srq->srqn = resp.srqn;
+	srq->rsc.rsn = resp.srqn;
+	srq->rsc.type = MLX5_RSC_TYPE_SRQ;
 
-		qp->sq.wqe_head = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wqe_head));
-		if (!qp->sq.wqe_head) {
-			errno = ENOMEM;
-			err = -1;
-			goto ex_wrid;
-		}
+	if(ibv_cmd_install_srq_handle_mapping(pd->context, ibsrq->handle, ibsrq->handle)) {
+		ibv_destroy_srq(ibsrq);
+		return NULL;
 	}
 
-	if (qp->rq.wqe_cnt) {
-		qp->rq.wrid = malloc(qp->rq.wqe_cnt * sizeof(uint64_t));
-		if (!qp->rq.wrid) {
-			errno = ENOMEM;
-			err = -1;
-			goto ex_wrid;
-		}
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/srq_%d/buf_addr",
+				rdma_getpid(pd->context), pd->context->cmd_fd, ibsrq->handle);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_destroy_srq(ibsrq);
+		return NULL;
 	}
 
-	/* compatibility support */
-	qp_huge_key  = qptype2key(qp->ibv_qp->qp_type);
-	if (mlx5_use_huge(qp_huge_key))
-		default_alloc_type = MLX5_ALLOC_TYPE_HUGE;
+	if(write(info_fd, &srq->migr_buf.buf, sizeof(void *)) < 0) {
+		close(info_fd);
+		ibv_destroy_srq(ibsrq);
+		return NULL;
+	}
 
-	mlx5_get_alloc_type(to_mctx(context), attr->pd, MLX5_QP_PREFIX,
-			    &alloc_type, default_alloc_type);
+	close(info_fd);
 
-	if (alloc_type == MLX5_ALLOC_TYPE_CUSTOM) {
-		qp->buf.mparent_domain = to_mparent_domain(attr->pd);
-		if (attr->qp_type != IBV_QPT_RAW_PACKET &&
-		    !(qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY))
-			req_align = mlx5_set_custom_qp_alignment(context, qp);
-		qp->buf.req_alignment = req_align;
-		qp->buf.resource_type = MLX5DV_RES_TYPE_QP;
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/srq_%d/db_addr",
+				rdma_getpid(pd->context), pd->context->cmd_fd, ibsrq->handle);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_destroy_srq(ibsrq);
+		return NULL;
 	}
 
-	err = mlx5_alloc_prefered_buf(to_mctx(context), &qp->buf,
-				      align(qp->buf_size, req_align),
-				      to_mdev(context->device)->page_size,
-				      alloc_type,
-				      MLX5_QP_PREFIX);
-
-	if (err) {
-		err = -ENOMEM;
-		goto ex_wrid;
+	if(write(info_fd, &srq->migr_db, sizeof(void *)) < 0) {
+		close(info_fd);
+		ibv_destroy_srq(ibsrq);
+		return NULL;
 	}
 
-	if (qp->buf.type != MLX5_ALLOC_TYPE_CUSTOM)
-		memset(qp->buf.buf, 0, qp->buf_size);
+	close(info_fd);
 
-	if (attr->qp_type == IBV_QPT_RAW_PACKET ||
-	    qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY) {
-		size_t aligned_sq_buf_size = align(qp->sq_buf_size,
-						   to_mdev(context->device)->page_size);
+	return ibsrq;
 
-		if (alloc_type == MLX5_ALLOC_TYPE_CUSTOM) {
-			qp->sq_buf.mparent_domain = to_mparent_domain(attr->pd);
-			qp->sq_buf.req_alignment = to_mdev(context->device)->page_size;
-			qp->sq_buf.resource_type = MLX5DV_RES_TYPE_QP;
-		}
-
-		/* For Raw Packet QP, allocate a separate buffer for the SQ */
-		err = mlx5_alloc_prefered_buf(to_mctx(context), &qp->sq_buf,
-					      aligned_sq_buf_size,
-					      to_mdev(context->device)->page_size,
-					      alloc_type,
-					      MLX5_QP_PREFIX);
-		if (err) {
-			err = -ENOMEM;
-			goto rq_buf;
-		}
-
-		if (qp->sq_buf.type != MLX5_ALLOC_TYPE_CUSTOM)
-			memset(qp->sq_buf.buf, 0, aligned_sq_buf_size);
-	}
+err_destroy:
+	ibv_cmd_destroy_srq(ibsrq);
 
-	return 0;
-rq_buf:
-	mlx5_free_actual_buf(to_mctx(context), &qp->buf);
-ex_wrid:
-	if (qp->rq.wrid)
-		free(qp->rq.wrid);
+err_db:
+	pthread_mutex_unlock(&ctx->srq_table_mutex);
+	mlx5_free_db(to_mctx(pd->context), srq->db, pd, srq->custom_db);
+	mlx5_free_db(to_mctx(pd->context), srq->migr_db, pd, srq->custom_db);
 
-	if (qp->sq.wqe_head)
-		free(qp->sq.wqe_head);
+err_free:
+	free(srq->wrid);
+	free(srq->tmp_wrid);
+	mlx5_free_actual_buf(ctx, &srq->buf);
+	mlx5_free_actual_buf(ctx, &srq->migr_buf);
 
-	if (qp->sq.wr_data)
-		free(qp->sq.wr_data);
-	if (qp->sq.wrid)
-		free(qp->sq.wrid);
+err:
+	free(srq);
 
-	return err;
+	return NULL;
 }
 
-static void mlx5_free_qp_buf(struct mlx5_context *ctx, struct mlx5_qp *qp)
-{
-	mlx5_free_actual_buf(ctx, &qp->buf);
-
-	if (qp->sq_buf.buf)
-		mlx5_free_actual_buf(ctx, &qp->sq_buf);
+struct ibv_srq *mlx5_resume_srq(struct ibv_pd *pd, struct ibv_resume_srq_param *param) {
+	struct mlx5_create_srq      cmd;
+	struct mlx5_create_srq_resp resp;
+	struct mlx5_srq		   *srq;
+	int			    ret;
+	struct mlx5_context	   *ctx;
+	int			    max_sge;
+	struct ibv_srq		   *ibsrq;
+	char fname[128];
+	int info_fd;
+	struct ibv_srq_init_attr *attr = &param->init_attr;
+	struct mlx5_srq *orig_msrq;
 
-	if (qp->rq.wrid)
-		free(qp->rq.wrid);
+	orig_msrq = to_msrq((struct ibv_srq *)param->meta_uaddr);
 
-	if (qp->sq.wqe_head)
-		free(qp->sq.wqe_head);
+	ctx = to_mctx(pd->context);
+	srq = calloc(1, sizeof *srq);
+	if (!srq) {
+		mlx5_err(ctx->dbg_fp, "%s-%d:\n", __func__, __LINE__);
+		return NULL;
+	}
+	ibsrq = &srq->vsrq.srq;
 
-	if (qp->sq.wrid)
-		free(qp->sq.wrid);
+	srq->staged_index = calloc(attr->attr.max_wr, sizeof(int));
+	if(!srq->staged_index) {
+		return NULL;
+	}
 
-	if (qp->sq.wr_data)
-		free(qp->sq.wr_data);
-}
+	srq->staged_tail = 0;
+	srq->staged_head = 0;
 
-int mlx5_set_ece(struct ibv_qp *qp, struct ibv_ece *ece)
-{
-	struct mlx5_context *context = to_mctx(qp->context);
-	struct mlx5_qp *mqp = to_mqp(qp);
+	memset(&cmd, 0, sizeof cmd);
+	if (mlx5_spinlock_init_pd(&srq->lock, pd)) {
+		mlx5_err(ctx->dbg_fp, "%s-%d:\n", __func__, __LINE__);
+		goto err;
+	}
 
-	if (ece->comp_mask) {
+	if (attr->attr.max_wr > ctx->max_srq_recv_wr) {
+		mlx5_err(ctx->dbg_fp, "%s-%d:max_wr %d, max_srq_recv_wr %d\n", __func__, __LINE__,
+			 attr->attr.max_wr, ctx->max_srq_recv_wr);
 		errno = EINVAL;
-		return errno;
+		goto err;
 	}
 
-	if (ece->vendor_id != PCI_VENDOR_ID_MELLANOX) {
+	/*
+	 * this calculation does not consider required control segments. The
+	 * final calculation is done again later. This is done so to avoid
+	 * overflows of variables
+	 */
+	max_sge = ctx->max_rq_desc_sz / sizeof(struct mlx5_wqe_data_seg);
+	if (attr->attr.max_sge > max_sge) {
+		mlx5_err(ctx->dbg_fp, "%s-%d:max_wr %d, max_srq_recv_wr %d\n", __func__, __LINE__,
+			attr->attr.max_wr, ctx->max_srq_recv_wr);
 		errno = EINVAL;
-		return errno;
+		goto err;
 	}
 
-	if (!(context->flags & MLX5_CTX_FLAGS_ECE_SUPPORTED)) {
-		errno = EOPNOTSUPP;
-		return errno;
+	srq->max_gs  = attr->attr.max_sge;
+	srq->counter = 0;
+
+	if (mlx5_alloc_srq_buf(pd->context, srq, attr->attr.max_wr, pd)) {
+		mlx5_err(ctx->dbg_fp, "%s-%d:\n", __func__, __LINE__);
+		goto err;
 	}
 
-	mqp->set_ece = ece->options;
-	/* Clean previously returned ECE options */
-	mqp->get_ece = 0;
-	return 0;
-}
+	srq->onflight_recv_wr = calloc(srq->max,
+					sizeof(struct srq_recv_wr_item));
+	if(!srq->onflight_recv_wr) {
+		return NULL;
+	}
 
-int mlx5_query_ece(struct ibv_qp *qp, struct ibv_ece *ece)
-{
-	struct mlx5_qp *mqp = to_mqp(qp);
+	for(int i = 0; i < srq->max; i++) {
+		srq->onflight_recv_wr[i].recv_wr.sg_list =
+							calloc(attr->attr.max_sge,
+							sizeof(struct ibv_sge));
+	}
 
-	ece->vendor_id = PCI_VENDOR_ID_MELLANOX;
-	ece->options = mqp->get_ece;
-	ece->comp_mask = 0;
-	return 0;
-}
+	srq->migrrdma_onflight = calloc(srq->max,
+					sizeof(struct srq_recv_wr_item));
+	if(!srq->migrrdma_onflight) {
+		return NULL;
+	}
 
-static int mlx5_cmd_create_rss_qp(struct ibv_context *context,
-				 struct ibv_qp_init_attr_ex *attr,
-				 struct mlx5_qp *qp,
-				 uint32_t mlx5_create_flags)
-{
-	struct mlx5_create_qp_ex_rss cmd_ex_rss = {};
-	struct mlx5_create_qp_ex_resp resp = {};
-	struct mlx5_ib_create_qp_resp *resp_drv;
-	int ret;
+	for(int i = 0; i < srq->max; i++) {
+		srq->migrrdma_onflight[i].recv_wr.sg_list =
+							calloc(attr->attr.max_sge,
+							sizeof(struct ibv_sge));
+	}
 
-	if (attr->rx_hash_conf.rx_hash_key_len > sizeof(cmd_ex_rss.rx_hash_key)) {
-		errno = EINVAL;
-		return errno;
+	srq->onflight_cap = srq->max;
+	srq->onflight_max_sge = attr->attr.max_sge;
+	list_head_init(&srq->onflight_list);
+	list_head_init(&srq->migrrdma_onflight_list);
+	list_head_init(&srq->staged_onflight_list);
+	srq->inspect_flag = 0;
+
+	srq->db = mlx5_alloc_dbrec(to_mctx(pd->context), pd, &srq->custom_db);
+	if (!srq->db) {
+		mlx5_err(ctx->dbg_fp, "%s-%d:\n", __func__, __LINE__);
+		goto err_free;
 	}
 
-	cmd_ex_rss.rx_hash_fields_mask = attr->rx_hash_conf.rx_hash_fields_mask;
-	cmd_ex_rss.rx_hash_function = attr->rx_hash_conf.rx_hash_function;
-	cmd_ex_rss.rx_key_len = attr->rx_hash_conf.rx_hash_key_len;
-	cmd_ex_rss.flags = mlx5_create_flags;
-	memcpy(cmd_ex_rss.rx_hash_key, attr->rx_hash_conf.rx_hash_key,
-			attr->rx_hash_conf.rx_hash_key_len);
+	if (!srq->custom_db) {
+		*srq->db = 0;
+		*((__be32*)param->db_addr) = 0;
+	}
 
-	ret = ibv_cmd_create_qp_ex2(context, &qp->verbs_qp,
-				    attr,
-				    &cmd_ex_rss.ibv_cmd, sizeof(cmd_ex_rss),
-				    &resp.ibv_resp, sizeof(resp));
-	if (ret)
-		return ret;
+	cmd.buf_addr = (uintptr_t)param->buf_addr;
+	cmd.db_addr  = (uintptr_t)param->db_addr;
+	srq->wq_sig = srq_sig_enabled();
+	if (srq->wq_sig)
+		cmd.flags = MLX5_SRQ_FLAG_SIGNATURE;
 
-	resp_drv = &resp.drv_payload;
+	attr->attr.max_sge = srq->max_gs;
+	pthread_mutex_lock(&ctx->srq_table_mutex);
 
-	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TIRN)
-		qp->tirn = resp_drv->tirn;
+	/* Override max_wr to let kernel know about extra WQEs for the
+	 * wait queue.
+	 */
+	attr->attr.max_wr = srq->max - 1;
 
-	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TIR_ICM_ADDR)
-		qp->tir_icm_addr = resp_drv->tir_icm_addr;
+	ret = ibv_cmd_create_srq(pd, ibsrq, attr, &cmd.ibv_cmd, sizeof(cmd),
+				 &resp.ibv_resp, sizeof(resp));
+	if (ret)
+		goto err_db;
 
-	qp->rss_qp = 1;
-	return 0;
-}
+	/* Override kernel response that includes the wait queue with the real
+	 * number of WQEs that are applicable for the application.
+	 */
+	attr->attr.max_wr = srq->tail;
 
-static int mlx5_cmd_create_qp_ex(struct ibv_context *context,
-				 struct ibv_qp_init_attr_ex *attr,
-				 struct mlx5_create_qp *cmd,
-				 struct mlx5_qp *qp,
-				 struct mlx5_create_qp_ex_resp *resp)
-{
-	struct mlx5_create_qp_ex cmd_ex;
-	int ret;
+	ret = mlx5_store_srq(ctx, resp.srqn, srq);
+	if (ret)
+		goto err_destroy;
 
-	memset(&cmd_ex, 0, sizeof(cmd_ex));
-	*ibv_create_qp_ex_to_reg(&cmd_ex.ibv_cmd) = cmd->ibv_cmd.core_payload;
+	pthread_mutex_unlock(&ctx->srq_table_mutex);
 
-	cmd_ex.drv_payload = cmd->drv_payload;
+	srq->srqn = resp.srqn;
+	srq->rsc.rsn = resp.srqn;
+	srq->rsc.type = MLX5_RSC_TYPE_SRQ;
 
-	ret = ibv_cmd_create_qp_ex2(context, &qp->verbs_qp,
-				    attr, &cmd_ex.ibv_cmd,
-				    sizeof(cmd_ex), &resp->ibv_resp,
-				    sizeof(*resp));
+	mlx5_free_db(to_mctx(pd->context), srq->db, pd, srq->custom_db);
+	mlx5_free_actual_buf(ctx, &srq->buf);
 
-	return ret;
-}
+	if(ibv_cmd_install_srq_handle_mapping(pd->context, param->vhandle, ibsrq->handle)) {
+		ibv_destroy_srq(ibsrq);
+		return NULL;
+	}
 
-enum {
-	MLX5_CREATE_QP_SUP_COMP_MASK = (IBV_QP_INIT_ATTR_PD |
-					IBV_QP_INIT_ATTR_XRCD |
-					IBV_QP_INIT_ATTR_CREATE_FLAGS |
-					IBV_QP_INIT_ATTR_MAX_TSO_HEADER |
-					IBV_QP_INIT_ATTR_IND_TABLE |
-					IBV_QP_INIT_ATTR_RX_HASH |
-					IBV_QP_INIT_ATTR_SEND_OPS_FLAGS),
-};
+	ibsrq->handle = param->vhandle;
 
-enum {
-	MLX5_DV_CREATE_QP_SUP_COMP_MASK = MLX5DV_QP_INIT_ATTR_MASK_QP_CREATE_FLAGS |
-					  MLX5DV_QP_INIT_ATTR_MASK_DC |
-					  MLX5DV_QP_INIT_ATTR_MASK_SEND_OPS_FLAGS
-};
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/srq_%d/buf_addr",
+				rdma_getpid(pd->context), pd->context->cmd_fd, ibsrq->handle);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_destroy_srq(ibsrq);
+		return NULL;
+	}
 
-enum {
-	MLX5_CREATE_QP_EX2_COMP_MASK = (IBV_QP_INIT_ATTR_CREATE_FLAGS |
-					IBV_QP_INIT_ATTR_MAX_TSO_HEADER |
-					IBV_QP_INIT_ATTR_IND_TABLE |
-					IBV_QP_INIT_ATTR_RX_HASH),
-};
+	if(write(info_fd, &orig_msrq->buf.buf, sizeof(void *)) < 0) {
+		close(info_fd);
+		ibv_destroy_srq(ibsrq);
+		return NULL;
+	}
 
-enum {
-	MLX5DV_QP_CREATE_SUP_FLAGS =
-		(MLX5DV_QP_CREATE_TUNNEL_OFFLOADS |
-		 MLX5DV_QP_CREATE_TIR_ALLOW_SELF_LOOPBACK_UC |
-		 MLX5DV_QP_CREATE_TIR_ALLOW_SELF_LOOPBACK_MC |
-		 MLX5DV_QP_CREATE_DISABLE_SCATTER_TO_CQE |
-		 MLX5DV_QP_CREATE_ALLOW_SCATTER_TO_CQE |
-		 MLX5DV_QP_CREATE_PACKET_BASED_CREDIT_MODE |
-		 MLX5DV_QP_CREATE_SIG_PIPELINING),
-};
+	srq->buf.buf = param->buf_addr;
+	close(info_fd);
 
-static int create_dct(struct ibv_context *context,
-		      struct ibv_qp_init_attr_ex *attr,
-		      struct mlx5dv_qp_init_attr *mlx5_qp_attr,
-		      struct mlx5_qp *qp, uint32_t mlx5_create_flags)
-{
-	struct mlx5_create_qp		cmd = {};
-	struct mlx5_create_qp_resp	resp = {};
-	int				ret;
-	struct mlx5_context	       *ctx = to_mctx(context);
-	int32_t				usr_idx = 0xffffff;
-	FILE *fp = ctx->dbg_fp;
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/srq_%d/db_addr",
+				rdma_getpid(pd->context), pd->context->cmd_fd, ibsrq->handle);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_destroy_srq(ibsrq);
+		return NULL;
+	}
 
-	if (!check_comp_mask(attr->comp_mask, IBV_QP_INIT_ATTR_PD)) {
+	if(write(info_fd, &orig_msrq->db, sizeof(void *)) < 0) {
+		close(info_fd);
+		ibv_destroy_srq(ibsrq);
+		return NULL;
+	}
+
+	srq->db = param->db_addr;
+	close(info_fd);
+
+	{
+		typeof(orig_msrq->counter) *content_p;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = 0;
+		if(register_update_mem(&orig_msrq->counter,
+					sizeof(*content_p), content_p)) {
+			ibv_destroy_srq(ibsrq);
+			return NULL;
+		}
+	}
+
+	return ibsrq;
+
+err_destroy:
+	ibv_cmd_destroy_srq(ibsrq);
+
+err_db:
+	pthread_mutex_unlock(&ctx->srq_table_mutex);
+	mlx5_free_db(to_mctx(pd->context), srq->db, pd, srq->custom_db);
+
+err_free:
+	free(srq->wrid);
+	free(srq->tmp_wrid);
+	mlx5_free_actual_buf(ctx, &srq->buf);
+
+err:
+	free(srq);
+
+	return NULL;
+}
+
+int mlx5_modify_srq(struct ibv_srq *srq,
+		    struct ibv_srq_attr *attr,
+		    int attr_mask)
+{
+	struct ibv_modify_srq cmd;
+
+	return ibv_cmd_modify_srq(srq, attr, attr_mask, &cmd, sizeof cmd);
+}
+
+int mlx5_query_srq(struct ibv_srq *srq,
+		    struct ibv_srq_attr *attr)
+{
+	struct ibv_query_srq cmd;
+
+	return ibv_cmd_query_srq(srq, attr, &cmd, sizeof cmd);
+}
+
+int mlx5_destroy_srq(struct ibv_srq *srq)
+{
+	int ret;
+	struct mlx5_srq *msrq = to_msrq(srq);
+	struct mlx5_context *ctx = to_mctx(srq->context);
+
+	for(int i = 0; i < msrq->max; i++) {
+		free(msrq->onflight_recv_wr[i].recv_wr.sg_list);
+	}
+
+	for(int i = 0; i < msrq->max; i++) {
+		free(msrq->migrrdma_onflight[i].recv_wr.sg_list);
+	}
+
+	list_head_init(&msrq->onflight_list);
+	list_head_init(&msrq->migrrdma_onflight_list);
+	list_head_init(&msrq->staged_onflight_list);
+	free(msrq->onflight_recv_wr);
+	free(msrq->migrrdma_onflight);
+	free(msrq->staged_index);
+
+	if (msrq->cmd_qp) {
+		ret = mlx5_destroy_qp(msrq->cmd_qp);
+		if (ret)
+			return ret;
+		msrq->cmd_qp = NULL;
+	}
+
+	ret = ibv_cmd_destroy_srq(srq);
+	if (ret)
+		return ret;
+
+	if (ctx->cqe_version && msrq->rsc.type == MLX5_RSC_TYPE_XSRQ)
+		mlx5_clear_uidx(ctx, msrq->rsc.rsn);
+	else
+		mlx5_clear_srq(ctx, msrq->srqn);
+
+	mlx5_free_db(ctx, msrq->db, srq->pd, msrq->custom_db);
+	mlx5_free_actual_buf(ctx, &msrq->buf);
+	mlx5_free_db(ctx, msrq->migr_db, srq->pd, msrq->custom_db);
+	mlx5_free_actual_buf(ctx, &msrq->migr_buf);
+	free(msrq->tm_list);
+	free(msrq->wrid);
+	free(msrq->tmp_wrid);
+	free(msrq->op);
+	free(msrq);
+
+	return 0;
+}
+
+static int _sq_overhead(struct mlx5_qp *qp,
+			enum ibv_qp_type qp_type,
+			uint64_t ops,
+			uint64_t mlx5_ops)
+{
+	size_t size = sizeof(struct mlx5_wqe_ctrl_seg);
+	size_t rdma_size = 0;
+	size_t atomic_size = 0;
+	size_t mw_size = 0;
+
+	/* Operation overhead */
+	if (ops & (IBV_QP_EX_WITH_RDMA_WRITE |
+		   IBV_QP_EX_WITH_RDMA_WRITE_WITH_IMM |
+		   IBV_QP_EX_WITH_RDMA_READ))
+		rdma_size = sizeof(struct mlx5_wqe_ctrl_seg) +
+			    sizeof(struct mlx5_wqe_raddr_seg);
+
+	if (ops & (IBV_QP_EX_WITH_ATOMIC_CMP_AND_SWP |
+		   IBV_QP_EX_WITH_ATOMIC_FETCH_AND_ADD))
+		atomic_size = sizeof(struct mlx5_wqe_ctrl_seg) +
+			      sizeof(struct mlx5_wqe_raddr_seg) +
+			      sizeof(struct mlx5_wqe_atomic_seg);
+
+	if (ops & (IBV_QP_EX_WITH_BIND_MW | IBV_QP_EX_WITH_LOCAL_INV) ||
+	    (mlx5_ops & (MLX5DV_QP_EX_WITH_MR_INTERLEAVED |
+			 MLX5DV_QP_EX_WITH_MR_LIST)))
+		mw_size = sizeof(struct mlx5_wqe_ctrl_seg) +
+			  sizeof(struct mlx5_wqe_umr_ctrl_seg) +
+			  sizeof(struct mlx5_wqe_mkey_context_seg) +
+			  max_t(size_t, sizeof(struct mlx5_wqe_umr_klm_seg), 64);
+
+	size = max_t(size_t, size, rdma_size);
+	size = max_t(size_t, size, atomic_size);
+	size = max_t(size_t, size, mw_size);
+
+	/* Transport overhead */
+	switch (qp_type) {
+	case IBV_QPT_DRIVER:
+		if (qp->dc_type != MLX5DV_DCTYPE_DCI)
+			return -EINVAL;
+		SWITCH_FALLTHROUGH;
+
+	case IBV_QPT_UD:
+		size += sizeof(struct mlx5_wqe_datagram_seg);
+		if (qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY)
+			size += sizeof(struct mlx5_wqe_eth_seg) +
+				sizeof(struct mlx5_wqe_eth_pad);
+		break;
+
+	case IBV_QPT_XRC_RECV:
+	case IBV_QPT_XRC_SEND:
+		size += sizeof(struct mlx5_wqe_xrc_seg);
+		break;
+
+	case IBV_QPT_RAW_PACKET:
+		size += sizeof(struct mlx5_wqe_eth_seg);
+		break;
+
+	case IBV_QPT_RC:
+	case IBV_QPT_UC:
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	return size;
+}
+
+static int sq_overhead(struct mlx5_qp *qp, struct ibv_qp_init_attr_ex *attr,
+		       struct mlx5dv_qp_init_attr *mlx5_qp_attr)
+{
+	uint64_t ops;
+	uint64_t mlx5_ops = 0;
+
+	if (attr->comp_mask & IBV_QP_INIT_ATTR_SEND_OPS_FLAGS) {
+		ops = attr->send_ops_flags;
+	} else {
+		switch (attr->qp_type) {
+		case IBV_QPT_RC:
+		case IBV_QPT_UC:
+		case IBV_QPT_DRIVER:
+		case IBV_QPT_XRC_RECV:
+		case IBV_QPT_XRC_SEND:
+			ops = IBV_QP_EX_WITH_SEND |
+			      IBV_QP_EX_WITH_SEND_WITH_INV |
+			      IBV_QP_EX_WITH_SEND_WITH_IMM |
+			      IBV_QP_EX_WITH_RDMA_WRITE |
+			      IBV_QP_EX_WITH_RDMA_WRITE_WITH_IMM |
+			      IBV_QP_EX_WITH_RDMA_READ |
+			      IBV_QP_EX_WITH_ATOMIC_CMP_AND_SWP |
+			      IBV_QP_EX_WITH_ATOMIC_FETCH_AND_ADD |
+			      IBV_QP_EX_WITH_LOCAL_INV |
+			      IBV_QP_EX_WITH_BIND_MW;
+			break;
+
+		case IBV_QPT_UD:
+			ops = IBV_QP_EX_WITH_SEND |
+			      IBV_QP_EX_WITH_SEND_WITH_IMM |
+			      IBV_QP_EX_WITH_TSO;
+			break;
+
+		case IBV_QPT_RAW_PACKET:
+			ops = IBV_QP_EX_WITH_SEND |
+			      IBV_QP_EX_WITH_TSO;
+			break;
+
+		default:
+			return -EINVAL;
+		}
+	}
+
+
+	if (mlx5_qp_attr &&
+	    mlx5_qp_attr->comp_mask & MLX5DV_QP_INIT_ATTR_MASK_SEND_OPS_FLAGS)
+		mlx5_ops = mlx5_qp_attr->send_ops_flags;
+
+	return _sq_overhead(qp, attr->qp_type, ops, mlx5_ops);
+}
+
+static int mlx5_calc_send_wqe(struct mlx5_context *ctx,
+			      struct ibv_qp_init_attr_ex *attr,
+			      struct mlx5dv_qp_init_attr *mlx5_qp_attr,
+			      struct mlx5_qp *qp)
+{
+	int size;
+	int inl_size = 0;
+	int max_gather;
+	int tot_size;
+
+	size = sq_overhead(qp, attr, mlx5_qp_attr);
+	if (size < 0)
+		return size;
+
+	if (attr->cap.max_inline_data) {
+		inl_size = size + align(sizeof(struct mlx5_wqe_inl_data_seg) +
+			attr->cap.max_inline_data, 16);
+	}
+
+	if (attr->comp_mask & IBV_QP_INIT_ATTR_MAX_TSO_HEADER) {
+		size += align(attr->max_tso_header, 16);
+		qp->max_tso_header = attr->max_tso_header;
+	}
+
+	max_gather = (ctx->max_sq_desc_sz - size) /
+		sizeof(struct mlx5_wqe_data_seg);
+	if (attr->cap.max_send_sge > max_gather)
+		return -EINVAL;
+
+	size += attr->cap.max_send_sge * sizeof(struct mlx5_wqe_data_seg);
+	tot_size = max_int(size, inl_size);
+
+	if (tot_size > ctx->max_sq_desc_sz)
+		return -EINVAL;
+
+	return align(tot_size, MLX5_SEND_WQE_BB);
+}
+
+static int mlx5_calc_rcv_wqe(struct mlx5_context *ctx,
+			     struct ibv_qp_init_attr_ex *attr,
+			     struct mlx5_qp *qp)
+{
+	uint32_t size;
+	int num_scatter;
+
+	if (attr->srq)
+		return 0;
+
+	num_scatter = max_t(uint32_t, attr->cap.max_recv_sge, 1);
+	size = sizeof(struct mlx5_wqe_data_seg) * num_scatter;
+	if (qp->wq_sig)
+		size += sizeof(struct mlx5_rwqe_sig);
+
+	if (size > ctx->max_rq_desc_sz)
+		return -EINVAL;
+
+	size = roundup_pow_of_two(size);
+
+	return size;
+}
+
+static int mlx5_calc_sq_size(struct mlx5_context *ctx,
+			     struct ibv_qp_init_attr_ex *attr,
+			     struct mlx5dv_qp_init_attr *mlx5_qp_attr,
+			     struct mlx5_qp *qp)
+{
+	int wqe_size;
+	int wq_size;
+	FILE *fp = ctx->dbg_fp;
+
+	if (!attr->cap.max_send_wr)
+		return 0;
+
+	wqe_size = mlx5_calc_send_wqe(ctx, attr, mlx5_qp_attr, qp);
+	if (wqe_size < 0) {
+		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
+		return wqe_size;
+	}
+
+	if (wqe_size > ctx->max_sq_desc_sz) {
+		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
+		return -EINVAL;
+	}
+
+	qp->max_inline_data = wqe_size - sq_overhead(qp, attr, mlx5_qp_attr) -
+		sizeof(struct mlx5_wqe_inl_data_seg);
+	attr->cap.max_inline_data = qp->max_inline_data;
+
+	/*
+	 * to avoid overflow, we limit max_send_wr so
+	 * that the multiplication will fit in int
+	 */
+	if (attr->cap.max_send_wr > 0x7fffffff / ctx->max_sq_desc_sz) {
+		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
+		return -EINVAL;
+	}
+
+	wq_size = roundup_pow_of_two(attr->cap.max_send_wr * wqe_size);
+	qp->sq.wqe_cnt = wq_size / MLX5_SEND_WQE_BB;
+	if (qp->sq.wqe_cnt > ctx->max_send_wqebb) {
+		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
+		return -EINVAL;
+	}
+
+	qp->sq.wqe_shift = STATIC_ILOG_32(MLX5_SEND_WQE_BB) - 1;
+	qp->sq.max_gs = attr->cap.max_send_sge;
+	qp->sq.max_post = wq_size / wqe_size;
+
+	return wq_size;
+}
+
+enum {
+	DV_CREATE_WQ_SUPPORTED_COMP_MASK = MLX5DV_WQ_INIT_ATTR_MASK_STRIDING_RQ
+};
+
+static int mlx5_calc_rwq_size(struct mlx5_context *ctx,
+			      struct mlx5_rwq *rwq,
+			      struct ibv_wq_init_attr *attr,
+			      struct mlx5dv_wq_init_attr *mlx5wq_attr)
+{
+	size_t wqe_size;
+	int wq_size;
+	uint32_t num_scatter;
+	int is_mprq = 0;
+	int scat_spc;
+
+	if (!attr->max_wr)
+		return -EINVAL;
+	if (mlx5wq_attr) {
+		if (!check_comp_mask(mlx5wq_attr->comp_mask,
+				     DV_CREATE_WQ_SUPPORTED_COMP_MASK))
+			return -EINVAL;
+
+		is_mprq = !!(mlx5wq_attr->comp_mask &
+			     MLX5DV_WQ_INIT_ATTR_MASK_STRIDING_RQ);
+	}
+
+	/* TBD: check caps for RQ */
+	num_scatter = max_t(uint32_t, attr->max_sge, 1);
+	wqe_size = sizeof(struct mlx5_wqe_data_seg) * num_scatter +
+		sizeof(struct mlx5_wqe_srq_next_seg) * is_mprq;
+
+	if (rwq->wq_sig)
+		wqe_size += sizeof(struct mlx5_rwqe_sig);
+
+	if (wqe_size <= 0 || wqe_size > ctx->max_rq_desc_sz)
+		return -EINVAL;
+
+	wqe_size = roundup_pow_of_two(wqe_size);
+	wq_size = roundup_pow_of_two(attr->max_wr) * wqe_size;
+	wq_size = max(wq_size, MLX5_SEND_WQE_BB);
+	rwq->rq.wqe_cnt = wq_size / wqe_size;
+	rwq->rq.wqe_shift = ilog32(wqe_size - 1);
+	rwq->rq.max_post = 1 << ilog32(wq_size / wqe_size - 1);
+	scat_spc = wqe_size -
+		((rwq->wq_sig) ? sizeof(struct mlx5_rwqe_sig) : 0) -
+		is_mprq * sizeof(struct mlx5_wqe_srq_next_seg);
+	rwq->rq.max_gs = scat_spc / sizeof(struct mlx5_wqe_data_seg);
+	return wq_size;
+}
+
+static int mlx5_calc_rq_size(struct mlx5_context *ctx,
+			     struct ibv_qp_init_attr_ex *attr,
+			     struct mlx5_qp *qp)
+{
+	int wqe_size;
+	int wq_size;
+	int scat_spc;
+	FILE *fp = ctx->dbg_fp;
+
+	if (!attr->cap.max_recv_wr)
+		return 0;
+
+	if (attr->cap.max_recv_wr > ctx->max_recv_wr) {
+		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
+		return -EINVAL;
+	}
+
+	wqe_size = mlx5_calc_rcv_wqe(ctx, attr, qp);
+	if (wqe_size < 0 || wqe_size > ctx->max_rq_desc_sz) {
+		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
+		return -EINVAL;
+	}
+
+	wq_size = roundup_pow_of_two(attr->cap.max_recv_wr) * wqe_size;
+	if (wqe_size) {
+		wq_size = max(wq_size, MLX5_SEND_WQE_BB);
+		qp->rq.wqe_cnt = wq_size / wqe_size;
+		qp->rq.wqe_shift = ilog32(wqe_size - 1);
+		qp->rq.max_post = 1 << ilog32(wq_size / wqe_size - 1);
+		scat_spc = wqe_size -
+			(qp->wq_sig ? sizeof(struct mlx5_rwqe_sig) : 0);
+		qp->rq.max_gs = scat_spc / sizeof(struct mlx5_wqe_data_seg);
+	} else {
+		qp->rq.wqe_cnt = 0;
+		qp->rq.wqe_shift = 0;
+		qp->rq.max_post = 0;
+		qp->rq.max_gs = 0;
+	}
+	return wq_size;
+}
+
+static int mlx5_calc_wq_size(struct mlx5_context *ctx,
+			     struct ibv_qp_init_attr_ex *attr,
+			     struct mlx5dv_qp_init_attr *mlx5_qp_attr,
+			     struct mlx5_qp *qp)
+{
+	int ret;
+	int result;
+
+	ret = mlx5_calc_sq_size(ctx, attr, mlx5_qp_attr, qp);
+	if (ret < 0)
+		return ret;
+
+	result = ret;
+	ret = mlx5_calc_rq_size(ctx, attr, qp);
+	if (ret < 0)
+		return ret;
+
+	result += ret;
+
+	qp->sq.offset = ret;
+	qp->rq.offset = 0;
+
+	return result;
+}
+
+static void map_uuar(struct ibv_context *context, struct mlx5_qp *qp,
+		     int uuar_index, struct mlx5_bf *dyn_bf)
+{
+	struct mlx5_context *ctx = to_mctx(context);
+
+	if (!dyn_bf)
+		qp->bf = &ctx->bfs[uuar_index];
+	else
+		qp->bf = dyn_bf;
+}
+
+static const char *qptype2key(enum ibv_qp_type type)
+{
+	switch (type) {
+	case IBV_QPT_RC: return "HUGE_RC";
+	case IBV_QPT_UC: return "HUGE_UC";
+	case IBV_QPT_UD: return "HUGE_UD";
+	case IBV_QPT_RAW_PACKET: return "HUGE_RAW_ETH";
+	default: return "HUGE_NA";
+	}
+}
+
+static size_t mlx5_set_custom_qp_alignment(struct ibv_context *context,
+					   struct mlx5_qp *qp)
+{
+	uint32_t max_stride;
+	uint32_t buf_page;
+
+	/* The main QP buffer alignment requirement is QP_PAGE_SIZE /
+	 * MLX5_QPC_PAGE_OFFSET_QUANTA. In case the buffer is contig, then
+	 * QP_PAGE_SIZE is the buffer size align to system page_size roundup to
+	 * the next pow of two.
+	 */
+	buf_page = roundup_pow_of_two(align(qp->buf_size,
+					    to_mdev(context->device)->page_size));
+	/* Another QP buffer alignment requirement is to consider send wqe and
+	 * receive wqe strides.
+	 */
+	max_stride = max((1 << qp->sq.wqe_shift), (1 << qp->rq.wqe_shift));
+	return max(max_stride, buf_page / MLX5_QPC_PAGE_OFFSET_QUANTA);
+}
+
+static int mlx5_alloc_qp_buf(struct ibv_context *context,
+			     struct ibv_qp_init_attr_ex *attr,
+			     struct mlx5_qp *qp,
+			     int size)
+{
+	int err;
+	enum mlx5_alloc_type alloc_type;
+	enum mlx5_alloc_type default_alloc_type = MLX5_ALLOC_TYPE_ANON;
+	const char *qp_huge_key;
+	size_t req_align = to_mdev(context->device)->page_size;
+
+	if (qp->sq.wqe_cnt) {
+		qp->sq.wrid = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wrid));
+		if (!qp->sq.wrid) {
+			errno = ENOMEM;
+			err = -1;
+			return err;
+		}
+
+		qp->sq.wr_data = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wr_data));
+		if (!qp->sq.wr_data) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+
+		qp->sq.wqe_head = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wqe_head));
+		if (!qp->sq.wqe_head) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+
+		qp->sq.tmp_wrid = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wrid));
+		if (!qp->sq.tmp_wrid) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+
+		qp->sq.tmp_wr_data = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wr_data));
+		if (!qp->sq.tmp_wr_data) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+
+		qp->sq.tmp_wqe_head = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wqe_head));
+		if (!qp->sq.tmp_wqe_head) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+	}
+
+	if (qp->rq.wqe_cnt) {
+		qp->rq.wrid = malloc(qp->rq.wqe_cnt * sizeof(uint64_t));
+		if (!qp->rq.wrid) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+
+		qp->rq.tmp_wrid = malloc(qp->rq.wqe_cnt * sizeof(uint64_t));
+		if (!qp->rq.tmp_wrid) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+	}
+
+	/* compatibility support */
+	qp_huge_key  = qptype2key(qp->ibv_qp->qp_type);
+	if (mlx5_use_huge(qp_huge_key))
+		default_alloc_type = MLX5_ALLOC_TYPE_HUGE;
+
+	mlx5_get_alloc_type(to_mctx(context), attr->pd, MLX5_QP_PREFIX,
+			    &alloc_type, default_alloc_type);
+
+	if (alloc_type == MLX5_ALLOC_TYPE_CUSTOM) {
+		qp->buf.mparent_domain = to_mparent_domain(attr->pd);
+		if (attr->qp_type != IBV_QPT_RAW_PACKET &&
+		    !(qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY))
+			req_align = mlx5_set_custom_qp_alignment(context, qp);
+		qp->buf.req_alignment = req_align;
+		qp->buf.resource_type = MLX5DV_RES_TYPE_QP;
+	}
+
+	err = mlx5_alloc_prefered_buf(to_mctx(context), &qp->buf,
+				      align(qp->buf_size, req_align),
+				      to_mdev(context->device)->page_size,
+				      alloc_type,
+				      MLX5_QP_PREFIX);
+
+	if (err) {
+		err = -ENOMEM;
+		goto ex_wrid;
+	}
+
+	err = mlx5_alloc_prefered_buf(to_mctx(context), &qp->migr_buf,
+				      align(qp->buf_size, req_align),
+				      to_mdev(context->device)->page_size,
+				      alloc_type,
+				      MLX5_QP_PREFIX);
+
+	if(err) {
+		err = -ENOMEM;
+		goto rq_buf;
+	}
+
+	if (qp->buf.type != MLX5_ALLOC_TYPE_CUSTOM)
+		memset(qp->buf.buf, 0, qp->buf_size);
+
+	if (attr->qp_type == IBV_QPT_RAW_PACKET ||
+	    qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY) {
+		size_t aligned_sq_buf_size = align(qp->sq_buf_size,
+						   to_mdev(context->device)->page_size);
+
+		if (alloc_type == MLX5_ALLOC_TYPE_CUSTOM) {
+			qp->sq_buf.mparent_domain = to_mparent_domain(attr->pd);
+			qp->sq_buf.req_alignment = to_mdev(context->device)->page_size;
+			qp->sq_buf.resource_type = MLX5DV_RES_TYPE_QP;
+		}
+
+		/* For Raw Packet QP, allocate a separate buffer for the SQ */
+		err = mlx5_alloc_prefered_buf(to_mctx(context), &qp->sq_buf,
+					      aligned_sq_buf_size,
+					      to_mdev(context->device)->page_size,
+					      alloc_type,
+					      MLX5_QP_PREFIX);
+		if (err) {
+			err = -ENOMEM;
+			goto rq_buf;
+		}
+
+		if (qp->sq_buf.type != MLX5_ALLOC_TYPE_CUSTOM)
+			memset(qp->sq_buf.buf, 0, aligned_sq_buf_size);
+	}
+
+	return 0;
+rq_buf:
+	if(qp->migr_buf.buf) {
+		mlx5_free_actual_buf(to_mctx(context), &qp->migr_buf);
+	}
+	mlx5_free_actual_buf(to_mctx(context), &qp->buf);
+ex_wrid:
+	if (qp->rq.wrid)
+		free(qp->rq.wrid);
+	if(qp->rq.tmp_wrid)
+		free(qp->rq.tmp_wrid);
+
+	if (qp->sq.wqe_head)
+		free(qp->sq.wqe_head);
+	if(qp->sq.tmp_wqe_head)
+		free(qp->sq.tmp_wqe_head);
+
+	if (qp->sq.wr_data)
+		free(qp->sq.wr_data);
+	if(qp->sq.tmp_wr_data)
+		free(qp->sq.tmp_wr_data);
+
+	if (qp->sq.wrid)
+		free(qp->sq.wrid);
+	if(qp->sq.tmp_wrid)
+		free(qp->sq.tmp_wrid);
+
+	return err;
+}
+
+static int mlx5_resume_alloc_qp_buf(struct ibv_context *context,
+			     struct ibv_qp_init_attr_ex *attr,
+			     struct mlx5_qp *qp,
+			     int size, void *buf_addr)
+{
+	int err;
+	enum mlx5_alloc_type alloc_type;
+	enum mlx5_alloc_type default_alloc_type = MLX5_ALLOC_TYPE_ANON;
+	const char *qp_huge_key;
+	size_t req_align = to_mdev(context->device)->page_size;
+
+	if (qp->sq.wqe_cnt) {
+		qp->sq.wrid = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wrid));
+		if (!qp->sq.wrid) {
+			errno = ENOMEM;
+			err = -1;
+			return err;
+		}
+
+		qp->sq.wr_data = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wr_data));
+		if (!qp->sq.wr_data) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+
+		qp->sq.wqe_head = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wqe_head));
+		if (!qp->sq.wqe_head) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+
+		qp->sq.tmp_wrid = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wrid));
+		if (!qp->sq.tmp_wrid) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+
+		qp->sq.tmp_wr_data = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wr_data));
+		if (!qp->sq.tmp_wr_data) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+
+		qp->sq.tmp_wqe_head = malloc(qp->sq.wqe_cnt * sizeof(*qp->sq.wqe_head));
+		if (!qp->sq.tmp_wqe_head) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+	}
+
+	if (qp->rq.wqe_cnt) {
+		qp->rq.wrid = malloc(qp->rq.wqe_cnt * sizeof(uint64_t));
+		if (!qp->rq.wrid) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+
+		qp->rq.tmp_wrid = malloc(qp->rq.wqe_cnt * sizeof(uint64_t));
+		if (!qp->rq.tmp_wrid) {
+			errno = ENOMEM;
+			err = -1;
+			goto ex_wrid;
+		}
+	}
+
+	/* compatibility support */
+	qp_huge_key  = qptype2key(qp->ibv_qp->qp_type);
+	if (mlx5_use_huge(qp_huge_key))
+		default_alloc_type = MLX5_ALLOC_TYPE_HUGE;
+
+	mlx5_get_alloc_type(to_mctx(context), attr->pd, MLX5_QP_PREFIX,
+			    &alloc_type, default_alloc_type);
+
+	if (alloc_type == MLX5_ALLOC_TYPE_CUSTOM) {
+		qp->buf.mparent_domain = to_mparent_domain(attr->pd);
+		if (attr->qp_type != IBV_QPT_RAW_PACKET &&
+		    !(qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY))
+			req_align = mlx5_set_custom_qp_alignment(context, qp);
+		qp->buf.req_alignment = req_align;
+		qp->buf.resource_type = MLX5DV_RES_TYPE_QP;
+	}
+
+//	err = mlx5_alloc_prefered_buf(to_mctx(context), &qp->buf,
+//				      align(qp->buf_size, req_align),
+//				      to_mdev(context->device)->page_size,
+//				      alloc_type,
+//				      MLX5_QP_PREFIX);
+
+	qp->buf.buf = buf_addr;
+
+	if (err) {
+		err = -ENOMEM;
+		goto ex_wrid;
+	}
+
+//	if (qp->buf.type != MLX5_ALLOC_TYPE_CUSTOM)
+//		memset(qp->buf.buf, 0, qp->buf_size);
+
+	if (attr->qp_type == IBV_QPT_RAW_PACKET ||
+	    qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY) {
+			err = -EOPNOTSUPP;
+			goto rq_buf;
+#if 0
+		size_t aligned_sq_buf_size = align(qp->sq_buf_size,
+						   to_mdev(context->device)->page_size);
+
+		if (alloc_type == MLX5_ALLOC_TYPE_CUSTOM) {
+			qp->sq_buf.mparent_domain = to_mparent_domain(attr->pd);
+			qp->sq_buf.req_alignment = to_mdev(context->device)->page_size;
+			qp->sq_buf.resource_type = MLX5DV_RES_TYPE_QP;
+		}
+
+		/* For Raw Packet QP, allocate a separate buffer for the SQ */
+		err = mlx5_alloc_prefered_buf(to_mctx(context), &qp->sq_buf,
+					      aligned_sq_buf_size,
+					      to_mdev(context->device)->page_size,
+					      alloc_type,
+					      MLX5_QP_PREFIX);
+		if (err) {
+			err = -ENOMEM;
+			goto rq_buf;
+		}
+
+		if (qp->sq_buf.type != MLX5_ALLOC_TYPE_CUSTOM)
+			memset(qp->sq_buf.buf, 0, aligned_sq_buf_size);
+#endif
+	}
+
+	return 0;
+rq_buf:
+//	mlx5_free_actual_buf(to_mctx(context), &qp->buf);
+ex_wrid:
+	if (qp->rq.wrid)
+		free(qp->rq.wrid);
+	if(qp->rq.tmp_wrid)
+		free(qp->rq.tmp_wrid);
+
+	if (qp->sq.wqe_head)
+		free(qp->sq.wqe_head);
+	if(qp->sq.tmp_wqe_head)
+		free(qp->sq.tmp_wqe_head);
+
+	if (qp->sq.wr_data)
+		free(qp->sq.wr_data);
+	if(qp->sq.tmp_wr_data)
+		free(qp->sq.tmp_wr_data);
+
+	if (qp->sq.wrid)
+		free(qp->sq.wrid);
+	if(qp->sq.tmp_wrid)
+		free(qp->sq.tmp_wrid);
+
+	return err;
+}
+
+static void mlx5_free_qp_buf(struct mlx5_context *ctx, struct mlx5_qp *qp)
+{
+	mlx5_free_actual_buf(ctx, &qp->buf);
+
+	if (qp->rq.wrid)
+		free(qp->rq.wrid);
+	if(qp->rq.tmp_wrid)
+		free(qp->rq.tmp_wrid);
+
+	if (qp->sq.wqe_head)
+		free(qp->sq.wqe_head);
+	if(qp->sq.tmp_wqe_head)
+		free(qp->sq.tmp_wqe_head);
+
+	if (qp->sq.wr_data)
+		free(qp->sq.wr_data);
+	if(qp->sq.tmp_wr_data)
+		free(qp->sq.tmp_wr_data);
+
+	if (qp->sq.wrid)
+		free(qp->sq.wrid);
+	if(qp->sq.tmp_wrid)
+		free(qp->sq.tmp_wrid);
+}
+
+int mlx5_set_ece(struct ibv_qp *qp, struct ibv_ece *ece)
+{
+	struct mlx5_context *context = to_mctx(qp->context);
+	struct mlx5_qp *mqp = to_mqp(qp);
+
+	if (ece->comp_mask) {
+		errno = EINVAL;
+		return errno;
+	}
+
+	if (ece->vendor_id != PCI_VENDOR_ID_MELLANOX) {
+		errno = EINVAL;
+		return errno;
+	}
+
+	if (!(context->flags & MLX5_CTX_FLAGS_ECE_SUPPORTED)) {
+		errno = EOPNOTSUPP;
+		return errno;
+	}
+
+	mqp->set_ece = ece->options;
+	/* Clean previously returned ECE options */
+	mqp->get_ece = 0;
+	return 0;
+}
+
+int mlx5_query_ece(struct ibv_qp *qp, struct ibv_ece *ece)
+{
+	struct mlx5_qp *mqp = to_mqp(qp);
+
+	ece->vendor_id = PCI_VENDOR_ID_MELLANOX;
+	ece->options = mqp->get_ece;
+	ece->comp_mask = 0;
+	return 0;
+}
+
+static int mlx5_cmd_create_rss_qp(struct ibv_context *context,
+				 struct ibv_qp_init_attr_ex *attr,
+				 struct mlx5_qp *qp,
+				 uint32_t mlx5_create_flags)
+{
+	struct mlx5_create_qp_ex_rss cmd_ex_rss = {};
+	struct mlx5_create_qp_ex_resp resp = {};
+	struct mlx5_ib_create_qp_resp *resp_drv;
+	int ret;
+
+	if (attr->rx_hash_conf.rx_hash_key_len > sizeof(cmd_ex_rss.rx_hash_key)) {
+		errno = EINVAL;
+		return errno;
+	}
+
+	cmd_ex_rss.rx_hash_fields_mask = attr->rx_hash_conf.rx_hash_fields_mask;
+	cmd_ex_rss.rx_hash_function = attr->rx_hash_conf.rx_hash_function;
+	cmd_ex_rss.rx_key_len = attr->rx_hash_conf.rx_hash_key_len;
+	cmd_ex_rss.flags = mlx5_create_flags;
+	memcpy(cmd_ex_rss.rx_hash_key, attr->rx_hash_conf.rx_hash_key,
+			attr->rx_hash_conf.rx_hash_key_len);
+
+	ret = ibv_cmd_create_qp_ex2(context, &qp->verbs_qp,
+				    attr,
+				    &cmd_ex_rss.ibv_cmd, sizeof(cmd_ex_rss),
+				    &resp.ibv_resp, sizeof(resp));
+	if (ret)
+		return ret;
+
+	resp_drv = &resp.drv_payload;
+
+	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TIRN)
+		qp->tirn = resp_drv->tirn;
+
+	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TIR_ICM_ADDR)
+		qp->tir_icm_addr = resp_drv->tir_icm_addr;
+
+	qp->rss_qp = 1;
+	return 0;
+}
+
+static int mlx5_cmd_create_qp_ex(struct ibv_context *context,
+				 struct ibv_qp_init_attr_ex *attr,
+				 struct mlx5_create_qp *cmd,
+				 struct mlx5_qp *qp,
+				 struct mlx5_create_qp_ex_resp *resp)
+{
+	struct mlx5_create_qp_ex cmd_ex;
+	int ret;
+
+	memset(&cmd_ex, 0, sizeof(cmd_ex));
+	*ibv_create_qp_ex_to_reg(&cmd_ex.ibv_cmd) = cmd->ibv_cmd.core_payload;
+
+	cmd_ex.drv_payload = cmd->drv_payload;
+
+	ret = ibv_cmd_create_qp_ex2(context, &qp->verbs_qp,
+				    attr, &cmd_ex.ibv_cmd,
+				    sizeof(cmd_ex), &resp->ibv_resp,
+				    sizeof(*resp));
+
+	return ret;
+}
+
+enum {
+	MLX5_CREATE_QP_SUP_COMP_MASK = (IBV_QP_INIT_ATTR_PD |
+					IBV_QP_INIT_ATTR_XRCD |
+					IBV_QP_INIT_ATTR_CREATE_FLAGS |
+					IBV_QP_INIT_ATTR_MAX_TSO_HEADER |
+					IBV_QP_INIT_ATTR_IND_TABLE |
+					IBV_QP_INIT_ATTR_RX_HASH |
+					IBV_QP_INIT_ATTR_SEND_OPS_FLAGS),
+};
+
+enum {
+	MLX5_DV_CREATE_QP_SUP_COMP_MASK = MLX5DV_QP_INIT_ATTR_MASK_QP_CREATE_FLAGS |
+					  MLX5DV_QP_INIT_ATTR_MASK_DC |
+					  MLX5DV_QP_INIT_ATTR_MASK_SEND_OPS_FLAGS
+};
+
+enum {
+	MLX5_CREATE_QP_EX2_COMP_MASK = (IBV_QP_INIT_ATTR_CREATE_FLAGS |
+					IBV_QP_INIT_ATTR_MAX_TSO_HEADER |
+					IBV_QP_INIT_ATTR_IND_TABLE |
+					IBV_QP_INIT_ATTR_RX_HASH),
+};
+
+enum {
+	MLX5DV_QP_CREATE_SUP_FLAGS =
+		(MLX5DV_QP_CREATE_TUNNEL_OFFLOADS |
+		 MLX5DV_QP_CREATE_TIR_ALLOW_SELF_LOOPBACK_UC |
+		 MLX5DV_QP_CREATE_TIR_ALLOW_SELF_LOOPBACK_MC |
+		 MLX5DV_QP_CREATE_DISABLE_SCATTER_TO_CQE |
+		 MLX5DV_QP_CREATE_ALLOW_SCATTER_TO_CQE |
+		 MLX5DV_QP_CREATE_PACKET_BASED_CREDIT_MODE |
+		 MLX5DV_QP_CREATE_SIG_PIPELINING),
+};
+
+static int create_dct(struct ibv_context *context,
+		      struct ibv_qp_init_attr_ex *attr,
+		      struct mlx5dv_qp_init_attr *mlx5_qp_attr,
+		      struct mlx5_qp *qp, uint32_t mlx5_create_flags)
+{
+	struct mlx5_create_qp		cmd = {};
+	struct mlx5_create_qp_resp	resp = {};
+	int				ret;
+	struct mlx5_context	       *ctx = to_mctx(context);
+	int32_t				usr_idx = 0xffffff;
+	FILE *fp = ctx->dbg_fp;
+
+	if (!check_comp_mask(attr->comp_mask, IBV_QP_INIT_ATTR_PD)) {
 		mlx5_dbg(fp, MLX5_DBG_QP,
 			 "Unsupported comp_mask for %s\n", __func__);
 		errno = EINVAL;
 		return errno;
 	}
 
-	if (!check_comp_mask(mlx5_qp_attr->comp_mask,
-			     MLX5DV_QP_INIT_ATTR_MASK_DC |
-			     MLX5DV_QP_INIT_ATTR_MASK_QP_CREATE_FLAGS)) {
-		mlx5_dbg(fp, MLX5_DBG_QP,
-			 "Unsupported vendor comp_mask for %s\n", __func__);
-		errno = EINVAL;
-		return errno;
+	if (!check_comp_mask(mlx5_qp_attr->comp_mask,
+			     MLX5DV_QP_INIT_ATTR_MASK_DC |
+			     MLX5DV_QP_INIT_ATTR_MASK_QP_CREATE_FLAGS)) {
+		mlx5_dbg(fp, MLX5_DBG_QP,
+			 "Unsupported vendor comp_mask for %s\n", __func__);
+		errno = EINVAL;
+		return errno;
+	}
+
+	if (!check_comp_mask(mlx5_create_flags, MLX5_QP_FLAG_SCATTER_CQE)) {
+		mlx5_dbg(fp, MLX5_DBG_QP,
+			 "Unsupported creation flags requested for DCT QP\n");
+		errno = EINVAL;
+		return errno;
+	}
+
+	if (!(ctx->vendor_cap_flags & MLX5_VENDOR_CAP_FLAGS_SCAT2CQE_DCT))
+		mlx5_create_flags &= ~MLX5_QP_FLAG_SCATTER_CQE;
+
+	cmd.flags = MLX5_QP_FLAG_TYPE_DCT | mlx5_create_flags;
+	cmd.access_key = mlx5_qp_attr->dc_init_attr.dct_access_key;
+
+	if (ctx->cqe_version) {
+		usr_idx = mlx5_store_uidx(ctx, qp);
+		if (usr_idx < 0) {
+			mlx5_dbg(fp, MLX5_DBG_QP, "Couldn't find free user index\n");
+			errno = ENOMEM;
+			return errno;
+		}
+	}
+	cmd.uidx = usr_idx;
+	if (ctx->flags & MLX5_CTX_FLAGS_ECE_SUPPORTED)
+		/* Create QP should start from ECE version 1 as a trigger */
+		cmd.ece_options = 0x10000000;
+
+	ret = ibv_cmd_create_qp_ex(context, &qp->verbs_qp,
+				   attr, &cmd.ibv_cmd, sizeof(cmd),
+				   &resp.ibv_resp, sizeof(resp));
+	if (ret) {
+		mlx5_dbg(fp, MLX5_DBG_QP, "Couldn't create dct, ret %d\n", ret);
+		if (ctx->cqe_version)
+			mlx5_clear_uidx(ctx, cmd.uidx);
+		return ret;
+	}
+
+	qp->get_ece = resp.ece_options;
+	qp->dc_type = MLX5DV_DCTYPE_DCT;
+	qp->rsc.type = MLX5_RSC_TYPE_QP;
+	if (ctx->cqe_version)
+		qp->rsc.rsn = usr_idx;
+	return 0;
+}
+
+static struct ibv_qp *create_qp(struct ibv_context *context,
+				struct ibv_qp_init_attr_ex *attr,
+				struct mlx5dv_qp_init_attr *mlx5_qp_attr)
+{
+	struct mlx5_create_qp		cmd;
+	struct mlx5_create_qp_resp	resp;
+	struct mlx5_create_qp_ex_resp  resp_ex;
+	struct mlx5_qp		       *qp;
+	int				ret;
+	struct mlx5_context	       *ctx = to_mctx(context);
+	struct ibv_qp		       *ibqp;
+	int32_t				usr_idx = 0;
+	uint32_t			mlx5_create_flags = 0;
+	struct mlx5_bf			*bf = NULL;
+	FILE *fp = ctx->dbg_fp;
+	struct mlx5_parent_domain *mparent_domain;
+	struct mlx5_ib_create_qp_resp  *resp_drv;
+
+	if (attr->comp_mask & ~MLX5_CREATE_QP_SUP_COMP_MASK)
+		return NULL;
+
+	if ((attr->comp_mask & IBV_QP_INIT_ATTR_MAX_TSO_HEADER) &&
+	    (attr->qp_type != IBV_QPT_RAW_PACKET))
+		return NULL;
+
+	if (attr->comp_mask & IBV_QP_INIT_ATTR_SEND_OPS_FLAGS &&
+	    (attr->comp_mask & IBV_QP_INIT_ATTR_RX_HASH ||
+	     (attr->qp_type == IBV_QPT_DRIVER &&
+	      mlx5_qp_attr &&
+	      mlx5_qp_attr->comp_mask & MLX5DV_QP_INIT_ATTR_MASK_DC &&
+	      mlx5_qp_attr->dc_init_attr.dc_type == MLX5DV_DCTYPE_DCT))) {
+		errno = EINVAL;
+		return NULL;
+	}
+
+	qp = calloc(1, sizeof(*qp));
+	if (!qp) {
+		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
+		return NULL;
+	}
+
+	ibqp = &qp->verbs_qp.qp;
+	qp->ibv_qp = ibqp;
+
+	qp->onflight_recv_wr = calloc(attr->cap.max_recv_wr,
+						sizeof(struct ibv_recv_wr));
+	if(!qp->onflight_recv_wr) {
+		return NULL;
+	}
+
+	for(int i = 0; i < attr->cap.max_recv_wr; i++) {
+		qp->onflight_recv_wr[i].sg_list = calloc(attr->cap.max_recv_sge,
+							sizeof(struct ibv_sge));
+	}
+
+	qp->onflight_tail = 0;
+	qp->onflight_head = 0;
+	qp->onflight_cap = attr->cap.max_recv_wr;
+	qp->onflight_max_sge = attr->cap.max_recv_sge;
+
+	qp->cached_send_wr_list = calloc(attr->cap.max_send_wr,
+						sizeof(struct ibv_send_wr));
+	if(!qp->cached_send_wr_list) {
+		return NULL;
+	}
+
+	for(int i = 0; i < attr->cap.max_send_wr; i++) {
+		qp->cached_send_wr_list[i].sg_list = calloc(attr->cap.max_send_sge,
+							sizeof(struct ibv_sge));
+	}
+
+	qp->n_cached = 0;
+	qp->cached_cap = attr->cap.max_send_wr;
+
+	if ((attr->comp_mask & IBV_QP_INIT_ATTR_CREATE_FLAGS) &&
+		(attr->create_flags & IBV_QP_CREATE_SOURCE_QPN)) {
+
+		if (attr->qp_type != IBV_QPT_UD) {
+			errno = EINVAL;
+			goto err;
+		}
+
+		qp->flags |= MLX5_QP_FLAGS_USE_UNDERLAY;
+	}
+
+	memset(&cmd, 0, sizeof(cmd));
+	memset(&resp, 0, sizeof(resp));
+	memset(&resp_ex, 0, sizeof(resp_ex));
+
+	if (use_scatter_to_cqe())
+		mlx5_create_flags |= MLX5_QP_FLAG_SCATTER_CQE;
+
+	if (mlx5_qp_attr) {
+		if (!check_comp_mask(mlx5_qp_attr->comp_mask,
+				     MLX5_DV_CREATE_QP_SUP_COMP_MASK)) {
+			mlx5_dbg(fp, MLX5_DBG_QP,
+				 "Unsupported vendor comp_mask for create_qp\n");
+			errno = EINVAL;
+			goto err;
+		}
+
+		if ((mlx5_qp_attr->comp_mask & MLX5DV_QP_INIT_ATTR_MASK_DC) &&
+		    (attr->qp_type != IBV_QPT_DRIVER)) {
+			mlx5_dbg(fp, MLX5_DBG_QP, "DC QP must be of type IBV_QPT_DRIVER\n");
+			errno = EINVAL;
+			goto err;
+		}
+		if (mlx5_qp_attr->comp_mask &
+		    MLX5DV_QP_INIT_ATTR_MASK_QP_CREATE_FLAGS) {
+			if (!check_comp_mask(mlx5_qp_attr->create_flags,
+					     MLX5DV_QP_CREATE_SUP_FLAGS)) {
+				mlx5_dbg(fp, MLX5_DBG_QP,
+					 "Unsupported creation flags requested for create_qp\n");
+				errno = EINVAL;
+				goto err;
+			}
+			if (mlx5_qp_attr->create_flags &
+			    MLX5DV_QP_CREATE_TUNNEL_OFFLOADS) {
+				mlx5_create_flags |= MLX5_QP_FLAG_TUNNEL_OFFLOADS;
+			}
+			if (mlx5_qp_attr->create_flags &
+			    MLX5DV_QP_CREATE_TIR_ALLOW_SELF_LOOPBACK_UC) {
+				mlx5_create_flags |=
+					MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_UC;
+			}
+			if (mlx5_qp_attr->create_flags &
+			    MLX5DV_QP_CREATE_TIR_ALLOW_SELF_LOOPBACK_MC) {
+				mlx5_create_flags |=
+					MLX5_QP_FLAG_TIR_ALLOW_SELF_LB_MC;
+			}
+			if (mlx5_qp_attr->create_flags &
+			    MLX5DV_QP_CREATE_DISABLE_SCATTER_TO_CQE) {
+				if (mlx5_qp_attr->create_flags &
+				    MLX5DV_QP_CREATE_ALLOW_SCATTER_TO_CQE) {
+					mlx5_dbg(fp, MLX5_DBG_QP,
+						 "Wrong usage of creation flags requested for create_qp\n");
+					errno = EINVAL;
+					goto err;
+				}
+				mlx5_create_flags &= ~MLX5_QP_FLAG_SCATTER_CQE;
+			}
+			if (mlx5_qp_attr->create_flags &
+			    MLX5DV_QP_CREATE_ALLOW_SCATTER_TO_CQE) {
+				mlx5_create_flags |=
+					(MLX5_QP_FLAG_ALLOW_SCATTER_CQE |
+					 MLX5_QP_FLAG_SCATTER_CQE);
+			}
+			if (mlx5_qp_attr->create_flags &
+			    MLX5DV_QP_CREATE_PACKET_BASED_CREDIT_MODE)
+				mlx5_create_flags |= MLX5_QP_FLAG_PACKET_BASED_CREDIT_MODE;
+
+			if (mlx5_qp_attr->create_flags &
+			    MLX5DV_QP_CREATE_SIG_PIPELINING) {
+				if (!(to_mctx(context)->flags &
+				      MLX5_CTX_FLAGS_SQD2RTS_SUPPORTED)) {
+					errno = EOPNOTSUPP;
+					goto err;
+				}
+				qp->flags |= MLX5_QP_FLAGS_DRAIN_SIGERR;
+			}
+
+		}
+
+		if (attr->qp_type == IBV_QPT_DRIVER) {
+			if (mlx5_qp_attr->comp_mask & MLX5DV_QP_INIT_ATTR_MASK_DC) {
+				if (mlx5_qp_attr->dc_init_attr.dc_type == MLX5DV_DCTYPE_DCT) {
+					ret = create_dct(context, attr, mlx5_qp_attr,
+							 qp, mlx5_create_flags);
+					if (ret)
+						goto err;
+					return ibqp;
+				} else if (mlx5_qp_attr->dc_init_attr.dc_type == MLX5DV_DCTYPE_DCI) {
+					mlx5_create_flags |= MLX5_QP_FLAG_TYPE_DCI;
+					qp->dc_type = MLX5DV_DCTYPE_DCI;
+				} else {
+					errno = EINVAL;
+					goto err;
+				}
+			} else {
+				errno = EINVAL;
+				goto err;
+			}
+		}
+
+	} else {
+		if (attr->qp_type == IBV_QPT_DRIVER)
+			goto err;
+	}
+
+	if (attr->comp_mask & IBV_QP_INIT_ATTR_RX_HASH) {
+		/* Scatter2CQE is unsupported for RSS QP */
+		mlx5_create_flags &= ~MLX5_QP_FLAG_SCATTER_CQE;
+
+		ret = mlx5_cmd_create_rss_qp(context, attr, qp,
+					     mlx5_create_flags);
+		if (ret)
+			goto err;
+
+		return ibqp;
 	}
 
-	if (!check_comp_mask(mlx5_create_flags, MLX5_QP_FLAG_SCATTER_CQE)) {
-		mlx5_dbg(fp, MLX5_DBG_QP,
-			 "Unsupported creation flags requested for DCT QP\n");
-		errno = EINVAL;
-		return errno;
+	if (ctx->atomic_cap)
+		qp->atomics_enabled = 1;
+
+	if (attr->comp_mask & IBV_QP_INIT_ATTR_SEND_OPS_FLAGS ||
+	    (mlx5_qp_attr &&
+	     mlx5_qp_attr->comp_mask & MLX5DV_QP_INIT_ATTR_MASK_SEND_OPS_FLAGS)) {
+		/*
+		 * Scatter2cqe, which is a data-path optimization, is disabled
+		 * since driver DC data-path doesn't support it.
+		 */
+		if (mlx5_qp_attr &&
+		    mlx5_qp_attr->comp_mask & MLX5DV_QP_INIT_ATTR_MASK_DC) {
+			mlx5_create_flags &= ~MLX5_QP_FLAG_SCATTER_CQE;
+		}
+
+		ret = mlx5_qp_fill_wr_pfns(qp, attr, mlx5_qp_attr);
+		if (ret) {
+			errno = ret;
+			mlx5_dbg(fp, MLX5_DBG_QP, "Failed to handle operations flags (errno %d)\n", errno);
+			goto err;
+		}
 	}
 
-	if (!(ctx->vendor_cap_flags & MLX5_VENDOR_CAP_FLAGS_SCAT2CQE_DCT))
-		mlx5_create_flags &= ~MLX5_QP_FLAG_SCATTER_CQE;
+	cmd.flags = mlx5_create_flags;
+	qp->wq_sig = qp_sig_enabled();
+	if (qp->wq_sig)
+		cmd.flags |= MLX5_QP_FLAG_SIGNATURE;
 
-	cmd.flags = MLX5_QP_FLAG_TYPE_DCT | mlx5_create_flags;
-	cmd.access_key = mlx5_qp_attr->dc_init_attr.dct_access_key;
+	ret = mlx5_calc_wq_size(ctx, attr, mlx5_qp_attr, qp);
+	if (ret < 0) {
+		errno = -ret;
+		goto err;
+	}
+
+	if (attr->qp_type == IBV_QPT_RAW_PACKET ||
+	    qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY) {
+		qp->buf_size = qp->sq.offset;
+		qp->sq_buf_size = ret - qp->buf_size;
+		qp->sq.offset = 0;
+	} else {
+		qp->buf_size = ret;
+		qp->sq_buf_size = 0;
+	}
+
+	if (mlx5_alloc_qp_buf(context, attr, qp, ret)) {
+		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
+		goto err;
+	}
+
+	if (attr->qp_type == IBV_QPT_RAW_PACKET ||
+	    qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY) {
+		qp->sq_start = qp->sq_buf.buf;
+		qp->sq.qend = qp->sq_buf.buf +
+				(qp->sq.wqe_cnt << qp->sq.wqe_shift);
+	} else {
+		qp->sq_start = qp->buf.buf + qp->sq.offset;
+		qp->sq.qend = qp->buf.buf + qp->sq.offset +
+				(qp->sq.wqe_cnt << qp->sq.wqe_shift);
+	}
+
+	mlx5_init_qp_indices(qp);
+
+	qp->migr_reg = aligned_alloc(4096, 4096);
+	if(!qp->migr_reg) {
+		goto err;
+	}
+
+	if (mlx5_spinlock_init_pd(&qp->sq.lock, attr->pd) ||
+			mlx5_spinlock_init_pd(&qp->rq.lock, attr->pd))
+		goto err_free_qp_buf;
+
+	qp->db = mlx5_alloc_dbrec(ctx, attr->pd, &qp->custom_db);
+	qp->migr_db = mlx5_alloc_dbrec(ctx, attr->pd, &qp->custom_db);
+	if (!qp->db || !qp->migr_db) {
+		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
+		goto err_free_qp_buf;
+	}
+
+	if (!qp->custom_db) {
+		qp->db[MLX5_RCV_DBR] = 0;
+		qp->db[MLX5_SND_DBR] = 0;
+	}
+
+	cmd.buf_addr = (uintptr_t) qp->buf.buf;
+	cmd.sq_buf_addr = (attr->qp_type == IBV_QPT_RAW_PACKET ||
+			   qp->flags & MLX5_QP_FLAGS_USE_UNDERLAY) ?
+			  (uintptr_t) qp->sq_buf.buf : 0;
+	cmd.db_addr  = (uintptr_t) qp->db;
+	cmd.sq_wqe_count = qp->sq.wqe_cnt;
+	cmd.rq_wqe_count = qp->rq.wqe_cnt;
+	cmd.rq_wqe_shift = qp->rq.wqe_shift;
+
+	if (!ctx->cqe_version) {
+		cmd.uidx = 0xffffff;
+		pthread_mutex_lock(&ctx->qp_table_mutex);
+	} else if (!is_xrc_tgt(attr->qp_type)) {
+		usr_idx = mlx5_store_uidx(ctx, qp);
+		if (usr_idx < 0) {
+			mlx5_dbg(fp, MLX5_DBG_QP, "Couldn't find free user index\n");
+			goto err_rq_db;
+		}
+
+		cmd.uidx = usr_idx;
+	}
+
+	mparent_domain = to_mparent_domain(attr->pd);
+	if (mparent_domain && mparent_domain->mtd)
+		bf = mparent_domain->mtd->bf;
+
+	if (!bf && !(ctx->flags & MLX5_CTX_FLAGS_NO_KERN_DYN_UAR)) {
+		bf = mlx5_get_qp_uar(context);
+		if (!bf)
+			goto err_free_uidx;
+	}
+
+	if (bf) {
+		if (bf->dyn_alloc_uar) {
+			cmd.bfreg_index = bf->page_id;
+			cmd.flags |= MLX5_QP_FLAG_UAR_PAGE_INDEX;
+		} else {
+			cmd.bfreg_index = bf->bfreg_dyn_index;
+			cmd.flags |= MLX5_QP_FLAG_BFREG_INDEX;
+		}
+	}
+
+	if (ctx->flags & MLX5_CTX_FLAGS_ECE_SUPPORTED)
+		/* Create QP should start from ECE version 1 as a trigger */
+		cmd.ece_options = 0x10000000;
+
+	if (attr->comp_mask & MLX5_CREATE_QP_EX2_COMP_MASK)
+		ret = mlx5_cmd_create_qp_ex(context, attr, &cmd, qp, &resp_ex);
+	else
+		ret = ibv_cmd_create_qp_ex(context, &qp->verbs_qp,
+					   attr, &cmd.ibv_cmd, sizeof(cmd),
+					   &resp.ibv_resp, sizeof(resp));
+
+	if (ret) {
+		mlx5_dbg(fp, MLX5_DBG_QP, "ret %d\n", ret);
+		goto err_free_uidx;
+	}
+
+	resp_drv = attr->comp_mask & MLX5_CREATE_QP_EX2_COMP_MASK ?
+			&resp_ex.drv_payload : &resp.drv_payload;
+	if (!ctx->cqe_version) {
+		if (qp->sq.wqe_cnt || qp->rq.wqe_cnt) {
+			ret = mlx5_store_qp(ctx, ibqp->qp_num, qp);
+			if (ret) {
+				mlx5_dbg(fp, MLX5_DBG_QP, "ret %d\n", ret);
+				goto err_destroy;
+			}
+		}
+
+		pthread_mutex_unlock(&ctx->qp_table_mutex);
+	}
+
+	qp->get_ece = resp_drv->ece_options;
+	map_uuar(context, qp, resp_drv->bfreg_index, bf);
+
+	qp->rq.max_post = qp->rq.wqe_cnt;
+	if (attr->sq_sig_all)
+		qp->sq_signal_bits = MLX5_WQE_CTRL_CQ_UPDATE;
+	else
+		qp->sq_signal_bits = 0;
+
+	attr->cap.max_send_wr = qp->sq.max_post;
+	attr->cap.max_recv_wr = qp->rq.max_post;
+	attr->cap.max_recv_sge = qp->rq.max_gs;
+
+	qp->rsc.type = MLX5_RSC_TYPE_QP;
+	qp->rsc.rsn = (ctx->cqe_version && !is_xrc_tgt(attr->qp_type)) ?
+		      usr_idx : ibqp->qp_num;
+
+	if (mparent_domain)
+		atomic_fetch_add(&mparent_domain->mpd.refcount, 1);
+
+	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TIRN)
+		qp->tirn = resp_drv->tirn;
+
+	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TISN)
+		qp->tisn = resp_drv->tisn;
+
+	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_RQN)
+		qp->rqn = resp_drv->rqn;
+
+	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_SQN)
+		qp->sqn = resp_drv->sqn;
+
+	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TIR_ICM_ADDR)
+		qp->tir_icm_addr = resp_drv->tir_icm_addr;
+
+	if (attr->comp_mask & IBV_QP_INIT_ATTR_SEND_OPS_FLAGS)
+		qp->verbs_qp.comp_mask |= VERBS_QP_EX;
+
+	return ibqp;
+
+err_destroy:
+	ibv_cmd_destroy_qp(ibqp);
+
+err_free_uidx:
+	if (bf)
+		mlx5_put_qp_uar(ctx, bf);
+	if (!ctx->cqe_version)
+		pthread_mutex_unlock(&to_mctx(context)->qp_table_mutex);
+	else if (!is_xrc_tgt(attr->qp_type))
+		mlx5_clear_uidx(ctx, usr_idx);
+
+err_rq_db:
+	mlx5_free_db(to_mctx(context), qp->db, attr->pd, qp->custom_db);
 
-	if (ctx->cqe_version) {
-		usr_idx = mlx5_store_uidx(ctx, qp);
-		if (usr_idx < 0) {
-			mlx5_dbg(fp, MLX5_DBG_QP, "Couldn't find free user index\n");
-			errno = ENOMEM;
-			return errno;
-		}
-	}
-	cmd.uidx = usr_idx;
-	if (ctx->flags & MLX5_CTX_FLAGS_ECE_SUPPORTED)
-		/* Create QP should start from ECE version 1 as a trigger */
-		cmd.ece_options = 0x10000000;
+err_free_qp_buf:
+	mlx5_free_qp_buf(ctx, qp);
 
-	ret = ibv_cmd_create_qp_ex(context, &qp->verbs_qp,
-				   attr, &cmd.ibv_cmd, sizeof(cmd),
-				   &resp.ibv_resp, sizeof(resp));
-	if (ret) {
-		mlx5_dbg(fp, MLX5_DBG_QP, "Couldn't create dct, ret %d\n", ret);
-		if (ctx->cqe_version)
-			mlx5_clear_uidx(ctx, cmd.uidx);
-		return ret;
-	}
+err:
+	free(qp);
 
-	qp->get_ece = resp.ece_options;
-	qp->dc_type = MLX5DV_DCTYPE_DCT;
-	qp->rsc.type = MLX5_RSC_TYPE_QP;
-	if (ctx->cqe_version)
-		qp->rsc.rsn = usr_idx;
-	return 0;
+	return NULL;
 }
 
-static struct ibv_qp *create_qp(struct ibv_context *context,
+static struct ibv_qp *resume_qp(struct ibv_context *context,
 				struct ibv_qp_init_attr_ex *attr,
-				struct mlx5dv_qp_init_attr *mlx5_qp_attr)
+				struct mlx5dv_qp_init_attr *mlx5_qp_attr,
+				void *buf_addr, void *db_addr, int32_t usr_idx,
+				struct ibv_qp *orig_qp)
 {
 	struct mlx5_create_qp		cmd;
 	struct mlx5_create_qp_resp	resp;
@@ -2178,7 +3628,7 @@ static struct ibv_qp *create_qp(struct ibv_context *context,
 	int				ret;
 	struct mlx5_context	       *ctx = to_mctx(context);
 	struct ibv_qp		       *ibqp;
-	int32_t				usr_idx = 0;
+//	int32_t				usr_idx = 0;
 	uint32_t			mlx5_create_flags = 0;
 	struct mlx5_bf			*bf = NULL;
 	FILE *fp = ctx->dbg_fp;
@@ -2211,6 +3661,36 @@ static struct ibv_qp *create_qp(struct ibv_context *context,
 	ibqp = &qp->verbs_qp.qp;
 	qp->ibv_qp = ibqp;
 
+	qp->onflight_recv_wr = calloc(attr->cap.max_recv_wr,
+						sizeof(struct ibv_recv_wr));
+	if(!qp->onflight_recv_wr) {
+		return NULL;
+	}
+
+	for(int i = 0; i < attr->cap.max_recv_wr; i++) {
+		qp->onflight_recv_wr[i].sg_list = calloc(attr->cap.max_recv_sge,
+							sizeof(struct ibv_sge));
+	}
+
+	qp->onflight_tail = 0;
+	qp->onflight_head = 0;
+	qp->onflight_cap = attr->cap.max_recv_wr;
+	qp->onflight_max_sge = attr->cap.max_recv_sge;
+
+	qp->cached_send_wr_list = calloc(attr->cap.max_send_wr,
+						sizeof(struct ibv_send_wr));
+	if(!qp->cached_send_wr_list) {
+		return NULL;
+	}
+
+	for(int i = 0; i < attr->cap.max_send_wr; i++) {
+		qp->cached_send_wr_list[i].sg_list = calloc(attr->cap.max_send_sge,
+							sizeof(struct ibv_sge));
+	}
+
+	qp->n_cached = 0;
+	qp->cached_cap = attr->cap.max_send_wr;
+
 	if ((attr->comp_mask & IBV_QP_INIT_ATTR_CREATE_FLAGS) &&
 		(attr->create_flags & IBV_QP_CREATE_SOURCE_QPN)) {
 
@@ -2382,7 +3862,7 @@ static struct ibv_qp *create_qp(struct ibv_context *context,
 		qp->sq_buf_size = 0;
 	}
 
-	if (mlx5_alloc_qp_buf(context, attr, qp, ret)) {
+	if (mlx5_resume_alloc_qp_buf(context, attr, qp, ret, buf_addr)) {
 		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
 		goto err;
 	}
@@ -2404,16 +3884,20 @@ static struct ibv_qp *create_qp(struct ibv_context *context,
 			mlx5_spinlock_init_pd(&qp->rq.lock, attr->pd))
 		goto err_free_qp_buf;
 
+#if 0
 	qp->db = mlx5_alloc_dbrec(ctx, attr->pd, &qp->custom_db);
 	if (!qp->db) {
 		mlx5_dbg(fp, MLX5_DBG_QP, "\n");
 		goto err_free_qp_buf;
 	}
+#endif
 
-	if (!qp->custom_db) {
+	qp->db = db_addr;
+
+//	if (!qp->custom_db) {
 		qp->db[MLX5_RCV_DBR] = 0;
 		qp->db[MLX5_SND_DBR] = 0;
-	}
+//	}
 
 	cmd.buf_addr = (uintptr_t) qp->buf.buf;
 	cmd.sq_buf_addr = (attr->qp_type == IBV_QPT_RAW_PACKET ||
@@ -2428,132 +3912,663 @@ static struct ibv_qp *create_qp(struct ibv_context *context,
 		cmd.uidx = 0xffffff;
 		pthread_mutex_lock(&ctx->qp_table_mutex);
 	} else if (!is_xrc_tgt(attr->qp_type)) {
+#if 0
 		usr_idx = mlx5_store_uidx(ctx, qp);
 		if (usr_idx < 0) {
 			mlx5_dbg(fp, MLX5_DBG_QP, "Couldn't find free user index\n");
 			goto err_rq_db;
 		}
+#endif
+#if 0
+		pthread_mutex_lock(&ctx->uidx_table_mutex);
+		if(!ctx->uidx_table[usr_idx >> MLX5_UIDX_TABLE_SHIFT].refcnt) {
+			ctx->uidx_table[usr_idx >> MLX5_UIDX_TABLE_SHIFT].table =
+					calloc(MLX5_UIDX_TABLE_MASK + 1, sizeof(struct mlx5_resource *));
+			if(!ctx->uidx_table[usr_idx >> MLX5_UIDX_TABLE_SHIFT].table) {
+				pthread_mutex_unlock(&ctx->uidx_table_mutex);
+				goto err_free_uidx;
+			}
+		}
+
+		ctx->uidx_table[usr_idx >> MLX5_UIDX_TABLE_SHIFT].refcnt++;
+		ctx->uidx_table[usr_idx >> MLX5_UIDX_TABLE_SHIFT]
+				.table[usr_idx & MLX5_UIDX_TABLE_MASK] = to_mqp(orig_qp);
+		pthread_mutex_unlock(&ctx->uidx_table_mutex);
+#endif
+		cmd.uidx = usr_idx;
+	}
+
+	mparent_domain = to_mparent_domain(attr->pd);
+	if (mparent_domain && mparent_domain->mtd)
+		bf = mparent_domain->mtd->bf;
+
+	if (!bf && !(ctx->flags & MLX5_CTX_FLAGS_NO_KERN_DYN_UAR)) {
+		bf = mlx5_get_qp_uar(context);
+		if (!bf)
+			goto err_free_uidx;
+	}
+
+	if (bf) {
+		if (bf->dyn_alloc_uar) {
+			cmd.bfreg_index = bf->page_id;
+			cmd.flags |= MLX5_QP_FLAG_UAR_PAGE_INDEX;
+		} else {
+			cmd.bfreg_index = bf->bfreg_dyn_index;
+			cmd.flags |= MLX5_QP_FLAG_BFREG_INDEX;
+		}
+	}
+
+	if (ctx->flags & MLX5_CTX_FLAGS_ECE_SUPPORTED)
+		/* Create QP should start from ECE version 1 as a trigger */
+		cmd.ece_options = 0x10000000;
+
+	if (attr->comp_mask & MLX5_CREATE_QP_EX2_COMP_MASK)
+		ret = mlx5_cmd_create_qp_ex(context, attr, &cmd, qp, &resp_ex);
+	else
+		ret = ibv_cmd_create_qp_ex(context, &qp->verbs_qp,
+					   attr, &cmd.ibv_cmd, sizeof(cmd),
+					   &resp.ibv_resp, sizeof(resp));
+	if (ret) {
+		mlx5_dbg(fp, MLX5_DBG_QP, "ret %d\n", ret);
+		goto err_free_uidx;
+	}
+
+	resp_drv = attr->comp_mask & MLX5_CREATE_QP_EX2_COMP_MASK ?
+			&resp_ex.drv_payload : &resp.drv_payload;
+	if (!ctx->cqe_version) {
+		if (qp->sq.wqe_cnt || qp->rq.wqe_cnt) {
+			ret = mlx5_store_qp(ctx, ibqp->qp_num, qp);
+			if (ret) {
+				mlx5_dbg(fp, MLX5_DBG_QP, "ret %d\n", ret);
+				goto err_destroy;
+			}
+		}
+
+		pthread_mutex_unlock(&ctx->qp_table_mutex);
+	}
+
+	qp->get_ece = resp_drv->ece_options;
+	map_uuar(context, qp, resp_drv->bfreg_index, bf);
+
+	qp->rq.max_post = qp->rq.wqe_cnt;
+	if (attr->sq_sig_all)
+		qp->sq_signal_bits = MLX5_WQE_CTRL_CQ_UPDATE;
+	else
+		qp->sq_signal_bits = 0;
+
+	attr->cap.max_send_wr = qp->sq.max_post;
+	attr->cap.max_recv_wr = qp->rq.max_post;
+	attr->cap.max_recv_sge = qp->rq.max_gs;
+
+	qp->rsc.type = MLX5_RSC_TYPE_QP;
+	qp->rsc.rsn = (ctx->cqe_version && !is_xrc_tgt(attr->qp_type)) ?
+		      usr_idx : ibqp->qp_num;
+
+	if (mparent_domain)
+		atomic_fetch_add(&mparent_domain->mpd.refcount, 1);
+
+	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TIRN)
+		qp->tirn = resp_drv->tirn;
+
+	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TISN)
+		qp->tisn = resp_drv->tisn;
+
+	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_RQN)
+		qp->rqn = resp_drv->rqn;
+
+	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_SQN)
+		qp->sqn = resp_drv->sqn;
+
+	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TIR_ICM_ADDR)
+		qp->tir_icm_addr = resp_drv->tir_icm_addr;
+
+	if (attr->comp_mask & IBV_QP_INIT_ATTR_SEND_OPS_FLAGS)
+		qp->verbs_qp.comp_mask |= VERBS_QP_EX;
+
+	return ibqp;
+
+err_destroy:
+	ibv_cmd_destroy_qp(ibqp);
+
+err_free_uidx:
+	if (bf)
+		mlx5_put_qp_uar(ctx, bf);
+	if (!ctx->cqe_version)
+		pthread_mutex_unlock(&to_mctx(context)->qp_table_mutex);
+	else if (!is_xrc_tgt(attr->qp_type))
+		mlx5_clear_uidx(ctx, usr_idx);
+
+err_rq_db:
+//	mlx5_free_db(to_mctx(context), qp->db, attr->pd, qp->custom_db);
+
+err_free_qp_buf:
+	if (qp->rq.wrid)
+		free(qp->rq.wrid);
+	if(qp->rq.tmp_wrid)
+		free(qp->rq.tmp_wrid);
+
+	if (qp->sq.wqe_head)
+		free(qp->sq.wqe_head);
+	if(qp->sq.tmp_wqe_head)
+		free(qp->sq.tmp_wqe_head);
+
+	if (qp->sq.wr_data)
+		free(qp->sq.wr_data);
+	if(qp->sq.tmp_wr_data)
+		free(qp->sq.tmp_wr_data);
+
+	if (qp->sq.wrid)
+		free(qp->sq.wrid);
+	if(qp->sq.tmp_wrid)
+		free(qp->sq.tmp_wrid);
+
+err:
+	free(qp);
+
+	return NULL;
+}
+
+struct ibv_qp *mlx5_resume_qp(struct ibv_context *context, int pd_handle, int qp_handle,
+				struct ibv_qp_init_attr *attr, void *buf_addr, void *db_addr,
+				int32_t usr_idx, struct ibv_qp *orig_qp, unsigned long long *bf_reg) {
+	struct ibv_qp *qp;
+	struct ibv_qp_init_attr_ex attrx;
+	struct mlx5_pd pd;
+	struct mlx5_qp *mqp;
+	struct mlx5_qp *orig_mqp;
+	char fname[128];
+	int info_fd;
+
+//	struct mlx5_context *ctx;
+
+	memset(&pd, 0, sizeof(pd));
+	pd.ibv_pd.context = context;
+	pd.ibv_pd.handle = pd_handle;
+	atomic_init(&pd.refcount, 1);
+
+	memset(&attrx, 0, sizeof(attrx));
+	memcpy(&attrx, attr, sizeof(*attr));
+	attrx.comp_mask = IBV_QP_INIT_ATTR_PD;
+	attrx.pd = &pd;
+	qp = resume_qp(context, &attrx, NULL, buf_addr,
+					db_addr, usr_idx, orig_qp);
+	if(qp)
+		memcpy(attr, &attrx, sizeof(*attr));
+
+	if(!qp)
+		return NULL;
+
+	if(bf_reg)
+		*bf_reg = to_mqp(qp)->bf->reg;
+	
+	if(ibv_cmd_install_qp_handle_mapping(context, qp_handle, qp->handle)) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+	
+	mqp = to_mqp(qp);
+	orig_mqp = to_mqp(orig_qp);
+
+	orig_mqp->ibv_qp = orig_qp;
+//	orig_mqp->bf = mqp->bf;
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/buf_addr",
+					rdma_getpid(context), context->cmd_fd, qp_handle);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	if(write(info_fd, &orig_mqp->buf.buf, sizeof(void *)) < 0) {
+		close(info_fd);
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	close(info_fd);
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/db_addr",
+					rdma_getpid(context), context->cmd_fd, qp_handle);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	if(write(info_fd, &orig_mqp->db, sizeof(void *)) < 0) {
+		close(info_fd);
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	close(info_fd);
+	return qp;
+}
+
+void mlx5_free_qp(struct ibv_qp *qp) {
+	struct mlx5_qp *mqp = to_mqp(qp);
+	free(mqp);
+}
+
+struct ibv_qp *mlx5_calloc_qp(void) {
+	struct mlx5_qp *mqp;
+
+	mqp = calloc(1, sizeof(*mqp));
+	return mqp? &mqp->verbs_qp.qp: NULL;
+}
+
+static struct ibv_recv_wr *get_inflight_recv_wr(struct mlx5_qp *mqp,
+						uint64_t n_posted) {
+	struct ibv_recv_wr *wr_head = NULL;
+	struct ibv_recv_wr *wr = NULL;
+	struct ibv_recv_wr *last_wr = NULL;
+	int i;
+
+	for(i = mqp->onflight_tail > n_posted? mqp->onflight_tail: n_posted;
+							i < mqp->onflight_head; i++) {
+		wr = calloc(1, sizeof(*wr));
+		wr->wr_id = mqp->onflight_recv_wr[i % mqp->onflight_cap].wr_id;
+		wr->next = NULL;
+		wr->num_sge = mqp->onflight_recv_wr[i % mqp->onflight_cap].num_sge;
+		wr->sg_list = calloc(wr->num_sge, sizeof(struct ibv_sge));
+		memcpy(wr->sg_list, mqp->onflight_recv_wr[i % mqp->onflight_cap].sg_list,
+							sizeof(struct ibv_sge) * wr->num_sge);
+
+		if(!wr_head)
+			wr_head = wr;
+
+		if(last_wr)
+			last_wr->next = wr;
+
+		last_wr = wr;
+	}
+
+	return wr_head;
+}
+
+void mlx5_copy_qp(struct ibv_qp *ibqp1, struct ibv_qp *ibqp2,
+					void *param) {
+	struct mlx5_qp *mqp1 = to_mqp(ibqp1);
+	struct mlx5_qp *mqp2 = to_mqp(ibqp2);
+	struct ibv_qp *orig_qp = ibqp1;
+	struct ibv_recv_wr *wr_head;
+	struct ibv_recv_wr *bad_wr;
+	struct ibv_send_wr *bad_snd_wr;
+	uint64_t *n_posted = (uint64_t *)param;
+
+	wr_head = get_inflight_recv_wr(mqp1, *n_posted);
+	if(wr_head) {
+		mlx5_post_recv(ibqp2, wr_head, &bad_wr);
+		for(bad_wr = wr_head? wr_head->next: NULL; wr_head;
+					wr_head = bad_wr, bad_wr = wr_head? wr_head->next: NULL) {
+			free(wr_head->sg_list);
+			free(wr_head);
+		}
+	}
+	if(mqp1->n_cached > 0)
+		mlx5_post_send(ibqp2, mqp1->cached_send_wr_list, &bad_snd_wr);
+
+	mqp1->migr_flag = 1;
+	mqp1->switch_qp = mqp2;
+}
+
+int mlx5_prepare_qp_recv_replay(struct ibv_qp *qp, struct ibv_qp *new_qp) {
+	struct mlx5_qp *mqp = to_mqp(qp);
+
+	{
+		typeof(mqp->migr_bf) *content_p;
+		content_p = malloc(sizeof(*content_p));
+		memcpy(content_p, to_mqp(new_qp)->bf, sizeof(struct mlx5_bf));
+		if(register_update_mem(&mqp->migr_bf, sizeof(*content_p), content_p)) {
+			return -1;
+		}
+	}
+
+	{
+		typeof(to_mqp(new_qp)->bf) *content_p;
+		content_p = malloc(sizeof(*content_p));
+		*content_p = to_mqp(new_qp)->bf;
+		if(register_update_mem(&mqp->new_bf, sizeof(*content_p), content_p)) {
+			return -1;
+		}
+	}
+
+	return 0;
+}
+
+int mlx5_replay_recv_wr(struct ibv_qp *qp) {
+	struct mlx5_qp *mqp = to_mqp(qp);
+	struct ibv_recv_wr *wr_head = NULL;
+	struct ibv_recv_wr *wr = NULL;
+	struct ibv_recv_wr *tmp;
+	struct ibv_recv_wr *last_wr = NULL;
+	struct ibv_recv_wr *bad_wr;
+	struct ibv_send_wr *bad_snd_wr;
+	void *old_lkey_mapping = qp->context->lkey_mapping;
+	struct mlx5_buf tmp_buf;
+	struct mlx5_wq tmp_wq;
+	__be32 *tmp_db;
+	int err = 0;
+	int i;
+	struct mlx5_bf *orig_bf;
+
+	mqp->ibv_qp = qp;
+	memset(mqp->migr_buf.buf, 0,
+				align(mqp->buf_size, to_mdev(qp->context->device)->page_size));
+
+	memcpy(&tmp_buf, &mqp->buf, sizeof(tmp_buf));
+	memcpy(&mqp->buf, &mqp->migr_buf, sizeof(tmp_buf));
+	memcpy(&mqp->migr_buf, &tmp_buf, sizeof(tmp_buf));
+
+	tmp_db = mqp->db;
+	mqp->db = mqp->migr_db;
+	mqp->migr_db = tmp_db;
+
+	mqp->sq_start = mqp->buf.buf + mqp->sq.offset;
+	mqp->sq.qend = mqp->buf.buf + mqp->sq.offset +
+						(mqp->sq.wqe_cnt << mqp->sq.wqe_shift);
+
+	memcpy(&mqp->rollback_rq, &mqp->rq, sizeof(struct mlx5_wq));
+	memcpy(&mqp->rollback_sq, &mqp->sq, sizeof(struct mlx5_wq));
+
+	mqp->sq.cur_post = 0;
+	mqp->sq.head = 0;
+	mqp->sq.tail = 0;
+	mqp->rq.head = 0;
+	mqp->rq.tail = 0;
+
+	memcpy(mqp->sq.tmp_wrid, mqp->sq.wrid,
+						mqp->sq.wqe_cnt * sizeof(*mqp->sq.wrid));
+	memcpy(mqp->sq.tmp_wr_data, mqp->sq.wr_data,
+						mqp->sq.wqe_cnt * sizeof(*mqp->sq.wr_data));
+	memcpy(mqp->sq.tmp_wqe_head, mqp->sq.wqe_head,
+						mqp->sq.wqe_cnt * sizeof(*mqp->sq.wqe_head));
+	memcpy(mqp->rq.tmp_wrid, mqp->rq.wrid,
+						mqp->rq.wqe_cnt * sizeof(*mqp->rq.wrid));
+
+	qp->context->lkey_mapping = MAP_FAILED;
+
+	for(i = mqp->onflight_tail > mqp->migrrdma_rq.n_acked? mqp->onflight_tail: mqp->migrrdma_rq.n_acked;
+						i < mqp->onflight_head; i++) {
+		wr = calloc(1, sizeof(*wr));
+		wr->wr_id = mqp->onflight_recv_wr[i % mqp->onflight_cap].wr_id;
+		wr->next = NULL;
+		wr->num_sge = mqp->onflight_recv_wr[i % mqp->onflight_cap].num_sge;
+		wr->sg_list = calloc(wr->num_sge, sizeof(struct ibv_sge));
+		memcpy(wr->sg_list, mqp->onflight_recv_wr[i % mqp->onflight_cap].sg_list,
+							sizeof(struct ibv_sge) * wr->num_sge);
+
+		if(!wr_head)
+			wr_head = wr;
+
+		if(last_wr)
+			last_wr->next = wr;
+
+		last_wr = wr;
+	}
+
+	if(!wr_head) {
+		err = 0;
+		goto out;
+	}
 
-		cmd.uidx = usr_idx;
+	qp->context->lkey_mapping = mmap(NULL, getpagesize(), PROT_READ, MAP_SHARED,
+									qp->context->lkey_fd, 0);
+	if(qp->context->lkey_mapping == MAP_FAILED) {
+		err = -1;
+		goto out;
 	}
 
-	mparent_domain = to_mparent_domain(attr->pd);
-	if (mparent_domain && mparent_domain->mtd)
-		bf = mparent_domain->mtd->bf;
+	err = __mlx5_post_recv__(qp, wr_head, &bad_wr, 0);
 
-	if (!bf && !(ctx->flags & MLX5_CTX_FLAGS_NO_KERN_DYN_UAR)) {
-		bf = mlx5_get_qp_uar(context);
-		if (!bf)
-			goto err_free_uidx;
+out:
+	qp->context->lkey_mapping = mmap(NULL, getpagesize(), PROT_READ, MAP_SHARED,
+									qp->context->lkey_fd, 0);
+	if(qp->context->lkey_mapping == MAP_FAILED) {
+		err = -1;
 	}
 
-	if (bf) {
-		if (bf->dyn_alloc_uar) {
-			cmd.bfreg_index = bf->page_id;
-			cmd.flags |= MLX5_QP_FLAG_UAR_PAGE_INDEX;
-		} else {
-			cmd.bfreg_index = bf->bfreg_dyn_index;
-			cmd.flags |= MLX5_QP_FLAG_BFREG_INDEX;
-		}
+	orig_bf = mqp->bf;
+	mqp->bf = get_alloc_bf(mqp->new_bf);
+	if(!mqp->bf) {
+		mqp->bf = calloc(1, sizeof(struct mlx5_bf));
+		memcpy(mqp->bf, &mqp->migr_bf, sizeof(struct mlx5_bf));
+		add_bf_addr_map_entry(mqp->new_bf, mqp->bf);
 	}
 
-	if (ctx->flags & MLX5_CTX_FLAGS_ECE_SUPPORTED)
-		/* Create QP should start from ECE version 1 as a trigger */
-		cmd.ece_options = 0x10000000;
+	if(qp->context->lkey_mapping != MAP_FAILED && mqp->n_cached > 0) {
+		__mlx5_post_send__(qp, mqp->cached_send_wr_list, &bad_snd_wr, 0);
+	}
 
-	if (attr->comp_mask & MLX5_CREATE_QP_EX2_COMP_MASK)
-		ret = mlx5_cmd_create_qp_ex(context, attr, &cmd, qp, &resp_ex);
-	else
-		ret = ibv_cmd_create_qp_ex(context, &qp->verbs_qp,
-					   attr, &cmd.ibv_cmd, sizeof(cmd),
-					   &resp.ibv_resp, sizeof(resp));
-	if (ret) {
-		mlx5_dbg(fp, MLX5_DBG_QP, "ret %d\n", ret);
-		goto err_free_uidx;
+	mqp->bf = orig_bf;
+
+	if(qp->context->lkey_mapping != MAP_FAILED)
+		munmap(qp->context->lkey_mapping, getpagesize());
+	qp->context->lkey_mapping = old_lkey_mapping;
+	for(wr = wr_head, tmp = wr? wr->next: NULL; wr != NULL;
+					wr = tmp, tmp = wr? wr->next: NULL) {
+		free(wr->sg_list);
+		free(wr);
 	}
 
-	resp_drv = attr->comp_mask & MLX5_CREATE_QP_EX2_COMP_MASK ?
-			&resp_ex.drv_payload : &resp.drv_payload;
-	if (!ctx->cqe_version) {
-		if (qp->sq.wqe_cnt || qp->rq.wqe_cnt) {
-			ret = mlx5_store_qp(ctx, ibqp->qp_num, qp);
-			if (ret) {
-				mlx5_dbg(fp, MLX5_DBG_QP, "ret %d\n", ret);
-				goto err_destroy;
-			}
-		}
+	memcpy(&tmp_buf, &mqp->buf, sizeof(tmp_buf));
+	memcpy(&mqp->buf, &mqp->migr_buf, sizeof(tmp_buf));
+	memcpy(&mqp->migr_buf, &tmp_buf, sizeof(tmp_buf));
 
-		pthread_mutex_unlock(&ctx->qp_table_mutex);
+	tmp_db = mqp->db;
+	mqp->db = mqp->migr_db;
+	mqp->migr_db = tmp_db;
+
+	mqp->sq_start = mqp->buf.buf + mqp->sq.offset;
+	mqp->sq.qend = mqp->buf.buf + mqp->sq.offset +
+						(mqp->sq.wqe_cnt << mqp->sq.wqe_shift);
+
+	memcpy(&tmp_wq, &mqp->rollback_rq, sizeof(struct mlx5_wq));
+	memcpy(&mqp->rollback_rq, &mqp->rq, sizeof(struct mlx5_wq));
+	memcpy(&mqp->rq, &tmp_wq, sizeof(struct mlx5_wq));
+
+	memcpy(&tmp_wq, &mqp->rollback_sq, sizeof(struct mlx5_wq));
+	memcpy(&mqp->rollback_sq, &mqp->sq, sizeof(struct mlx5_wq));
+	memcpy(&mqp->sq, &tmp_wq, sizeof(struct mlx5_wq));
+
+	mqp->migr_flag = 1;
+
+	return err;
+}
+
+static void init_srq_buf(struct mlx5_srq *srq, int start, int end) {
+	struct mlx5_wqe_srq_next_seg *next;
+	int i;
+
+	for(i = start; i < end; i++) {
+		next = srq->migr_buf.buf + (i << srq->wqe_shift);
+		next->next_wqe_index = htobe16(i + 1);
 	}
+}
 
-	qp->get_ece = resp_drv->ece_options;
-	map_uuar(context, qp, resp_drv->bfreg_index, bf);
+int mlx5_prepare_srq_replay(struct ibv_srq *srq, struct ibv_srq *new_srq,
+						int *head, int *tail) {
+	*head = to_msrq(new_srq)->head;
+	*tail = to_msrq(new_srq)->tail;
+	return 0;
+}
 
-	qp->rq.max_post = qp->rq.wqe_cnt;
-	if (attr->sq_sig_all)
-		qp->sq_signal_bits = MLX5_WQE_CTRL_CQ_UPDATE;
-	else
-		qp->sq_signal_bits = 0;
+int mlx5_replay_srq_recv_wr(struct ibv_srq *ibsrq, int head, int tail) {
+	struct mlx5_srq *srq = to_msrq(ibsrq);
+	struct ibv_recv_wr *wr_head = NULL;
+	struct ibv_recv_wr *wr = NULL;
+	struct ibv_recv_wr *tmp;
+	struct ibv_recv_wr *last_wr = NULL;
+	struct ibv_recv_wr *bad_wr;
+	void *old_lkey_mapping = ibsrq->context->lkey_mapping;
+	struct mlx5_buf tmp_buf;
+	__be32 *tmp_db;
+	int err = 0;
+	int tmp_head, tmp_tail;
+	int i;
+	int size;
+	int buf_size;
+	struct srq_recv_wr_item *iter_wr_item;
 
-	attr->cap.max_send_wr = qp->sq.max_post;
-	attr->cap.max_recv_wr = qp->rq.max_post;
-	attr->cap.max_recv_sge = qp->rq.max_gs;
+	size = sizeof(struct mlx5_wqe_srq_next_seg) +
+				srq->max_gs * sizeof(struct mlx5_wqe_data_seg);
+	size = max(32, size);
+	size = roundup_pow_of_two(size);
+	buf_size = srq->max * size;
 
-	qp->rsc.type = MLX5_RSC_TYPE_QP;
-	qp->rsc.rsn = (ctx->cqe_version && !is_xrc_tgt(attr->qp_type)) ?
-		      usr_idx : ibqp->qp_num;
+	memset(srq->migr_buf.buf, 0, buf_size);
+	init_srq_buf(srq, head, tail);
 
-	if (mparent_domain)
-		atomic_fetch_add(&mparent_domain->mpd.refcount, 1);
+	memcpy(&tmp_buf, &srq->buf, sizeof(tmp_buf));
+	memcpy(&srq->buf, &srq->migr_buf, sizeof(tmp_buf));
+	memcpy(&srq->migr_buf, &tmp_buf, sizeof(tmp_buf));
 
-	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TIRN)
-		qp->tirn = resp_drv->tirn;
+	tmp_db = srq->db;
+	srq->db = srq->migr_db;
+	srq->migr_db = tmp_db;
 
-	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TISN)
-		qp->tisn = resp_drv->tisn;
+	srq->rollback_head = srq->head;
+	srq->rollback_tail = srq->tail;
 
-	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_RQN)
-		qp->rqn = resp_drv->rqn;
+	srq->head = head;
+	srq->tail = tail;
 
-	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_SQN)
-		qp->sqn = resp_drv->sqn;
+	memcpy(srq->tmp_wrid, srq->wrid, srq->max * sizeof(*srq->wrid));
 
-	if (resp_drv->comp_mask & MLX5_IB_CREATE_QP_RESP_MASK_TIR_ICM_ADDR)
-		qp->tir_icm_addr = resp_drv->tir_icm_addr;
+	ibsrq->context->lkey_mapping = MAP_FAILED;
 
-	if (attr->comp_mask & IBV_QP_INIT_ATTR_SEND_OPS_FLAGS)
-		qp->verbs_qp.comp_mask |= VERBS_QP_EX;
+	list_for_each(&srq->migrrdma_onflight_list, iter_wr_item, list) {
+		wr = calloc(1, sizeof(*wr));
+		wr->wr_id = iter_wr_item->recv_wr.wr_id;
+		wr->next = NULL;
+		wr->num_sge = iter_wr_item->recv_wr.num_sge;
+		wr->sg_list = calloc(wr->num_sge, sizeof(struct ibv_sge));
+		memcpy(wr->sg_list, iter_wr_item->recv_wr.sg_list,
+							sizeof(struct ibv_sge) * wr->num_sge);
 
-	return ibqp;
+		if(!wr_head)
+			wr_head = wr;
 
-err_destroy:
-	ibv_cmd_destroy_qp(ibqp);
+		if(last_wr)
+			last_wr->next = wr;
 
-err_free_uidx:
-	if (bf)
-		mlx5_put_qp_uar(ctx, bf);
-	if (!ctx->cqe_version)
-		pthread_mutex_unlock(&to_mctx(context)->qp_table_mutex);
-	else if (!is_xrc_tgt(attr->qp_type))
-		mlx5_clear_uidx(ctx, usr_idx);
+		last_wr = wr;
+	}
 
-err_rq_db:
-	mlx5_free_db(to_mctx(context), qp->db, attr->pd, qp->custom_db);
+	if(!wr_head) {
+		err = 0;
+		goto out;
+	}
 
-err_free_qp_buf:
-	mlx5_free_qp_buf(ctx, qp);
+	ibsrq->context->lkey_mapping = mmap(NULL, getpagesize(), PROT_READ, MAP_SHARED,
+						ibsrq->context->lkey_fd, 0);
+	if(ibsrq->context->lkey_mapping == MAP_FAILED) {
+		err = -1;
+		goto out;
+	}
 
-err:
-	free(qp);
+	err = __mlx5_post_srq_recv__(ibsrq, wr_head, &bad_wr);
 
-	return NULL;
+out:
+	if(ibsrq->context->lkey_mapping != MAP_FAILED)
+		munmap(ibsrq->context->lkey_mapping, getpagesize());
+	ibsrq->context->lkey_mapping = old_lkey_mapping;
+	for(wr = wr_head, tmp = wr? wr->next: NULL; wr != NULL;
+				wr = tmp, tmp = wr? wr->next: NULL) {
+		free(wr->sg_list);
+		free(wr);
+	}
+
+	memcpy(&tmp_buf, &srq->buf, sizeof(tmp_buf));
+	memcpy(&srq->buf, &srq->migr_buf, sizeof(tmp_buf));
+	memcpy(&srq->migr_buf, &tmp_buf, sizeof(tmp_buf));
+
+	tmp_db = srq->db;
+	srq->db = srq->migr_db;
+	srq->migr_db = tmp_db;
+
+	tmp_head = srq->rollback_head;
+	srq->rollback_head = srq->head;
+	srq->head = tmp_head;
+
+	tmp_tail = srq->rollback_tail;
+	srq->rollback_tail = srq->tail;
+	srq->tail = tmp_tail;
+
+	srq->migr_flag = 1;
+	srq->inspect_flag = 0;
+
+	srq->rollback_counter = srq->counter;
+}
+
+int mlx5_is_q_empty(struct ibv_qp *qp) {
+	struct mlx5_qp *mqp = to_mqp(qp);
+	int err;
+	
+	if(mqp->sq.commit_posted <= mqp->sq.n_acked)
+		return 1;
+
+	err = !(mqp->sq.commit_posted > mqp->sq.n_acked);
+	return err;
+}
+
+int mlx5_migrrdma_is_q_empty(struct ibv_qp *qp) {
+	struct mlx5_qp *mqp = to_mqp(qp);
+	int err;
+	
+	if(mqp->migrrdma_sq.commit_posted <= mqp->migrrdma_sq.n_acked)
+		return 1;
+
+	err = !(mqp->migrrdma_sq.commit_posted > mqp->migrrdma_sq.n_acked);
+	return err;
+}
+
+void migrrdma_start_inspect_qp(struct ibv_qp *qp) {
+	struct mlx5_qp *mqp = to_mqp(qp);
+	memcpy(&mqp->migrrdma_rq, &mqp->rq, sizeof(struct mlx5_wq));
+	memcpy(&mqp->migrrdma_sq, &mqp->sq, sizeof(struct mlx5_wq));
+}
+
+void migrrdma_start_inspect_qp_v2(struct ibv_qp *qp) {
+	struct mlx5_qp *mqp = to_mqp(qp);
+	memcpy(&mqp->migrrdma_rq, &mqp->rq, sizeof(struct mlx5_wq));
+	memcpy(&mqp->migrrdma_sq, &mqp->sq, sizeof(struct mlx5_wq));
+	if(qp->srq) {
+		struct mlx5_srq *msrq = to_msrq(qp->srq);
+		struct srq_recv_wr_item *cur_item;
+
+		if(!msrq->inspect_flag) {
+			msrq->inspect_flag = 1;
+
+			msrq->migrrdma_acked = msrq->n_acked;
+			list_for_each(&msrq->onflight_list, cur_item, list) {
+				int cur_idx = cur_item - msrq->onflight_recv_wr;
+				struct srq_recv_wr_item *from = cur_item;
+				struct srq_recv_wr_item *to = msrq->migrrdma_onflight + cur_idx;
+
+				to->recv_wr.wr_id = from->recv_wr.wr_id;
+				to->recv_wr.next = NULL;
+				to->recv_wr.num_sge = from->recv_wr.num_sge;
+				memcpy(to->recv_wr.sg_list, from->recv_wr.sg_list,
+							sizeof(struct ibv_sge) * from->recv_wr.num_sge);
+				list_add_tail(&msrq->migrrdma_onflight_list, &to->list);
+
+			}
+		}
+	}
 }
 
 struct ibv_qp *mlx5_create_qp(struct ibv_pd *pd,
 			      struct ibv_qp_init_attr *attr)
 {
 	struct ibv_qp *qp;
+	struct mlx5_qp *mqp;
 	struct ibv_qp_init_attr_ex attrx;
+	uint32_t max_send_wr = attr->cap.max_send_wr;
+	uint32_t max_recv_wr = attr->cap.max_recv_wr;
+	char fname[128];
+	int info_fd;
 
 	memset(&attrx, 0, sizeof(attrx));
 	memcpy(&attrx, attr, sizeof(*attr));
@@ -2562,15 +4577,83 @@ struct ibv_qp *mlx5_create_qp(struct ibv_pd *pd,
 	qp = create_qp(pd->context, &attrx, NULL);
 	if (qp)
 		memcpy(attr, &attrx, sizeof(*attr));
+	else {
+		return NULL;
+	}
+
+	mqp = to_mqp(qp);
+
+	mqp->sq.n_acked = 0;
+	mqp->sq.n_posted = 0;
+	mqp->rq.n_acked = 0;
+	mqp->rq.n_posted = 0;
+	mqp->two_sided_posted = 0;
+	mqp->sq.commit_posted = 0;
+	mqp->rq.commit_posted = 0;
+
+	if(ibv_cmd_install_qp_handle_mapping(pd->context, qp->handle, qp->handle)) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/buf_addr",
+					rdma_getpid(pd->context), pd->context->cmd_fd, qp->handle);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	if(write(info_fd, &mqp->migr_buf.buf, sizeof(void *)) < 0) {
+		close(info_fd);
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	close(info_fd);
+
+	sprintf(fname, "/proc/rdma_uwrite/%d/%d/qp_%d/db_addr",
+					rdma_getpid(pd->context), pd->context->cmd_fd, qp->handle);
+	info_fd = open(fname, O_WRONLY);
+	if(info_fd < 0) {
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	if(write(info_fd, &mqp->migr_db, sizeof(void *)) < 0) {
+		close(info_fd);
+		ibv_destroy_qp(qp);
+		return NULL;
+	}
+
+	close(info_fd);
 
 	return qp;
 }
 
+uint64_t mlx5_qp_get_n_posted(struct ibv_qp *qp) {
+	struct mlx5_qp *mqp = to_mqp(qp);
+	return mqp->two_sided_posted;
+}
+
+uint64_t mlx5_qp_get_n_acked(struct ibv_qp *qp) {
+	struct mlx5_qp *mqp = to_mqp(qp);
+	return mqp->migrrdma_rq.n_acked;
+}
+
+uint64_t mlx5_srq_get_n_acked(struct ibv_srq *srq) {
+	struct mlx5_srq *msrq = to_msrq(srq);
+	return msrq->migrrdma_acked;
+}
+
 static void mlx5_lock_cqs(struct ibv_qp *qp)
 {
 	struct mlx5_cq *send_cq = to_mcq(qp->send_cq);
 	struct mlx5_cq *recv_cq = to_mcq(qp->recv_cq);
 
+	if(!ibv_get_signal())
+		return;
+
 	if (send_cq && recv_cq) {
 		if (send_cq == recv_cq) {
 			mlx5_spin_lock(&send_cq->lock);
@@ -2593,6 +4676,9 @@ static void mlx5_unlock_cqs(struct ibv_qp *qp)
 	struct mlx5_cq *send_cq = to_mcq(qp->send_cq);
 	struct mlx5_cq *recv_cq = to_mcq(qp->recv_cq);
 
+	if(!ibv_get_signal())
+		return;
+
 	if (send_cq && recv_cq) {
 		if (send_cq == recv_cq) {
 			mlx5_spin_unlock(&send_cq->lock);
@@ -2610,6 +4696,11 @@ static void mlx5_unlock_cqs(struct ibv_qp *qp)
 	}
 }
 
+static int destroy_switch_qp(struct ibv_qp *orig_qp, struct ibv_qp *new_qp,
+						void *param) {
+	return mlx5_destroy_qp_tmp(new_qp);
+}
+
 int mlx5_destroy_qp(struct ibv_qp *ibqp)
 {
 	struct mlx5_qp *qp = to_mqp(ibqp);
@@ -2617,6 +4708,101 @@ int mlx5_destroy_qp(struct ibv_qp *ibqp)
 	int ret;
 	struct mlx5_parent_domain *mparent_domain = to_mparent_domain(ibqp->pd);
 
+	if(qp->switch_qp) {
+		mlx5_destroy_qp_tmp(&qp->switch_qp->verbs_qp.qp);
+		qp->switch_qp = NULL;
+	}
+
+	switch_to_new_qp(qp->verbs_qp.qp.real_qpn, NULL, destroy_switch_qp);
+
+	for(int i = 0; i < qp->onflight_max_sge; i++) {
+		free(qp->onflight_recv_wr[i].sg_list);
+	}
+
+	free(qp->onflight_recv_wr);
+
+	for(int i = 0; i < qp->cached_cap; i++) {
+		free(qp->cached_send_wr_list[i].sg_list);
+	}
+
+	free(qp->cached_send_wr_list);
+
+	if (qp->rss_qp) {
+		ret = ibv_cmd_destroy_qp(ibqp);
+		if (ret)
+			return ret;
+		goto free;
+	}
+
+	if (!ctx->cqe_version)
+		pthread_mutex_lock(&ctx->qp_table_mutex);
+
+	ret = ibv_cmd_destroy_qp(ibqp);
+	if (ret) {
+		if (!ctx->cqe_version)
+			pthread_mutex_unlock(&ctx->qp_table_mutex);
+		return ret;
+	}
+
+	mlx5_lock_cqs(ibqp);
+
+	__mlx5_cq_clean(to_mcq(ibqp->recv_cq), qp->rsc.rsn,
+			ibqp->srq ? to_msrq(ibqp->srq) : NULL);
+	if (ibqp->send_cq != ibqp->recv_cq)
+		__mlx5_cq_clean(to_mcq(ibqp->send_cq), qp->rsc.rsn, NULL);
+
+	if (!ctx->cqe_version) {
+		if (qp->dc_type == MLX5DV_DCTYPE_DCT) {
+			/* The QP was inserted to the tracking table only after
+			 * that it was modifed to RTR
+			 */
+			if (ibqp->state == IBV_QPS_RTR)
+				mlx5_clear_qp(ctx, ibqp->qp_num);
+		} else {
+			if (qp->sq.wqe_cnt || qp->rq.wqe_cnt)
+				mlx5_clear_qp(ctx, ibqp->qp_num);
+		}
+	}
+
+	mlx5_unlock_cqs(ibqp);
+	if (!ctx->cqe_version)
+		pthread_mutex_unlock(&ctx->qp_table_mutex);
+	else if (!is_xrc_tgt(ibqp->qp_type))
+		mlx5_clear_uidx(ctx, qp->rsc.rsn);
+
+	if (qp->dc_type != MLX5DV_DCTYPE_DCT) {
+		mlx5_free_db(ctx, qp->db, ibqp->pd, qp->custom_db);
+		mlx5_free_qp_buf(ctx, qp);
+	}
+free:
+	if (mparent_domain)
+		atomic_fetch_sub(&mparent_domain->mpd.refcount, 1);
+
+//	mlx5_put_qp_uar(ctx, qp->bf);
+	free(qp);
+
+	return 0;
+}
+
+int mlx5_destroy_qp_tmp(struct ibv_qp *ibqp)
+{
+	struct mlx5_qp *qp = to_mqp(ibqp);
+	struct mlx5_context *ctx = to_mctx(ibqp->context);
+	int ret;
+	struct mlx5_parent_domain *mparent_domain = to_mparent_domain(ibqp->pd);
+
+	for(int i = 0; i < qp->onflight_max_sge; i++) {
+		free(qp->onflight_recv_wr[i].sg_list);
+	}
+
+	free(qp->onflight_recv_wr);
+
+	for(int i = 0; i < qp->cached_cap; i++) {
+		free(qp->cached_send_wr_list[i].sg_list);
+	}
+
+	free(qp->cached_send_wr_list);
+
 	if (qp->rss_qp) {
 		ret = ibv_cmd_destroy_qp(ibqp);
 		if (ret)
@@ -2624,16 +4810,21 @@ int mlx5_destroy_qp(struct ibv_qp *ibqp)
 		goto free;
 	}
 
+#if 0
 	if (!ctx->cqe_version)
 		pthread_mutex_lock(&ctx->qp_table_mutex);
+#endif
 
 	ret = ibv_cmd_destroy_qp(ibqp);
 	if (ret) {
+#if 0
 		if (!ctx->cqe_version)
 			pthread_mutex_unlock(&ctx->qp_table_mutex);
+#endif
 		return ret;
 	}
 
+#if 0
 	mlx5_lock_cqs(ibqp);
 
 	__mlx5_cq_clean(to_mcq(ibqp->recv_cq), qp->rsc.rsn,
@@ -2659,6 +4850,7 @@ int mlx5_destroy_qp(struct ibv_qp *ibqp)
 		pthread_mutex_unlock(&ctx->qp_table_mutex);
 	else if (!is_xrc_tgt(ibqp->qp_type))
 		mlx5_clear_uidx(ctx, qp->rsc.rsn);
+#endif
 
 	if (qp->dc_type != MLX5DV_DCTYPE_DCT) {
 		mlx5_free_db(ctx, qp->db, ibqp->pd, qp->custom_db);
@@ -2668,7 +4860,7 @@ free:
 	if (mparent_domain)
 		atomic_fetch_sub(&mparent_domain->mpd.refcount, 1);
 
-	mlx5_put_qp_uar(ctx, qp->bf);
+//	mlx5_put_qp_uar(ctx, qp->bf);
 	free(qp);
 
 	return 0;
@@ -2898,9 +5090,11 @@ int mlx5_modify_qp(struct ibv_qp *qp, struct ibv_qp_attr *attr,
 	    attr->qp_state == IBV_QPS_RTR &&
 	    (qp->qp_type == IBV_QPT_RAW_PACKET ||
 	     mqp->flags & MLX5_QP_FLAGS_USE_UNDERLAY)) {
-		mlx5_spin_lock(&mqp->rq.lock);
+		if(ibv_get_signal())
+			mlx5_spin_lock(&mqp->rq.lock);
 		mqp->db[MLX5_RCV_DBR] = htobe32(mqp->rq.head & 0xffff);
-		mlx5_spin_unlock(&mqp->rq.lock);
+		if(ibv_get_signal())
+			mlx5_spin_unlock(&mqp->rq.lock);
 	}
 
 	if (!ret &&
@@ -3519,10 +5713,13 @@ err_free_uidx:
 
 err_free_db:
 	mlx5_free_db(ctx, msrq->db, attr->pd, msrq->custom_db);
+	mlx5_free_db(ctx, msrq->migr_db, attr->pd, msrq->custom_db);
 
 err_free:
 	free(msrq->wrid);
+	free(msrq->tmp_wrid);
 	mlx5_free_actual_buf(ctx, &msrq->buf);
+	mlx5_free_actual_buf(ctx, &msrq->migr_buf);
 
 err:
 	free(msrq);
@@ -4028,10 +6225,12 @@ int mlx5_modify_wq(struct ibv_wq *wq, struct ibv_wq_attr *attr)
 			return -EINVAL;
 
 		if (wq->state == IBV_WQS_RESET) {
-			mlx5_spin_lock(&to_mcq(wq->cq)->lock);
+			if(ibv_get_signal())
+				mlx5_spin_lock(&to_mcq(wq->cq)->lock);
 			__mlx5_cq_clean(to_mcq(wq->cq),
 					rwq->rsc.rsn, NULL);
-			mlx5_spin_unlock(&to_mcq(wq->cq)->lock);
+			if(ibv_get_signal())
+				mlx5_spin_unlock(&to_mcq(wq->cq)->lock);
 			mlx5_init_rwq_indices(rwq);
 			rwq->db[MLX5_RCV_DBR] = 0;
 			rwq->db[MLX5_SND_DBR] = 0;
@@ -4050,9 +6249,11 @@ int mlx5_destroy_wq(struct ibv_wq *wq)
 	if (ret)
 		return ret;
 
-	mlx5_spin_lock(&to_mcq(wq->cq)->lock);
+	if(ibv_get_signal())
+		mlx5_spin_lock(&to_mcq(wq->cq)->lock);
 	__mlx5_cq_clean(to_mcq(wq->cq), rwq->rsc.rsn, NULL);
-	mlx5_spin_unlock(&to_mcq(wq->cq)->lock);
+	if(ibv_get_signal())
+		mlx5_spin_unlock(&to_mcq(wq->cq)->lock);
 	mlx5_clear_uidx(to_mctx(wq->context), rwq->rsc.rsn);
 	mlx5_free_db(to_mctx(wq->context), rwq->db, wq->pd, rwq->custom_db);
 	mlx5_free_rwq_buf(rwq, wq->context);
